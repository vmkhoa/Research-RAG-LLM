[Sample 1]
========================================
Title: Two for the Price of One: Integrating Large Language Models to Learn Biophysical Interactions

Abstract: Deep learning models have become fundamental tools in drug design. In
particular, large language models trained on biochemical sequences learn
feature vectors that guide drug discovery through virtual screening. However,
such models do not capture the molecular interactions important for binding
affinity and specificity. Therefore, there is a need to 'compose'
representations from distinct biological modalities to effectively represent
molecular complexes. We present an overview of the methods to combine molecular
representations and propose that future work should balance computational
efficiency and expressiveness. Specifically, we argue that improvements in both
speed and accuracy are possible by learning to merge the representations from
internal layers of domain specific biological language models. We demonstrate
that 'composing' biochemical language models performs similar or better than
standard methods representing molecular interactions despite having
significantly fewer features. Finally, we discuss recent methods for
interpreting and democratizing large language models that could aid the
development of interaction aware foundation models for biology, as well as
their shortcomings.

Two for the Price of One: Integrating Large
Language Models to Learn Biophysical
Interactions
Joseph D. Clark,†,⊥Tanner J. Dean,‡,⊥and Diwakar Shukla*∗,‡,¶,§,∥
†School of Molecular and Cellular Biology, University of Illinois at Urbana-Champaign,
Urbana, IL 61801, USA
‡Center for Biophysics and Quantitative Biology, University of Illinois at
Urbana-Champaign, Urbana, IL 61801, USA
¶Department of Chemical and Biomolecular Engineering, University of Illinois at
Urbana-Champaign, Urbana, IL 61801, USA
§Department of Bioengineering, University of Illinois at Urbana-Champaign, Urbana, IL
61801, USA
∥Department of Chemistry, University of Illinois at Urbana-Chamapaign, Urbana, IL
61801, USA
⊥These authors contributed equally to this work.
E-mail: diwakar@illinois.edu
Abstract
Deep learning models have become fundamental tools in drug design. In particular,
large language models trained on biochemical sequences learn feature vectors that guide
drug discovery through virtual screening. However, such models do not capture the
molecular interactions important for binding affinity and specificity. Therefore, there
1
arXiv:2503.21017v1  [q-bio.BM]  26 Mar 2025

is a need to ‘compose’ representations from distinct biological modalities to effectively
represent molecular complexes. We present an overview of the methods to combine
molecular representations and propose that future work should balance computational
efficiency and expressiveness. Specifically, we argue that improvements in both speed
and accuracy are possible by learning to merge the representations from internal lay-
ers of domain specific biological language models. We demonstrate that ‘composing’
biochemical language models performs similar or better than standard methods repre-
senting molecular interactions despite having significantly fewer features. Finally, we
discuss recent methods for interpreting and democratizing large language models that
could aid the development of interaction aware foundation models for biology, as well
as their shortcomings.
Introduction
The vastness of chemical space severely limits experimental screening in drug design.1 Ad-
vances in deep learning can help circumvent this issue by enabling large scale computational
screening to identify potential drugs.2,3 First, molecules are transformed into feature vectors
(also called embeddings) which encode biochemical information (Figure 1A). In practice,
embeddings can take the form of hand picked features,4 topological encodings,5 or the in-
ternal representations from large language models.6 Machine learning (ML) models are then
trained to predict molecular properties given embeddings as input. Popular ML models in
drug discovery include random forests, support vector machines, and neural networks.7–9
Accurate ML models enable efficient screening of molecular libraries to identify candidate
molecules with desired properties. In the context of drug design, essential properties of inter-
est include high binding affinity and specificity for protein target(s).10,11 Therefore, ML mod-
els often predict properties of molecular complexes, such as protein-ligand, protein-protein,
protein-peptide, or protein-nucleic acid interactions. However, molecular representations are
typically unimodal in nature and lack explicit features describing intermolecular interactions
(Figure 1B). Additionally, most molecular representations are derived from sequence alone,
2

which limits their ability to capture structural information important for interactions.3,12
This can lead to inaccurate predictions for molecular interactions. For example, the model
(1) predicts binding affinity of the ligand, but it does not predict the specificity of binding
to the target protein.
Figure 1.
(A) A schematic of deep learning for drug design. Biochemical sequences are transformed into
feature vectors (embeddings). ML models are then trained to predict molecular properties given
embeddings as input. (B)

[Sample 2]
========================================
Title: The impact of future $D$- and $B$-meson measurements with the SMOG2 program at LHCb on the determination of nuclear parton distribution functions

Abstract: We perform an analysis of the potential impact of future $D$- and $B$-meson
measurement within the SMOG2 fixed-target program at the LHCb experiment on
nuclear parton distribution functions. Following~\cite{Bursche:2018orf}, we
assume that SMOG2 will collect data for five nuclear targets: He, Ne, Ar, Kr,
Xe and hydrogen which will provide a baseline for constructing nuclear ratios.
The analysis is performed by using the PDF reweighting method. We demonstrate
that such a measurement will allow to considerably reduce the current
uncertainties of the nuclear gluon distribution and, to some extent, even the
uncertainties of the light sea-quark distributions. Furthermore, it will
provide the first possibility to systematically study the current assumptions
on the $A$-dependence of the nuclear parton distributions.

The impact of future D- and B-meson measurements with the SMOG2 program at
LHCb on the determination of nuclear parton distribution functions
Carlo Flore
a,b, Cynthia Hadjidakis
c, Daniel Kikoła
d, Aleksander Kusina
e, Anton Safronov
d,∗
aDipartimento di Fisica, Università di Cagliari, Cittadella Universitaria, Cagliari, I-09042, Monserrato (CA), Italy
bINFN, Sezione di Cagliari, Cittadella Universitaria, Cagliari, I-09042, Monserrato (CA), Italy
cUniversité Paris-Saclay, CNRS/IN2P3, IJCLab, Orsay 91405, France
dFaculty of Physics, Warsaw University of Technology, plac Politechniki 1, Warszawa, 00-661, Poland
eInstitute of Nuclear Physics, Polish Academy of Sciences, ul. Radzikowskiego 152, Cracow, 31-342, Poland
Abstract
We perform an analysis of the potential impact of future D- and B-meson measurement within the SMOG2 fixed-target
program at the LHCb experiment on nuclear parton distribution functions. Following [1], we assume that SMOG2 will
collect data for five nuclear targets: He, Ne, Ar, Kr, Xe and hydrogen which will provide a baseline for constructing
nuclear ratios. The analysis is performed by using the PDF reweighting method. We demonstrate that such a mea-
surement will allow to considerably reduce the current uncertainties of the nuclear gluon distribution and, to some
extent, even the uncertainties of the light sea-quark distributions. Furthermore, it will provide the first possibility to
systematically study the current assumptions on the A-dependence of the nuclear parton distributions.
Keywords: LHC, fixed-target experiment, SMOG2, heavy flavour, nPDFs, reweighting
1. Introduction
One of the crucial aspects of high-energy nuclear
physics is studying the structure of matter, in partic-
ular, in terms of its smallest components, quarks and
gluons, collectively referred to as partons.
Within the
collinear factorisation approach [2, 3], the partonic struc-
ture of a hadron is described by parton distribution func-
tions (PDFs), which represent the probability of finding
partons with a given fraction x of the proton longitudi-
nal momentum. PDFs encapsulate the non-perturbative
information about hadron content, and are determined
through fits to experimental data [4–7].
Furthermore, the partonic structure of nucleons bound
in a nucleus differs from that of a free proton. This phe-
nomenon was first identified as the EMC effect [8]. Sub-
sequent studies have shown that this modification varies
with x and we can now recognise four characteristic re-
gions of nuclear modifications [9–15].
These are iden-
tified as: (i) the shadowing region at x ≲0.1 (charac-
terised by a depletion of nuclear cross-sections compared
to the free-nucleon ones), (ii) the antishadowing region
at 0.25 ≲x ≲0.1 (characterised by a corresponding en-
hancement), (iii) the EMC region 0.75 ≲x ≲0.25 (char-
∗Corresponding author. E-mail: d.kosina@up.pl.
arxiv.org/abs/2502.14094v1  
1

The impact of future D- and B-meson measurements with the SMOG2 program at LHCb on the determination of nuclear parton distribution functions

(iv) the normal region at x ≲0.75 (characterised by the de-
pendence of the nuclear cross

[Sample 3]
========================================
Title: Collab: Controlled Decoding using Mixture of Agents for LLM Alignment

Abstract: Alignment of Large Language models (LLMs) is crucial for safe and trustworthy
deployment in applications. Reinforcement learning from human feedback (RLHF)
has emerged as an effective technique to align LLMs to human preferences and
broader utilities, but it requires updating billions of model parameters, which
is computationally expensive. Controlled Decoding, by contrast, provides a
mechanism for aligning a model at inference time without retraining. However,
single-agent decoding approaches often struggle to adapt to diverse tasks due
to the complexity and variability inherent in these tasks. To strengthen the
test-time performance w.r.t the target task, we propose a mixture of
agent-based decoding strategies leveraging the existing off-the-shelf aligned
LLM policies. Treating each prior policy as an agent in the spirit of mixture
of agent collaboration, we develop a decoding method that allows for
inference-time alignment through a token-level selection strategy among
multiple agents. For each token, the most suitable LLM is dynamically chosen
from a pool of models based on a long-term utility metric. This
policy-switching mechanism ensures optimal model selection at each step,
enabling efficient collaboration and alignment among LLMs during decoding.
Theoretical analysis of our proposed algorithm establishes optimal performance
with respect to the target task represented via a target reward for the given
off-the-shelf models. We conduct comprehensive empirical evaluations with
open-source aligned models on diverse tasks and preferences, which demonstrates
the merits of this approach over single-agent decoding baselines. Notably,
Collab surpasses the current SoTA decoding strategy, achieving an improvement
of up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.

Published as a conference paper at ICLR 2025
Collab: Controlled Decoding using Mixture of
Agents for LLM Alignment
Souradip Chakraborty1,2 ∗
Sujay Bhatt1
Udari Madhushani Sehwag1
Alec Koppel1
Soumya Suvra Ghosal2
Jiahao Qiu3
Mengdi Wang3
Dinesh Manocha2
Furong Huang2
Sumitra Ganesh1
1JPMorgan AI Research
2University of Maryland, College Park
3Princeton University
Abstract
Alignment of Large Language models (LLMs) is crucial for safe and trustworthy
deployment in applications. Reinforcement learning from human feedback (RLHF)
has emerged as an effective technique to align LLMs to human preferences, and
broader utilities, but it requires updating billions of model parameters which is com-
putationally expensive. Controlled Decoding, by contrast, provides a mechanism
for aligning a model at inference time without retraining. However, single-agent
decoding approaches often struggle to adapt to diverse tasks due to the complexity
and variability inherent in these tasks. To strengthen the test-time performance
w.r.t the target task, we propose a mixture of agents-based decoding strategies
leveraging the existing off-the-shelf aligned LLM policies. Treating each prior
policy as an agent in the spirit of mixture of agent collaboration, we develop a
decoding method that allows for inference-time alignment through a token-level
selection strategy among multiple agents. For each token, the most suitable LLM is
dynamically chosen from a pool of models based on a long-term utility metric. This
policy-switching mechanism ensures optimal model selection at each step, enabling
efficient collaboration and alignment among LLMs during decoding. Theoretical
analysis of our proposed algorithm establishes optimal performance with respect to
the target task represented via a target reward, for the given off-the-shelf models.
We conduct comprehensive empirical evaluations with open-source aligned models
on diverse tasks and preferences, which demonstrates the merits of this approach
over single-agent decoding baselines. Notably, Collab surpasses the current SoTA
decoding strategy, achieving an improvement of up to 1.56x in average reward and
71.89% in GPT-4 based win-tie rate.
1
Introduction
Large language models (and generative models) excel at generating coherent and realistic text, but
many text generation tasks require outputs that not only preserve fluency but also require satisfying
constraints, such as factual accuracy (Wang et al., 2024), knowledge grounding (Liang et al., 2024),
adherence to safety guidelines (Dong et al., 2024; Xie et al., 2024), or task and domain-specific
objectives (Jeong, 2024). Ensuring personalized, relevant, and safe text generation has become a
critical challenge for both industry and academia (Liang et al., 2024; Wang et al., 2024; Jeong, 2024).
One approach to address this challenge is to leverage RLHF (Krishna et al., 2025; Chen et al., 2024;
Dong et al., 2024; Xie et al., 2024) to align models to user preferences and broader utility. RLHF
has been successfully applied to a wide range of

[Sample 4]
========================================
Title: Toward a Healthier Social Media Experience: Designing 'Inspiration' and 'Reality' Modes to Enhance Digital Well-Being for Generation Z

Abstract: This study presents a dual-mode interface design concept for social media
platforms aimed at reducing social comparison in health-related content among
Korean MZ (Millennials and Gen-Z) users. The proposed "Inspiration" and
"Reality" modes allow users to toggle between curated, idealized posts and more
realistic, candid content. This approach aims to alleviate negative
psychological effects, such as decreased self-esteem and body dissatisfaction.
The pre-study outlines the design framework and discusses potential
implications for user satisfaction, perceived authenticity, and mental
well-being.

Toward a Healthier Social Media Experience: Designing 'Inspiration' and 
'Reality' Modes to Enhance Digital Well-Being for Generation Z 
Sora Kang1 
1Human-Computer Interaction + Design Lab, Seoul National University, Seoul, Korea 
E-mail: sorakang@snu.ac.kr 
 
 
Fig. 1. Envisioned ‘Inspiration Mode’ and ‘Reality Mode’ Interface 
 
Abstract—This study presents a dual-mode interface 
design concept for social media platforms aimed at 
reducing social comparison in health-related content 
among Korean MZ (Millennials and Gen-Z) users. The 
proposed "Inspiration" and "Reality" modes allow 
users to toggle between curated, idealized posts and 
more realistic, candid content. This approach aims to 
alleviate negative psychological effects, such as 
decreased self-esteem and body dissatisfaction. The 
pre-study outlines the design framework and discusses 
potential implications for user satisfaction, perceived 
authenticity, and mental well-being. 
Keywords— User Interface Design, Healthy Pleasure, 
MZ Generation, Social Media, Social Comparison 
I. INTRODUCTION 
The rise of social media has significantly transformed how 
people share and consume health-related content, 
especially among the younger generations[1]. In South 
Korea, the MZ generation (Millennials and Gen-Z), who 
are known for their digital literacy and active social media 
engagement, have embraced the trend of “Healthy 
Pleasure”[2], [3]. This trend prioritizes wellness practices 
that bring joy and sustainability over rigorous, discipline-
focused routines. Popular practices include sharing 
workout completion posts using hashtags like #오운완 (o-
un-wan, 오늘 운동 완료/Workout done today), 
documenting low-calorie meal choices, and showcasing 
personal fitness transformations. These activities not only 
foster a sense of community but also serve as a source of 
motivation and inspiration for users seeking to improve 
their health and well-being. 
Despite the positive aspects of engaging with health 
content online, there is a growing concern about the 
negative psychological effects associated with social 
comparison on social media platforms[4], [5]. According 
to 
Festinger’s 
Social 
Comparison 
Theory(1954), 
individuals have an innate tendency to evaluate their own 
abilities and self-worth by comparing themselves to 
others[6]. On social media, where content is often curated 
and idealized, users are more likely to engage in upward 
comparisons, measuring themselves against posts that 
highlight the most polished and successful aspects of 
others’ health journeys. This can lead to feelings of 
inadequacy, reduced self-esteem, and increased body 
dissatisfaction, particularly among younger users who are 
still developing their self-identity. 
Research has shown that exposure to idealized health and 
fitness imagery on platforms like Instagram and TikTok 
can exacerbate the pressures of social comparison, 
contributing to negative mental health outcomes. Studies 
by Turner (2015) indicate that Gen-Z users, in particular, 
are vulnerable to the effects of social comparison due to 
their preference for authenticity but simultaneous 
attraction to aspirational, visually appealing content[7]. 
This paradox creates a challenging environment for users 
who seek inspiration while also desiring relatable, genuine 
content. 
- 138 -

To address these issues, recent work in the field of 
Human-Computer Interaction (HCI) has focused on designing 
interfaces that promote healthy online experiences. In particular, 
the "Healthy Pleasure" design paradigm emphasizes the importance 
of well-being and sustainability in the design of digital 
environments, encouraging users to engage with content that 
enriches their lives without compromising their mental health[8]. 
This paradigm has been applied to various contexts, including 
e-commerce platforms, where designers aim to create a balance 


[Sample 5]
========================================
Title: Reconstructing Cell Lineage Trees from Phenotypic Features with Metric Learning

Abstract: How a single fertilized cell gives rise to a complex array of specialized
cell types in development is a central question in biology. The cells grow,
divide, and acquire differentiated characteristics through poorly understood
molecular processes. A key approach to studying developmental processes is to
infer the tree graph of cell lineage division and differentiation histories,
providing an analytical framework for dissecting individual cells' molecular
decisions during replication and differentiation. Although genetically
engineered lineage-tracing methods have advanced the field, they are either
infeasible or ethically constrained in many organisms. In contrast, modern
single-cell technologies can measure high-content molecular profiles (e.g.,
transcriptomes) in a wide range of biological systems.
  Here, we introduce CellTreeQM, a novel deep learning method based on
transformer architectures that learns an embedding space with geometric
properties optimized for tree-graph inference. By formulating lineage
reconstruction as a tree-metric learning problem, we have systematically
explored supervised, weakly supervised, and unsupervised training settings and
present a Lineage Reconstruction Benchmark to facilitate comprehensive
evaluation of our learning method. We benchmarked the method on (1) synthetic
data modeled via Brownian motion with independent noise and spurious signals
and (2) lineage-resolved single-cell RNA sequencing datasets. Experimental
results show that CellTreeQM recovers lineage structures with minimal
supervision and limited data, offering a scalable framework for uncovering cell
lineage relationships in challenging animal models. To our knowledge, this is
the first method to cast cell lineage inference explicitly as a metric learning
task, paving the way for future computational models aimed at uncovering the
molecular dynamics of cell lineage.

Reconstructing Cell Lineage Trees from Phenotypic Features with
Metric Learning
Da Kuang1, Guanwen Qiu1, Junhyong Kim1,2
1Department of Computer and Information Science, University of Pennsylvania, Philadelphia, USA
2Department of Biology, University of Pennsylvania, Philadelphia, USA
kuangda@seas.upenn.edu, guanwenq@seas.upenn.edu, junhyong@sas.upenn.edu
Abstract
How a single fertilized cell gives rise to a complex array of specialized cell types in development is a
central question in biology. The cells grow, divide, and acquire differentiated characteristics through poorly
understood molecular processes. A key approach to studying developmental processes is to infer the tree
graph of cell lineage division and differentiation histories, providing an analytical framework for dissecting
individual cells’ molecular decisions during replication and differentiation. Although genetically engineered
lineage-tracing methods have advanced the field, they are either infeasible or ethically constrained in
many organisms. In contrast, modern single-cell technologies can measure high-content molecular profiles
(e.g., transcriptomes) in a wide range of biological systems.
Here, we introduce CellTreeQM, a novel deep learning method based on transformer architectures that
learns an embedding space with geometric properties optimized for tree-graph inference. By formulating
lineage reconstruction as a tree-metric learning problem, we have systematically explored supervised, weakly
supervised, and unsupervised training settings and present a Cell Lineage Reconstruction Benchmark to
facilitate comprehensive evaluation of our learning method. We benchmarked the method on (1) synthetic
data modeled via Brownian motion with independent noise and spurious signals and (2) lineage-resolved
single-cell RNA sequencing datasets.
Experimental results show that CellTreeQM recovers lineage
structures with minimal supervision and limited data, offering a scalable framework for uncovering cell
lineage relationships in challenging animal models. To our knowledge, this is the first method to cast cell
lineage inference explicitly as a metric learning task, paving the way for future computational models
aimed at uncovering the molecular dynamics of cell lineage.
1
Introduction
Understanding how the single cell of a fertilized egg repeatedly divides and differentiates, i.e., the process of
different cells acquiring unique characteristics (e.g., skin cell, muscle cell), to give rise to a fully formed animal
has been a long-standing goal in biology [Wolpert et al., 2015, Slack and Dale, 2021]. A key component of
this developmental process is the tree-graph of cell lineages, which provides a roadmap of how diverse cell
types arise from a single progenitor [Clevers, 2011, Shapiro et al., 2013, Wagner and Klein, 2020]. The cell
replication process cannot be directly observed for most organisms; therefore, inferring or reconstructing the
cell lineage tree from features measured on the individual cells is a fundamental problem in developmental biology.
Traditional methods for reconstructing lineage trees involve either genetic lineage tracing, which
involves inserting genetic markers into the cells and tracing their movement over time [Wu et al., 2013,
Crawford et al., 2017], or the integration of lineage-tracing data with statistical models [Kuhn et al.,
2017]. While genetic lineage tracing provides a direct and precise method of tracking cell lineage, it is either
infeas

[Sample 6]
========================================
Title: Numerical proof-of-concept of a photon, proton, and positron laser-driven source with nanostructured targets

Abstract: A source of high-energy photons, ions, and positrons can be attained with the
interaction of ultra-intense femtosecond laser pulses with advanced
nanostructured targets. We present and characterise a numerical model that
mimics the foam deposition process on solid substrates, as it occurs in
Double-Layer Target (DLT) manufacturing. The model is integrated into
Particle-In-Cell (PIC) simulations in full 3D geometry to study electron
acceleration, consequent high-energy photon emission, proton acceleration, and
pair production with realistic target and laser parameters. We highlight the
importance of realistic foam morphology modelling even at high-laser intensity
and the need for specific optimisation of target parameters with realistic PIC
simulations to improve radiation production efficiency. Our study shows that
the DLT could be a compact multi-purpose scheme to achieve high-brightness
photons and high-energy protons and to observe and optimise non-linear
Breit-Wheeler pair production.

NUMERICAL PROOF-OF-CONCEPT OF A PHOTON, PROTON, AND
POSITRON LASER-DRIVEN SOURCE WITH NANOSTRUCTURED
TARGETS
Marta Galbiati
Laboratoire pour l’Utilisation des Lasers Intenses
École Polytechnique, CNRS, CEA, Sorbonne Université, Institut Polytechnique de Paris
Palaiseau, France
Department of Energy, Politecnico di Milano
Milano, Italy
marta.galbiati@polytechnique.edu
Kevin Ambrogioni
Department of Energy, Politecnico di Milano
Milano, Italy
kevin.ambrogioni@polimi.it
Leonardo Francesco Claudio Monaco, Maria Sole Galli De Magistris, Davide Orecchia
Francesco Mirani, Alessandro Maffini, Matteo Passoni
Department of Energy, Politecnico di Milano
Milano, Italy
ABSTRACT
A source of high-energy photons, ions, and positrons can be attained with the interaction of ultra-
intense femtosecond laser pulses with advanced nanostructured targets. We present and characterise
a numerical model that mimics the foam deposition process on solid substrates, as it occurs in
Double-Layer Target (DLT) manufacturing. The model is integrated into Particle-In-Cell (PIC)
simulations in full 3D geometry to study electron acceleration, consequent high-energy photon
emission, proton acceleration, and pair production with realistic target and laser parameters. We
highlight the importance of realistic foam morphology modelling even at high-laser intensity and the
need for specific optimisation of target parameters with realistic PIC simulations to improve radiation
production efficiency. Our study shows that the DLT could be a compact multi-purpose scheme to
achieve high-brightness photons and high-energy protons and to observe and optimise non-linear
Breit-Wheeler pair production.
Keywords Ultra-intense Laser-Plasma interaction · Nanostructured Foam · Non-linear Inverse Compton Scattering ·
Non-linear Breit-Wheeler Pair Production
1
Introduction
The target design is a key step in the realization of laser-driven radiation sources at ultra-high intensity (I ≥1018
W/cm2, a0 = eE0/(meωc) ≥1 where E0, e and me, ω, and c are the electric field peak value, electron charge and
mass, laser frequency, and speed of light, respectively). Standard target schemes - solids or gases - represent a robust
methodology for the investigation of laser-driven particle sources of electrons, ions, high-energy photons and positrons
[1, 2, 3, 4, 5, 6, 7, 8, 9]. Instead, complex target schemes can pose relevant theoretical modelling and experiment
arXiv:2503.21630v1  [physics.plasm-ph]  27 Mar 2025

preparation - production and handling - challenges. Still, they provide the means to enable and enhance particle
acceleration and generation and possibly control these processes [10, 11].
For instance, Double-Layer Targets (DLTs) that combine a low-density layer with a solid-density one have shown
significant promise in enhancing ion acceleration [12, 13, 14, 15] while also allowing for direct or indirect production
of various types of radiative particles [16, 17]. This potential is currently being explored in a variety of experimental
schemes, e.g., the EPLASMA laser [18], the PION-XII laser [19], the MARTINI laser [20, 21], and the FLEX laser [22, 23, 24].
These lasers are all of ultra-high intensity, with peak powers >1018 W/cm2 and peak powers >1019 W/cm

[Sample 7]
========================================
Title: Intermolecular Radiative Decay: A non-local decay mechanism providing an insider's view of the solvation shell

Abstract: Aqueous solutions are crucial in chemistry, biology, environmental science,
and technology. The chemistry of solutes is influenced by the surrounding
solvation shell of water molecules, which have different chemical properties
than bulk water due to their different electronic and geometric structure. It
is an experimental challenge to selectively investigate this
property-determining electronic and geometric structure.
  Here, we report experimental results on a novel non-local X-ray emission
process, Intermolecular Radiative Decay (IRD), for the prototypical ions
Na$^{+}$ and Mg$^{2+}$ in water. We show that, in IRD, an electron from the
solvation shell fills a core hole in the solute, and the released energy is
emitted as an X-ray photon. We analyze the underlying mechanism using
theoretical calculations, and show how IRD will allow us to meet the challenge
of chemically selective probing of solvation shells from within.

Intermolecular Radiative Decay: A non-local
decay mechanism providing an insider’s view of
the solvation shell
Johan S¨oderstr¨om1, Lucas M. Cornetta2, Victor Ekholm3,
Vincenzo Carravetta4, Arnaldo Naves de Brito5, Ricardo Marinho6,
Marcus Ag˚aker1,3, Takashi Tokushima3, Conny S˚athe3, Anirudha
Ghosh3, Dana Bloß7, Andreas Hans7, Florian Trinter8, Iyas
Ismail9, Debora Vasconcelos1, Joel Pinheiro1, Yi-Ping Chang10,
Manuel Harder10, Zhong Yin11, Joseph Nordgren1, Gunnar
¨Ohrwall3, Hans ˚Agren1, Jan-Erik Rubensson1, Olle Bj¨orneholm1
1Department of Physics and Astronomy, Uppsala University, Uppsala,
Sweden.
2Instituto de F´ısica, Universidade de S˜ao Paulo, S˜ao Paulo, Brazil.
3MAX IV Laboratory, Lund University, Lund, Sweden.
4Institute of Chemical and Physical Processes, CNR-IPCF, Pisa, Italy.
5Institute of Physics Gleb Wataghin, State University of Campinas,
Campinas, Brazil.
6Department of Physics, University of Brasilia, Brasilia, Brazil.
7Institute of Physics, University of Kassel, Kassel, Germany.
8Molecular Physics, Fritz-Haber-Institut der Max-Planck-Gesellschaft,
Berlin, Germany.
9Sorbonne Universit´e, CNRS, Laboratoire de Chimie Physique –
Mati`ere et Rayonnement, LCPMR, F-75005 Paris, France.
10European XFEL, 22689 Schenefeld, Germany.
11International Center for Synchrotron Radiation Innovation Smart,
Tohoku University, 980-8577 Sendai, Japan.
Contributing authors: Johan.Soderstrom@physics.uu.se;
Olle.Bjorneholm@physics.uu.se;
1
arXiv:2503.19435v1  [physics.chem-ph]  25 Mar 2025

Abstract
Aqueous solutions are crucial in chemistry, biology, environmental science, and
technology. The chemistry of solutes is influenced by the surrounding solvation
shell of water molecules, which have different chemical properties than bulk water
due to their different electronic and geometric structure. It is an experimen-
tal challenge to selectively investigate this property-determining electronic and
geometric structure.
Here, we report experimental results on a novel non-local X-ray emission process,
Intermolecular Radiative Decay (IRD), for the prototypical ions Na+ and Mg2+
in water. We show that, in IRD, an electron from the solvation shell fills a core
hole in the solute, and the released energy is emitted as an X-ray photon. We
analyze the underlying mechanism using theoretical calculations, and show how
IRD will allow us to meet the challenge of chemically selective probing of solvation
shells from within.
Keywords: Aqueous Solution, Intermolecular Radiative Decay, Solvation Shell
Introduction
The fundamental importance of water in chemical reactions is intricately linked to the
behavior of water molecules in the immediate vicinity of the reactants. Water molecules
are constantly in motion, and their motion is governed by the fundamental laws of
statics and dynamics. In aqueous solution, water molecules are not free to move as
independently as in bulk water. Rather, they are confined to a 3D region in space,
a region that is called the solvation shell. The solvation shell is composed of a
sequence of water molecules that surround a solute particle (1). The solvation shell


[Sample 8]
========================================
Title: Kinetics of seeded protein aggregation: theory and application

Abstract: ``Seeding'' is the addition of preformed fibrils to a solution of monomeric
protein to accelerate its aggregation into new fibrils. It is a versatile and
widely-used tool for scientists studying protein aggregation kinetics, as it
enables the isolation and separate study of discrete reaction steps
contributing to protein aggregation, specifically elongation and secondary
nucleation. However, the seeding levels required to achieve dominating effects
on each of these steps separately have been established largely by
trial-and-error, due in part to the lack of availability of integrated rate
laws valid for moderate to high seeding levels and generally applicable to all
common underlying reaction mechanisms. Here, we improve on a recently developed
mathematical method based on Lie symmetries for solving differential equations,
and with it derive such an integrated rate law. We subsequently develop simple
expressions for the amounts of seed required to isolate each step. We
rationalize the empirical observation that fibril seeds must often be broken up
into small pieces to successfully isolate elongation. We also derive
expressions for average fibril lengths at different times in the aggregation
reaction, and explore different methods to break up fibrils. This paper will
provide an invaluable reference for future experimental and theoretical studies
in which seeding techniques are employed, and should enable more sophisticated
analyses than have been performed to date.

Kinetics of seeded protein aggregation: theory and application
Alexander J. Dear,1, 2, 3, ∗Georg Meisl,4 Jing Hu,5, 1 Tuomas P. J. Knowles,4, 6 and Sara Linse1, †
1Biochemistry and Structural Biology, Lund University, Lund, Sweden
2Department of Biology, Institute of Biochemistry,
ETH Zurich, Otto Stern Weg 3, 8093, Zurich, Switzerland
3Bringing Materials to Life Initiative, ETH Zurich, Switzerland
4Centre for Misfolding Diseases, Department of Chemistry,
University of Cambridge, Lensfield Road, Cambridge CB2 1EW, United Kingdom
5Division of Physical Chemistry, Department of Chemistry, Lund University, Lund, Sweden
6Cavendish Laboratory, University of Cambridge,
J J Thomson Avenue, CB3 0HE, United Kingdom
(Dated: March 28, 2025)
“Seeding” is the addition of preformed fibrils to a solution of monomeric protein to accelerate
its aggregation into new fibrils. It is a versatile and widely-used tool for scientists studying pro-
tein aggregation kinetics, as it enables the isolation and separate study of discrete reaction steps
contributing to protein aggregation, specifically elongation and secondary nucleation. However, the
seeding levels required to achieve dominating effects on each of these steps separately have been
established largely by trial-and-error, due in part to the lack of availability of integrated rate laws
valid for moderate to high seeding levels and generally applicable to all common underlying reac-
tion mechanisms. Here, we improve on a recently developed mathematical method based on Lie
symmetries for solving differential equations, and with it derive such an integrated rate law. We
subsequently develop simple expressions for the amounts of seed required to isolate each step. We
rationalize the empirical observation that fibril seeds must often be broken up into small pieces
to successfully isolate elongation. We also derive expressions for average fibril lengths at different
times in the aggregation reaction, and explore different methods to break up fibrils. This paper
will provide an invaluable reference for future experimental and theoretical studies in which seeding
techniques are employed, and should enable more sophisticated analyses than have been performed
to date.
I.
INTRODUCTION
Formation of amyloid fibrils from monomeric protein is
a heavily-studied phenomenon that, when uncontrolled,
drives a wide range of diseases in humans and animals [1–
4]. When tightly regulated, it also plays functional roles
in biology such as the formation of bacterial biofilms [5–
8], and is beginning to be used in the construction of
functional nanomaterials in industry [9–11].
A vital step toward preventing amyloid formation in
disease, and toward the deliberate design of amyloid-
based materials in industry, is to understand the molecu-
lar driving forces for amyloid formation as well as the un-
derlying reaction mechanisms of the processes, and how
these change from protein to protein and as a function of
the reaction conditions. This can be achieved by coarse-
grained kinetic modelling of experimental data, an ap-
proach which has met with much success in the past
decade [12–15].
The most common method for studying protein
aggregation kinetics is the “seeding” technique.
This involves adding preformed fibrils of a monomeric
protein to a solution containing only the monomer
and monitoring the rate of monomer incorporation into
the fibrils. This method is valuable because it allows
for the study of discrete steps in the aggregation
process, namely, elongation and secondary nuclea-
t

[Sample 9]
========================================
Title: Logging the conformal life of Ramanujan's $π$

Abstract: In 1914, Ramanujan presented 17 infinite series for $1/\pi$. We examine the
physics origin of these remarkable formulae by connecting them to 2D
logarithmic conformal field theories (LCFTs) which arise in various contexts
such as the fractional quantum hall effect, percolation and polymers. In light
of the LCFT connection, we investigate such infinite series in terms of the
physics data, i.e., the operator spectrum and OPE coefficients of the CFT and
the conformal block expansion. These considerations lead to novel
approximations for $1/\pi$. The rapid convergence of the Ramanujan series
motivates us to take advantage of the crossing symmetry of the LCFT correlators
to find new and efficient representations. To achieve this, we use the
parametric crossing symmetric dispersion relation which was recently developed
for string amplitudes. Quite strikingly, we find remarkable simplifications in
the new representations, where, in the Legendre relation, the entire
contribution to $1/\pi$ comes from the logarithmic identity operator, hinting
at a universal property of LCFTs. Additionally, the dispersive representation
gives us a new handle on the double-lightcone limit.

Logging the conformal life of Ramanujan’s π
Faizan Bhata∗and Aninda Sinhaa,b†
aCentre for High Energy Physics,
Indian Institute of Science,
C.V. Raman Avenue, Bangalore 560012, India.
bDepartment of Physics and Astronomy,
University of Calgary,
Alberta T2N 1N4, Canada.
(Dated: March 28, 2025)
In 1914, Ramanujan presented 17 infinite series for 1/π. We examine the physics origin of these
remarkable formulae by connecting them to 2D logarithmic conformal field theories (LCFTs) which
arise in various contexts such as the fractional quantum hall effect, percolation and polymers. In
light of the LCFT connection, we investigate such infinite series in terms of the physics data, i.e.,
the operator spectrum and OPE coefficients of the CFT and the conformal block expansion. These
considerations lead to novel approximations for 1/π.
The rapid convergence of the Ramanujan
series motivates us to take advantage of the crossing symmetry of the LCFT correlators to find new
and efficient representations. To achieve this, we use the parametric crossing symmetric dispersion
relation which was recently developed for string amplitudes. Quite strikingly, we find remarkable
simplifications in the new representations, where, in the Legendre relation, the entire contribution
to 1/π comes from the logarithmic identity operator, hinting at a universal property of LCFTs.
Additionally, the dispersive representation gives us a new handle on the double-lightcone limit.
Introduction: In 1914, Ramanujan [1] recorded 17
remarkable formulae for π of the form
∞
X
n=0
( 1
2)n(σ)n(1 −σ)n
n!3
(a + bn)zn = 1
π.
(1)
Here, σ ∈{ 1
2, 1
3, 1
4, 1
6}, while z, b, and a are algebraic
numbers. In this work, we show that the origin of these
formulae finds a natural place in the study of four-point
correlators in certain 2D logarithmic conformal field the-
ories (LCFTs) [2–4]. Specifically, we find that σ corre-
sponds to the scaling dimension of the external opera-
tor, z is identified with the conformal cross-ratio, and
n repackages the conformal data i.e.
the operator di-
mensions and spins. Thus, one can explicitly specify the
conformal data that make up Ramanujan’s 1/π! Further-
more, our investigation into the intricate mathematics
behind these expressions reveals some universal proper-
ties of LCFTs.
The correlators of interest turn out to be those of twist
operators in the well-studied c = −2 LCFT [2, 3, 5–8]
with conformal weight h = σ(σ −1)/2. This LCFT is
essentially the theory of a pair of symplectic fermions.
It has a global SL(2, C) symmetry which can be quo-
tized by a reflection of a plane containing the origin.
The corresponding two-point correlators, which arise
when the two fermions are brought close together,
have a simple structure. Specifically, they are the
cross-product of the two-point spinors of the two fermions,
which are denoted by S and S’, respectively. In this
work, we show that these correlators can be expressed in
terms of the log

[Sample 10]
========================================
Title: Bosonic quantum error correction with microwave cavities for quantum repeaters

Abstract: Long-distance quantum communication necessitates the use of quantum
repeaters, which typically include highly coherent quantum memories. We provide
a theoretical analysis of the secret key rates for a quantum repeater system
incorporating bosonic error correction and memory components. Specifically, we
focus on the application of Binomial codes for two repeater segments. Using
these codes, our investigation aims to suppress memory loss errors that
commonly affect systems such as atoms and microwave cavities, in contrast to
dephasing errors in single-spin memories. We further discuss a physical
implementation of such a quantum repeater comprising a microwave cavity and a
superconducting transmon, capable of state engineering with high fidelities
($>97\%$) and logical Bell state measurements for successful entanglement
swapping. As an alternative approach, we also discuss a realization in the
all-optical domain.

Bosonic quantum error correction with microwave cavities for quantum repeaters
S. Siddardha Chelluri,1, ∗Sanchar Sharma,2 Frank Schmidt,1 Silvia Viola Kusminskiy,3, 4 and Peter van Loock1, †
1Institute of Physics, Johannes-Gutenberg University of Mainz, Staudingerweg 7, 55128 Mainz, Germany
2Laboratoire de Physique de l’École Normale Supérieure, ENS, Université PSL,
CNRS, Sorbonne Université, Université de Paris, F-75005 Paris, France
3Institute for Theoretical Solid State Physics, RWTH Aachen University, 52074 Aachen, Germany
4Max Planck Institute for the Science of Light, Staudtstraße 2, 91058 Erlangen, Germany
(Dated: March 28, 2025)
Long-distance quantum communication necessitates the use of quantum repeaters, which typically
include highly coherent quantum memories. We provide a theoretical analysis of the secret key rates
for a quantum repeater system incorporating bosonic error correction and memory components.
Specifically, we focus on the application of Binomial codes for two repeater segments. Using these
codes, our investigation aims to suppress memory loss errors that commonly affect systems such as
atoms and microwave cavities, in contrast to dephasing errors in single-spin memories. We further
discuss a physical implementation of such a quantum repeater comprising a microwave cavity and
a superconducting transmon, capable of state engineering with high fidelities (> 97%) and logical
Bell state measurements for successful entanglement swapping. As an alternative approach, we also
discuss a realization in the all-optical domain.
I.
INTRODUCTION
The distribution of entanglement over long distances
and between multiple parties, forming quantum net-
works, enables numerous applications such as the quan-
tum Internet [1], uncompromised secure communication
through the establishment of private keys [2, 3], dis-
tributed quantum computing [4] and blind quantum com-
puting [5]. However, the fiber channel transmittivity η
decreases exponentially with distance between parties,
making long-distance quantum communication infeasi-
ble. This limitation can be addressed by the use of quan-
tum repeaters [6] in which a channel is divided into mul-
tiple, say n, segments. With the addition of quantum
memories, this division effectively increases the channel
attenuation length by a factor of n, thus enhancing the
transmittivity to η′ = η1/n.
Quantum repeaters are typically classified into three
generations based on their physical implementations [7]:
the first relies on memories and entanglement distilla-
tion, the second on memories and error correction, and
the third exclusively on error correction. As memories
are crucial for the first two generations, various experi-
mental platforms were explored as implementations, such
as atomic ensembles [8], color centers in diamonds [9, 10],
and quantum dots [11]. Due to decoherence in the mem-
ories, building quantum repeaters remains a significant
challenge.
Depending on the platform, either dephas-
ing or loss dominates the decoherence.
In the latter
case, since the primary imperfection in the entire re-
peater system is excitation loss — both in the optical
channel and the memory node — it is crucial to inves-
tigate the impact of memory loss [12]. While dephasing
∗schellur@uni-mainz.de
† loock@uni-mainz.de
in memories is well-studied, the effects of memory loss
on the repeater system are less explored. The
goal of our work is to investigate the impact of mem-
ory loss in a quantum repeater system, and to provide a
theoretical analysis of the secret key rates for such a
system.
The first generation of quantum repeaters
relies on the use of memories, typically based on
spin-based quantum systems. For example, atomic en-
semb

[Sample 11]
========================================
Title: DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data

Abstract: Backdoor attacks are among the most effective, practical, and stealthy
attacks in deep learning. In this paper, we consider a practical scenario where
a developer obtains a deep model from a third party and uses it as part of a
safety-critical system. The developer wants to inspect the model for potential
backdoors prior to system deployment. We find that most existing detection
techniques make assumptions that are not applicable to this scenario. In this
paper, we present a novel framework for detecting backdoors under realistic
restrictions. We generate candidate triggers by deductively searching over the
space of possible triggers. We construct and optimize a smoothed version of
Attack Success Rate as our search objective. Starting from a broad class of
template attacks and just using the forward pass of a deep model, we reverse
engineer the backdoor attack. We conduct extensive evaluation on a wide range
of attacks, models, and datasets, with our technique performing almost
perfectly across these settings.

DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep
Models with Limited Data
Dorde Popovic†, Amin Sadeghi†, Ting Yu*, Sanjay Chawla†, Issa Khalil†
†Qatar Computing Research Institute, Hamad Bin Khalifa University
*Mohamed bin Zayed University of Artificial Intelligence
Abstract
Backdoor attacks are among the most effective, practical, and
stealthy attacks in deep learning. In this paper, we consider
a practical scenario where a developer obtains a deep model
from a third party and uses it as part of a safety-critical sys-
tem. The developer wants to inspect the model for potential
backdoors prior to system deployment. We find that most
existing detection techniques make assumptions that are not
applicable to this scenario. In this paper, we present a novel
framework for detecting backdoors under realistic restrictions.
We generate candidate triggers by deductively searching over
the space of possible triggers. We construct and optimize a
smoothed version of Attack Success Rate as our search ob-
jective. Starting from a broad class of template attacks and
just using the forward pass of a deep model, we reverse en-
gineer the backdoor attack. We conduct extensive evaluation
on a wide range of attacks, models, and datasets, with our
technique performing almost perfectly across these settings.
1
Introduction
Practical systems such as self-driving cars [7], medical de-
vices [22], and facial recognition systems [34] are increas-
ingly relying on deep models to make critical decisions.
It is shown that deep models are prone to a range of at-
tacks [5,40]. Among these attacks, backdoor attacks are espe-
cially stealthy [27] and effective [12,49].
A backdoor attack injects a hidden trigger into a victim
model. When the model is given an input that contains this
trigger, the backdoor is activated and the model acts mali-
ciously. If this trigger is absent in the input, the backdoor is
not activated and the model behaves as expected. The attacker
specifies the trigger type and the expected malicious behavior.
Figure 1 illustrates a few different trigger types for a traffic
sign classifier. In this example, the trigger could be a specific
QR code. If the victim model encounters a stop sign contain-
ing this trigger, the model will misclassify it to a minimum
speed sign.
In this paper, we focus on a common scenario where a
developer intends to utilize a deep model as part of a larger
system. We assume that the developer obtains the model from
a third party that might not be trusted (e.g. open-source reposi-
tory [32], provider company [52]). Since the model will make
important decisions in the system, the developer wants to
verify whether the model contains a backdoor. Under this
scenario, we make a few assumptions:
• Pre-deployment: The developer has to verify the model
before the system is deployed to the users. Due to safety
and regulatory measures, the developer cannot wait until
some user is compromised by the attacker.
• Data-limited: The third party does not share the dataset
that is used to train the model. However, the developer
can obtain a small set of clean samples to run the model.
• Single-instance: The third party only shares one in-
stance of the model. The developer does not have access,
capabilities, or resources to train alternative clean and
backdoored versions of the model.
• Black-box: The developer can run the model on any
inputs without knowing the exact internal workings of the
model.
• Limited resources: The developer has limited time and
resources to evaluate the model.
• Limited access: The developer does not have access to
the original training data or the source code of the model.
• No developer knowledge: The developer is not a
deep learning expert and has no knowledge about
the internals of the model.
Given these assumptions, we can see that current detection
techniques are not suitable for

[Sample 12]
========================================
Title: Higher-spin symmetry in the $\mathfrak{sl}_3$ boundary Toda conformal field theory II: Singular vectors and BPZ equations

Abstract: This article is the second chapter of a two-part series dedicated to the
mathematical study of the higher-spin symmetry enjoyed by the $\mathfrak{sl}_3$
boundary Toda Conformal Field Theory. Namely, based on a probabilistic
definition of this model and building on the framework introduced in the first
article of this series, we compute some singular vectors of the theory which at
the level of correlation functions give rise to higher equations of motion.
Under additional assumptions these become BPZ-type differential equations
satisfied by the correlation functions, a key feature in the perspective of a
rigorous derivation of the structure constants of the theory. Such equations of
motion and differential equations were previously unknown in the physics
literature.

arXiv:2503.20548v1  [math.PR]  26 Mar 2025
HIGHER-SPIN SYMMETRY IN THE sl3 BOUNDARY TODA
CONFORMAL FIELD THEORY II: SINGULAR VECTORS AND BPZ
EQUATIONS
BAPTISTE CERCL´E AND NATHAN HUGUENIN
Abstract. This article is the second chapter of a two-part series dedicated to the math-
ematical study of the higher-spin symmetry enjoyed by the sl3 boundary Toda Conformal
Field Theory. Namely, based on a probabilistic deﬁnition of this model and building on the
framework introduced in the ﬁrst article of this series, we compute some singular vectors
of the theory which at the level of correlation functions give rise to higher equations of mo-
tion. Under additional assumptions these become BPZ-type diﬀerential equations satisﬁed
by the correlation functions, a key feature in the perspective of a rigorous derivation of the
structure constants of the theory. Such equations of motion and diﬀerential equations were
previously unknown in the physics literature.
Contents
1.
Introduction
2
1.1.
Boundary Toda CFT and its symmetries
2
1.2.
Singular vectors and representation theory of W-algebras
5
1.3.
Diﬀerential equations for the correlation functions
8
2.
Background and preliminary reminders
11
2.1.
Probabilistic deﬁnition of the correlation functions
11
2.2.
Construction of the descendant ﬁelds
13
2.3.
Method for deﬁning the singular and null vectors
18
3.
Singular vectors and higher equations of motion: levels one and two
19
3.1.
Singular vector at the level one
19
3.2.
Descendants and singular vectors at the level two
21
4.
Singular vectors at the level three
29
4.1.
Descendants at the third order
29
4.2.
Singular vector at the level three
42
Appendix A.
Technical estimates and auxiliary computations
52
A.1.
Technical estimates
52
A.2.
Auxiliary computations
53
References
59
1

2
BAPTISTE CERCL´E AND NATHAN HUGUENIN
1. Introduction
1.1. Boundary Toda CFT and its symmetries.
1.1.1. Liouville and Toda conformal ﬁeld theories. In the context of two-dimensional Con-
formal Field Theory (CFT hereafter), Liouville theory [30] is a fundamental instance of a
model enjoying Virasoro symmetry and for which the conformal bootstrap procedure —as
introduced in the pioneering work of Belavin-Polyakov-Zamolodchikov (BPZ) [5]— can be
implemented. This systematic method is based on the study of the symmetries of the model
in the form of representation theory of the Virasoro algebra, which in turn put strong con-
straints on the correlation functions of a CFT and provides a general method for solving a
theory that enjoys conformal symmetry. The ﬁrst stage of this program, which is the com-
putation of basic correlation functions of the theory named structure constants, has been
carried out for Liouville theory in [16, 37] for the case of the sphere, and in [18, 31, 27] for the
upper half-plane.
In the context of CFTs with boundary, the concept of conformal symmetry is extended
to include the boundary condition. The ﬁrst papers on conformal ﬁeld theories with a boundary
were published in [6, 21] and [8, 22]. In the latter case, a two-dimensional CFT
with a boundary is studied, and a method for deﬁning the conformal structure constants is
presented. The

[Sample 13]
========================================
Title: Statistical learning of structure-property relationships for transport in porous media, using hybrid AI modeling

Abstract: The 3D microstructure of porous media, such as electrodes in lithium-ion
batteries or fiber-based materials, significantly impacts the resulting
macroscopic properties, including effective diffusivity or permeability.
Consequently, quantitative structure-property relationships, which link
structural descriptors of 3D microstructures such as porosity or geodesic
tortuosity to effective transport properties, are crucial for further
optimizing the performance of porous media. To overcome the limitations of 3D
imaging, parametric stochastic 3D microstructure modeling is a powerful tool to
generate many virtual but realistic structures at the cost of computer
simulations. The present paper uses 90,000 virtually generated 3D
microstructures of porous media derived from literature by systematically
varying parameters of stochastic 3D microstructure models. Previously, this
data set has been used to establish quantitative microstructure-property
relationships. The present paper extends these findings by applying a hybrid AI
framework to this data set. More precisely, symbolic regression, powered by
deep neural networks, genetic algorithms, and graph attention networks, is used
to derive precise and robust analytical equations. These equations model the
relationships between structural descriptors and effective transport properties
without requiring manual specification of the underlying functional
relationship. By integrating AI with traditional computational methods, the
hybrid AI framework not only generates predictive equations but also enhances
conventional modeling approaches by capturing relationships influenced by
specific microstructural features traditionally underrepresented. Thus, this
paper significantly advances the predictive modeling capabilities in materials
science, offering vital insights for designing and optimizing new materials
with tailored transport properties.

Statistical learning of structure-property relationships for transport in
porous media, using hybrid AI modeling
Somayeh Hosseinhashemia,∗, Philipp Riederb, Orkun Furatb, Benedikt Priflingb, Changlin Wu a,
Christoph Thona, Volker Schmidtb, Carsten Schildea
March 28, 2025
aInstitute of Particle Technology, Technical University of Braunschweig, Franz-Liszt Straße 35A,
38104 Braunschweig, Germany
bInstitute of Stochastics, Ulm University, Helmholtzstraße 18, 89069 Ulm, Germany
Abstract
The 3D microstructure of porous media, such as electrodes in lithium-ion batteries and solid-oxide fuel cells or
fiber-based materials, significantly impacts the resulting macroscopic properties, including effective diffusivity or
permeability. Consequently, quantitative structure-property relationships, which link structural descriptors of 3D
microstructures such as porosity, specific surface area, or geodesic tortuosity to effective transport properties,
are crucial for further optimizing the performance of porous media. To overcome the limitations of 3D imaging,
which is time-consuming and costly, parametric stochastic 3D microstructure modeling, which uses tools from
stochastic geometry, is a powerful tool to generate many virtual but realistic structures at the cost of computer
simulations. The present paper uses 90,000 virtually generated 3D microstructures, consisting of a solid phase
and the pore space, derived from literature by systematically varying parameters of stochastic 3D microstructure
models. Previously, this data set has been used to establish quantitative microstructure-property relationships
utilizing analytical regression equations, artificial neural networks, and convolution neural networks. The present
paper extends these findings by applying a hybrid AI framework to this data set. More precisely, symbolic re-
gression, powered by deep neural networks, genetic algorithms, and graph attention networks, is used to derive
precise and robust analytical equations. These equations model the relationships between structural descriptors
and effective transport properties without requiring manual specification of the underlying functional relation-
ship. By integrating AI with traditional computational methods, the hybrid AI framework not only generates
predictive equations but also enhances conventional modeling approaches by capturing relationships influenced
by specific microstructural features traditionally underrepresented. Thus, this paper significantly advances the
predictive modeling capabilities in materials science and process engineering, offering vital insights for designing
and optimizing new materials with tailored transport properties.
Keywords: Porous material, Mass transport, Microstructure analysis, Stochastic 3D model, Predictive modeling,
Hybrid AI integration, Deep learning, Convolutional attention network
1
Introduction
The 3D microstructure of various porous functional materials such as solar cells and electrodes in lithium-ion batteries
or fuel cells significantly influences the resulting macroscopic properties such as effective diffusivity and permeability.
This relationship has been well established in the literature, and quantitative structure-property relationships (QSPR)
[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27

[Sample 14]
========================================
Title: Matrix Factorization for Inferring Associations and Missing Links

Abstract: Missing link prediction is a method for network analysis, with applications
in recommender systems, biology, social sciences, cybersecurity, information
retrieval, and Artificial Intelligence (AI) reasoning in Knowledge Graphs.
Missing link prediction identifies unseen but potentially existing connections
in a network by analyzing the observed patterns and relationships. In
proliferation detection, this supports efforts to identify and characterize
attempts by state and non-state actors to acquire nuclear weapons or associated
technology - a notoriously challenging but vital mission for global security.
Dimensionality reduction techniques like Non-Negative Matrix Factorization
(NMF) and Logistic Matrix Factorization (LMF) are effective but require
selection of the matrix rank parameter, that is, of the number of hidden
features, k, to avoid over/under-fitting. We introduce novel Weighted (WNMFk),
Boolean (BNMFk), and Recommender (RNMFk) matrix factorization methods, along
with ensemble variants incorporating logistic factorization, for link
prediction. Our methods integrate automatic model determination for rank
estimation by evaluating stability and accuracy using a modified bootstrap
methodology and uncertainty quantification (UQ), assessing prediction
reliability under random perturbations. We incorporate Otsu threshold selection
and k-means clustering for Boolean matrix factorization, comparing them to
coordinate descent-based Boolean thresholding. Our experiments highlight the
impact of rank k selection, evaluate model performance under varying test-set
sizes, and demonstrate the benefits of UQ for reliable predictions using
abstention. We validate our methods on three synthetic datasets (Boolean and
uniformly distributed) and benchmark them against LMF and symmetric LMF
(symLMF) on five real-world protein-protein interaction networks, showcasing an
improved prediction performance.

Matrix Factorization for Inferring Associations and Missing Links
RYAN BARRON∗, Theoretical Division, Los Alamos National Laboratory, USA
MAKSIM E. EREN∗, Information Systems and Modeling, Los Alamos National Laboratory, USA
DUC P. TRUONG∗, Theoretical Division, Los Alamos National Laboratory, USA
CYNTHIA MATUSZEK, Department of Computer Science and Electrical Engineering, University of Maryland,
Baltimore County, USA
JAMES WENDELBERGER, Computer, Computational, and Statistical Sciences, Los Alamos National Laboratory,
USA
MARY F. DORN, Computer, Computational, and Statistical Sciences, Los Alamos National Laboratory, USA
BOIAN ALEXANDROV, Theoretical Division, Los Alamos National Laboratory, USA
Missing link prediction is a method for network analysis, with applications in recommender systems, biology, social sciences,
cybersecurity, information retrieval, and Artificial Intelligence (AI) reasoning in Knowledge Graphs. Missing link prediction identifies
unseen but potentially existing connections in a network by analyzing the observed patterns and relationships. In proliferation
detection, this supports efforts to identify and characterize attempts by state and non-state actors to acquire nuclear weapons or
associated technology - a notoriously challenging but vital mission for global security. Dimensionality reduction techniques like
Non-Negative Matrix Factorization (NMF) and Logistic Matrix Factorization (LMF) are effective but require selection of the matrix
rank parameter, that is, of the number of hidden features, 𝑘, to avoid over/under-fitting. We introduce novel Weighted (WNMFk),
Boolean (BNMFk), and Recommender (RNMFk) matrix factorization methods, along with ensemble variants incorporating logistic
factorization, for link prediction. Our methods integrate automatic model determination for rank estimation by evaluating stability
and accuracy using a modified bootstrap methodology and uncertainty quantification (UQ), assessing prediction reliability under
random perturbations. Additionally, we incorporate Otsu threshold selection and k-means clustering for Boolean matrix factorization,
comparing them to coordinate descent-based Boolean thresholding. Our experiments highlight the impact of rank 𝑘selection, evaluate
model performance under varying test-set sizes, and demonstrate the benefits of UQ for reliable predictions using abstention. We
validate our methods on three synthetic datasets (Boolean and Gaussian distributed) and benchmark them against LMF and symmetric
LMF (symLMF) on five real-world protein-protein interaction networks, showcasing an improved prediction performance.
CCS Concepts: • Computing methodologies →Dimensionality reduction and manifold learning; Non-negative matrix
factorization; Boosting.
∗Authors contributed equally to this research.
Authors’ Contact Information: Ryan Barron, barron@lanl.gov
Maksim Eren, meren@lanl.gov
Duc Phuong Truong, dptruong@lanl.gov
Cynthia Matuszek, matuszek@umd.edu
James Wendelberger, james.wendelberger@lanl.gov
Mary F. Dorn, mfdorn@lanl.gov
Bian Alexandrov, bian.alexandrov@lanl.gov


[Sample 15]
========================================
Title: Towards Intelligent Control of MeV Energy Electrons and Protons from kHz Repetition Rate Ultra-Intense Laser Plasma Interactions

Abstract: Ultra-intense laser-matter interactions are often difficult to predict from
first principles because of the complexity of plasma processes and the many
degrees of freedom relating to the laser and target parameters. An important
approach to controlling and optimizing ultra-intense laser interactions
involves gathering large data sets and using this data to train statistical and
machine learning models. In this paper we describe experimental efforts to
accelerate electrons and protons to $\sim$MeV energies with this goal in mind.
These experiments involve a 1 kHz repetition rate ultra-intense laser system
with $\sim$10mJ per shot, a peak intensity near $5 \times 10^{18}$ W/cm$^{2}$,
and a "liquid leaf" target. Improvements to the data acquisition capabilities
of this laser system greatly aided this investigation. Generally, we find that
the trained models were very effective for controlling the numbers of MeV
electrons ejected. The models were less successful at shifting the energy range
of ejected electrons. Simultaneous control of the numbers of $\sim$MeV
electrons and the energy range will be the subject of future experimentation
using this platform.

Intelligent Control of MeV Electrons and Protons
Towards Intelligent Control of MeV Energy Electrons and Protons from
kHz Repetition Rate Ultra-Intense Laser Plasma Interactions
Nathaniel Tamminga,1 Scott Feister,2 Kyle D. Frische,3 Ronak Desai,1 Joseph Snyder,4 John J. Felice,1 Joseph
R. Smith,5 Chris Orban,1 Enam A. Chowdhury,6 Michael L. Dexter,3 and Anil K. Patnaik3
1)Department of Physics, The Ohio State University, Columbus, OH 43210, USA
2)Department of Computer Science, California State University Channel Islands, Camarillo, CA 93012,
USA
3)Air Force Institute of Technology, Wright-Patterson AFB, OH 45433, USA
4)Department of Mathematical and Physical Sciences, Miami University, Hamilton, OH 45011,
USA
5)Department of Physics, Marietta College, Marietta, OH 43750, USA
6)Department of Materials Science, The Ohio State University, Columbus, OH 43210,
USA
(*Electronic mail: tamminga.2@osu.edu)
(Dated: 28 March 2025)
Ultra-intense laser-matter interactions are often difficult to predict from first principles because of the complexity of
plasma processes and the many degrees of freedom relating to the laser and target parameters. An important approach
to controlling and optimizing ultra-intense laser interactions involves gathering large data sets and using this data to
train statistical and machine learning models. In this paper we describe experimental efforts to accelerate electrons
and protons to ∼MeV energies with this goal in mind. These experiments involve a 1 kHz repetition rate ultra-intense
laser system with ∼10 mJ per shot, a peak intensity near 5×1018 W/cm2, and a “liquid leaf" target. Improvements to
the data acquisition capabilities of this laser system greatly aided this investigation. Generally, we find that the trained
models were very effective for controlling the numbers of MeV electrons ejected. The models were less successful at
shifting the energy range of ejected electrons. Simultaneous control of the numbers of ∼MeV electrons and the energy
range will be the subject of future experimentation using this platform.
Keywords: Machine learning; ultra-intense laser particle acceleration
I.
INTRODUCTION
As discussed in two recent reviews1,2, machine learning
(ML) methods have increasingly become a useful tool in ex-
perimental plasma physics.
While there are a number of papers where machine learn-
ing methods have been used to optimize and control elec-
tron beams via laser wakefield3–7, there has been less ex-
perimental work in optimizing and controlling MeV electron
sources from laser interactions with denser targets (exceptions
include8). This is often referred to as the direct laser acceler-
ation (DLA) regime and these interactions typically produce
a broad energy spectrum of electrons. Another research op-
portunity in ultra-intense laser science is the application of
ML methods to controlling MeV proton acceleration9,10. A
perspectives article by Palmer 11 argues that a compact and
tunable proton source like this would be desirable for a va-
riety of scientific, defense and industrial purposes. Although
energetic electrons have different practical applications than
energetic protons, the electron dynamics produce the same
plasma instabilities as the proton dynamics, and thus, understanding
how to control these instabilities is of interest to both fields.
In this work we describe a series of experiments that have
been undertaken to explore the potential of ML methods for
optimizing MeV electron energies and numbers. These
experiments have been conducted with a 1 kHz repetition rate
ultra-intense laser system, with ∼10 mJ per shot, a peak
intensity near 5×

[Sample 16]
========================================
Title: THz carrier dynamics in $SrTiO_{3}/LaTiO_{3}$ interface two-dimensional electron gases

Abstract: A two-dimensional electron gas (2DEG) forms at the interface of complex
oxides like $SrTiO_{3}$ (STO) and $LaTiO_{3}$ (LTO), despite each material
having a low native conductivity, as a band and a Mott insulator, respectively.
The interface 2DEG hosts charge carriers with moderate charge carrier density
and mobility that raised interest as a material system for applications like
field-effect transistors or detectors. Of particular interest is the
integration of these oxide systems in silicon technology. To this end we study
the carrier dynamics in a STO/LTO/STO heterostructure epitaxially grown on
Si(001) both experimentally and theoretically. Linear THz spectroscopy was
performed to analyze the temperature dependent charge carrier density and
mobility, which was found to be in the range of $10^{12}$ $cm^2$ and 1000
$cm^2V^{-1}s^{-1}$, respectively. Pump-probe measurements revealed a very minor
optical nonlinearity caused by hot carriers with a relaxation time of several
10 ps, even at low temperature. Density functional theory calculations with a
Hubbard U term on ultrathin STO-capped LTO films on STO(001) show an effective
mass of 0.64-0.68 $m_{e}$.

THz carrier dynamics in SrTiO3/LaTiO3 interface two-
dimensional electron gases
Ahana Bhattacharya1, Andri Darmawan1,2, Jeong Woo Han1, Frederik Steinkamp1 
Nicholas S. Bingham3.4.5, Ryan J. Suess1, Stephan Winnerl6, Markus E.Gruner1,2, Eric 
N. Jin5,7, Frederick J. Walker5, Charles H. Ahn5, Rossitza Pentcheva1,2, and Martin 
Mittendorff1,*
1Universität Duisburg-Essen, Fakultät für Physik, 47057 Duisburg, Germany
2Universität Duisburg-Essen, Center for Nanointegration (CENIDE), 47057 Duisburg, 
Germany
3University of Maine, Department of Physics and Astronomy, Orono, ME 04469, USA
4University of Maine, Frontier Institute for Research in Sensor Technologies, Orono, ME 
04469 USA
5Yale University, Department of Applied Physics, New Haven, CT 06511 USA
6Helmholtz-Zentrum Dresden-Rossendorf, Bautzner Landstraße 400, 01328 Dresden, 
Germany
7 U.S. Naval Research Laboratory, 4555 Overlook Ave, SW Washington, D.C. 20375, USA
* email: martin.mittendorff@uni-due.de
A 2DEG forms at the interface of complex oxides like SrTiO3 (STO) and LaTiO3  
(LTO), despite each material having a low native conductivity, as a band and a 
Mott insulator, respectively.  The interface 2DEG hosts charge carriers with 
moderate charge carrier density and mobility that raised interest as a material 
system for  applications like field-effect transistors or detectors. Of particular 
interest is the integration of these oxide systems in silicon technology. To this 
end  we  study  the  carrier  dynamics  in  a  STO/LTO/STO  heterostructure 
epitaxially grown on Si(001) both experimentally and theoretically. Linear THz 
spectroscopy was performed to analyze the temperature dependent charge 
carrier density and mobility, which was found to be in the range of 1012 cm-2 and 
1000 cm2V-1s-1, respectively. Pump-probe measurements revealed a very minor 
optical nonlinearity caused by hot carriers with a relaxation time of several 
10ps, even at low temperature. Density functional theory calculations with a 
Hubbard  U term on ultrathin STO-capped LTO films  on STO(001)  show an 
effective mass of 0.64-0.68 me.
1

I Introduction
Heterostructures  of  complex  transition  metal  oxides  feature  interface 
phenomena that are distinct from those in bulk materials, including two-dimensional 
electron gases (2DEGs) [1–3]. For instance, the interface between two different 
oxides can host a 2DEG with distinct electronic properties from those of the 
host materials, such as band gaps, carrier densities, and mobility [4–6]. 
Moreover, heterostructures may exhibit optical properties that are not 
observed in the bulk materials [7]. The most

[Sample 17]
========================================
Title: OccRobNet : Occlusion Robust Network for Accurate 3D Interacting Hand-Object Pose Estimation

Abstract: Occlusion is one of the challenging issues when estimating 3D hand pose. This
problem becomes more prominent when hand interacts with an object or two hands
are involved. In the past works, much attention has not been given to these
occluded regions. But these regions contain important and beneficial
information that is vital for 3D hand pose estimation. Thus, in this paper, we
propose an occlusion robust and accurate method for the estimation of 3D
hand-object pose from the input RGB image. Our method includes first localising
the hand joints using a CNN based model and then refining them by extracting
contextual information. The self attention transformer then identifies the
specific joints along with the hand identity. This helps the model to identify
the hand belongingness of a particular joint which helps to detect the joint
even in the occluded region. Further, these joints with hand identity are then
used to estimate the pose using cross attention mechanism. Thus, by identifying
the joints in the occluded region, the obtained network becomes robust to
occlusion. Hence, this network achieves state-of-the-art results when evaluated
on the InterHand2.6M, HO3D and H$_2$O3D datasets.

OccRobNet : Occlusion Robust Network for Accurate
3D Interacting Hand-Object Pose Estimation
Mallika Garg
ECE Department
IIT Roorkee, India
mallika@ec.iitr.ac.in
Pyari Mohan Pradhan
ECE Department
IIT Roorkee, India
pmpradhan@ece.iitr.ac.in
Debashis Ghosh
ECE Department
IIT Roorkee, India
debashis.ghosh@ece.iitr.ac.in
Abstract—Occlusion is one of the challenging issues
when estimating 3D hand pose. This problem becomes
more prominent when hand interacts with an object
or two hands are involved. In the past works, much
attention has not been given to these occluded regions.
But these regions contain important and beneficial
information that is vital for 3D hand pose estimation.
Thus, in this paper, we propose an occlusion robust and
accurate method for the estimation of 3D hand-object
pose from the input RGB image. Our method includes
first localising the hand joints using a CNN based model
and then refining them by extracting contextual infor-
mation. The self attention transformer then identifies
the specific joints along with the hand identity. This
helps the model to identify the hand belongingness of
a particular joint which helps to detect the joint even
in the occluded region. Further, these joints with hand
identity are then used to estimate the pose using cross
attention mechanism. Thus, by identifying the joints in
the occluded region, the obtained network becomes ro-
bust to occlusion. Hence, this network achieves state-of-
the-art results when evaluated on the InterHand2.6M,
HO3D and H2O3D datasets.
Index Terms—Keypoint association, Cross attention,
Self attention, Contextual information, Sensor applica-
tions
I. Introduction
The objective of the hand pose estimation task is to
localize the hand joints which are extracted using different
methods [1]. This task can be accomplished using a single
RGB image or depth maps. Recently, many deep learning
based-methods have been developed with different Con-
volutional Neural Network (CNN)-based and Transformer
based architectures. In [2], instead of directly predicting
hand pose from input image, images are first mapped to
lixels. Then, mesh vertices are estimated from per-lixel
likelihood. This helps to develop the spatial relationship
between the input image and the predicted values.
Estimating hand pose is an ambiguous problem that
requires a large dataset [3–5]. It should include various
annotations like hand region segmentation mask, hand
pose annotations, etc. These datasets can be single hand
pose datasets, single hand interaction with object and
two hand interaction with object datasets. G. Moon et
al. [4] proposed multi-view hand dataset for single and
interacting hands. This dataset is InterHand2.6M which
consists of RGB images which are captured with various
poses with 80 to 140 high-resolution cameras. It also
includes automatic and manual 3D keypoint annotations
along with handedness, 2.5D hand pose, and relative
depth. However, C. Zimmermann et al. [6] proposed a
multi-view hand dataset for interacting hands. This
dataset is HO3D, which is a dataset for interacting hands
in 3D environments. This dataset consists of videos of
interacting hands with 2D and 3D annotations.
D.J. Duan et al. [7] proposed a dataset for interacting
hands with multiple cameras. This dataset is H2O3D
which is a dataset for interacting hands with multiple
camer

[Sample 18]
========================================
Title: Cellular Automata on Probability Measures

Abstract: Classical Cellular Automata (CCAs) are a powerful computational framework
widely used to model complex systems driven by local interactions. Their
simplicity lies in the use of a finite set of states and a uniform local rule,
yet this simplicity leads to rich and diverse dynamical behaviors. CCAs have
found applications in numerous scientific fields, including quantum computing,
biology, social sciences, and cryptography. However, traditional CCAs assume
complete certainty in the state of all cells, which limits their ability to
model systems with inherent uncertainty. This paper introduces a novel
generalization of CCAs, termed Cellular Automata on Measures (CAMs), which
extends the classical framework to incorporate probabilistic uncertainty. In
this setting, the state of each cell is described by a probability measure, and
the local rule operates on configurations of such measures. This generalization
encompasses the traditional Bernoulli measure framework of CCAs and enables the
study of more complex systems, including those with spatially varying
probabilities. We provide a rigorous mathematical foundation for CAMs,
demonstrate their applicability through concrete examples, and explore their
potential to model the dynamics of random graphs. Additionally, we establish
connections between CAMs and symbolic dynamics, presenting new avenues for
research in random graph theory. This study lays the groundwork for future
exploration of CAMs, offering a flexible and robust framework for modeling
uncertainty in cellular automata and opening new directions for both
theoretical analysis and practical applications.

arXiv:2503.15086v1  [nlin.CG]  19 Mar 2025
Cellular Automata on Probability Measures
Enrico Formenti1, Faizal Haﬁz2, Amelia Kunze1, and Davide La Torre2
1 Université Côte d’Azur, CNRS, i3S, Nice, France
2 SKEMA Business School, Sophia Antipolis, France
Abstract. Classical Cellular Automata (CCAs) are a powerful compu-
tational framework widely used to model complex systems driven by
local interactions. Their simplicity lies in the use of a ﬁnite set of states
and a uniform local rule, yet this simplicity leads to rich and diverse dy-
namical behaviors. CCAs have found applications in numerous scientiﬁc
ﬁelds, including quantum computing, biology, social sciences, and cryp-
tography. However, traditional CCAs assume complete certainty in the
state of all cells, which limits their ability to model systems with inher-
ent uncertainty. This paper introduces a novel generalization of CCAs,
termed Cellular Automata on Measures (CAMs), which extends the clas-
sical framework to incorporate probabilistic uncertainty. In this setting,
the state of each cell is described by a probability measure, and the local
rule operates on conﬁgurations of such measures. This generalization en-
compasses the traditional Bernoulli measure framework of CCAs and en-
ables the study of more complex systems, including those with spatially
varying probabilities. We provide a rigorous mathematical foundation
for CAMs, demonstrate their applicability through concrete examples,
and explore their potential to model the dynamics of random graphs.
Additionally, we establish connections between CAMs and symbolic dy-
namics, presenting new avenues for research in random graph theory.
This study lays the groundwork for future exploration of CAMs, oﬀer-
ing a ﬂexible and robust framework for modeling uncertainty in cellular
automata and opening new directions for both theoretical analysis and
practical applications.
Keywords: Cellular Automata · Probability Measures · Random Graphs
1
Introduction
Classical Cellular Automata (hereafter referred to as CCAs) are extensively used
as formal tools to model various complex phenomena arising from local inter-
actions and a ﬁnite number of states. CCAs ﬁnd applications across numerous
scientiﬁc domains, including quantum computing, biology, social sciences, and
more [1,19,21,23]. In computer science, they are particularly valuable in cryp-
tography, where they are employed for tasks such as pseudo-random number
generation, secret sharing schemes, and the design of S-boxes [5,14,15].
Informally, a CCA consists of an inﬁnite collection of automata (referred to
as cells), each of which assumes a state (or a value from the alphabet) drawn

2
E. Formenti et al.
from a ﬁnite set (the set of states). These automata are connected by a local
rule, which dictates the state of each cell based on its neighboring cells. The
state of a cell at each time step is determined by the state of its neighbors,
subject to the local rule. The transition function (or the local rule) is typically
specified by a set of rules that define how the state of each cell changes
conﬂicting rules, resulting in diverse and complex patterns of behavior.

One of the key

[Sample 19]
========================================
Title: Specifying the Intrinsic Back-action of a General Measurement

Abstract: Understanding the invasive nature of quantum measurement and its implications
in quantum foundations and information science demands a mathematically
rigorous and physically well-grounded characterization of intrinsic back-action
in general measurement processes. However, such a framework remains elusive,
leaving a critical gap in quantum theory. Here, we address this issue by
conceptualizing a general quantum measurement as a reduction of extended
projection measurements ensured by Naimark's theorem and, derive a
state-updating rule for the concerned measurement as a reduction of the
projective measurements postulate. Our framework provides a detailed analysis
by explicitly decomposing the disturbance effects into two distinct
contributions: those arising from the measurement elements themselves and those
resulting from the dilation process. Notably, this formulation naturally
recovers the projection postulate in the case of projective measurements.
Beyond providing insights into joint measurability, non-disturbance, our rule
establishes quantitaive connections between intrinsic disturbance and other
fundamental quantum features, such as randomness, uncertainty, and information
gain.

arXiv:2503.21296v1  [quant-ph]  27 Mar 2025
Specifying the Intrinsic Back-action of a General Measurement
Liang-Liang Sun,1, ∗Armin Tavakoli,2 René Schwonnek,3 Matthias Kleinmann,4 Zhen-Peng Xu,5 and Sixia Yu1, †
1Department of Modern Physics and National Laboratory for Physical Sciences at Microscale,
University of Science and Technology of China, Hefei, Anhui 230026, China
2Physics Department, Lund University, Box 118, 22100 Lund, Sweden
3Institut für Theoretische Physik, Leibniz Universität Hannover, Appelstrasse 2, 30167, Germany
4Naturwissenschaftlich-Technische Fakultät, Universität Siegen, Walter-Flex-Straße 3, 57068 Siegen, Germany
5School of Physics and Optoelectronics Engineering,
Anhui University, Hefei 230601, People’s Republic of China
(Dated: March 28, 2025)
Understanding the invasive nature of quantum measurement and its implications in quantum foun-
dations and information science demands a mathematically rigorous and physically well-grounded
characterization of intrinsic back-action in general measurement processes. However, such a frame-
work remains elusive, leaving a critical gap in quantum theory. Here, we address this issue by con-
ceptualizing a general quantum measurement as a reduction of extended projection measurements
ensured by Naimark’s theorem and, derive a state-updating rule for the concerned measurement as
a reduction of the projective measurements postulate. Our framework provides a detailed analy-
sis by explicitly decomposing the disturbance eﬀects into two distinct contributions: those arising
from the measurement elements themselves and those resulting from the dilation process. Notably,
this formulation naturally recovers the projection postulate in the case of projective measurements.
Beyond providing insights into joint measurability, non-disturbance, our rule establishes quantita-
tive connections between intrinsic disturbance and other fundamental quantum features, such as
randomness, uncertainty, and information gain.
Previously, a measurement has been perceived as a
passive recording of pre-existing values of physical quan-
tities assuming a realistic interpretation. This classical
intuition was revolutionized by quantum theory, where a
particle can exist in a superposition of multiple states,
and a projection measurement forces it to randomly and
abruptly "jump" into one eigenstate of the measured ob-
servable, meanwhile assigning a value to the observable.
This state change is referred to intrinsic disturbance ef-
fect of quantum measurement [1–3] that underpins nearly
all fundamental quantum properties [4], from the initial
complementarity principle to nonlocality [5, 6], contex-
tuality [7], steering [8, 9], and coherence [10, 11].
It
also leads to a fundamental distinction between the quan-
tum and the classical information science by limiting the
amount of information an adversary can extract from a
single particle [12] and ensuring that any such attempt
inevitably causes a detectable disturbance, providing the
basis for secure quantum cryptography [13, 14].
How-
ever, what is surprising is that an accurate characteri-
zation of this eﬀect for general measurement process re-
mains elusive, leaving signiﬁcant gaps in the quantum
foundation. In particular, there is a lack of a mat-
hematically rigorous and physically well-grounded
characterization of intrinsic disturbance.
In this paper, we address this issue by conceptual-
izing a general quantum measurement as a reduction
of extended projection measurements ensured by
Naimark’s theorem [15] and, derive a state-updating
rule for the concerned measurement as a reduction of
the projective measurements

[Sample 20]
========================================
Title: Quantum enhanced parameter estimation with monitored quantum nonequilibrium systems using inefficient photo detection

Abstract: Many-body quantum systems hosting emergent collective behavior bear the
promise to enable quantum enhanced parameter estimation. Formally this means
that the variance of the parameter to be estimated decreases faster than
$N^{-1}$, where $N$ is the number of particles forming the quantum system. In
practice such scaling is challenging to achieve as the underlying many-body
correlations are fragile. Moreover, devising the optimal measurements that
indeed tap the quantum enhancement is often rather involved. Here we show that
the inefficient detection of the photo emission from a dissipative quantum
many-body system is sufficient to reach quantum enhanced parameter estimation
even when some loss channels remain completely unmonitored. We illustrate our
approach by considering the so-called boundary time-crystal, which is a
nonequilibrium many-body system that has been realized recently experimentally
in cold atomic gases. By analyzing the structure of the temporal correlations
of its emission field, we are able to construct a family of near optimal
parameter estimation measurements with a simple interferometric setup.

Quantum enhanced parameter estimation with monitored quantum nonequilibrium
systems using inefficient photo detection
Albert Cabot,1 Federico Carollo,2 and Igor Lesanovsky1, 3, 4
1Institut f¨ur Theoretische Physik and Center for Integrated Quantum Science and Technology,
Universit¨at T¨ubingen, Auf der Morgenstelle 14, 72076 T¨ubingen, Germany.
2Centre for Fluid and Complex Systems, Coventry University, Coventry, CV1 2TT, United Kingdom
3School of Physics and Astronomy, University of Nottingham, Nottingham, NG7 2RD, UK.
4Centre for the Mathematics and Theoretical Physics of Quantum Non-Equilibrium Systems,
University of Nottingham, Nottingham, NG7 2RD, UK
Many-body quantum systems hosting emergent collective behavior bear the promise to enable
quantum enhanced parameter estimation. Formally this means that the variance of the parameter
to be estimated decreases faster than N −1, where N is the number of particles forming the quantum
system. In practice such scaling is challenging to achieve as the underlying many-body correlations
are fragile. Moreover, devising the optimal measurements that indeed tap the quantum enhance-
ment is often rather involved. Here we show that the inefficient detection of the photo emission from
a dissipative quantum many-body system is sufficient to reach quantum enhanced parameter esti-
mation even when some loss channels remain completely unmonitored. We illustrate our approach
by considering the so-called boundary time-crystal, which is a nonequilibrium many-body system
that has been realized recently experimentally in cold atomic gases. By analyzing the structure of
the temporal correlations of its emission field, we are able to construct a family of near optimal
parameter estimation measurements with a simple interferometric setup.
I.
INTRODUCTION
Quantum correlations and collective phenomena in
many-body systems can serve as a resource for sensing
applications [1, 2]. Examples include protocols to detect
small displacements or weak electromagnetic fields in sys-
tems of trapped ions [3], or Rydberg atoms [4, 5]. In these
settings, the number of particles, N, represents a resource
for increasing the precision of sensors and quantum ef-
fects can lead to enhanced scalings of the sensitivity with
it. The variance of the estimate of a quantity of interest
can indeed pass from decreasing with the scaling N −1,
the so-called standard quantum limit, to N −2, the so-
called Heisenberg limit, in the presence of quantum cor-
relations [1, 2]. The Heisenberg scaling with the number
of particles has been observed, for instance, in protocols
exploiting so-called N00N entangled states to estimate
small phases [6]. However, the practical implementation
of these quantum enhanced protocols is typically chal-
lenging due to both the susceptibility of quantum systems
to decoherence effects, suppressing quantum correlations,
and to the difficulty of preparing N00N states for large
systems [1]. Another route to achieve quantum-enhanced
sensing is through the use of spin squeezed states, which
allow one to, e.g., surpass the standard quantum limit in
the estimation of phases [7–9]. Large clouds of atoms can
be prepared in spin squeezed states by trapping them in
high-finesse optical cavities. Here the coupling between
the atoms and the cavity modes can be leveraged to gen-
erate spin squeezing [9, 10], for instance by engineering
collective Hamiltonian interactions or by implementing
feedback control [11]. However, spin squeezing also
implies a loss of quantum correlations due to the local
thermalization of the system.
In the present work, we focus on the application of
quantum enhanced sensing to nonequilibrium many-body
systems. Here, we are particularly interested in systems
that exhibit emergent collective behavior, which can
be viewed as a natural resource for enhanced parameter
estimation. One can think of such systems as a kind of

[Sample 21]
========================================
Title: X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction

Abstract: Four-dimensional computed tomography (4D CT) reconstruction is crucial for
capturing dynamic anatomical changes but faces inherent limitations from
conventional phase-binning workflows. Current methods discretize temporal
resolution into fixed phases with respiratory gating devices, introducing
motion misalignment and restricting clinical practicality. In this paper, We
propose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT
reconstruction by integrating dynamic radiative Gaussian splatting with
self-supervised respiratory motion learning. Our approach models anatomical
dynamics through a spatiotemporal encoder-decoder architecture that predicts
time-varying Gaussian deformations, eliminating phase discretization. To remove
dependency on external gating devices, we introduce a physiology-driven
periodic consistency loss that learns patient-specific breathing cycles
directly from projections via differentiable optimization. Extensive
experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR
gain over traditional methods and 2.25 dB improvement against prior Gaussian
splatting techniques. By unifying continuous motion modeling with hardware-free
period learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for
dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.

X2-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time
Tomographic Reconstruction
Weihao Yu1
Yuanhao Cai2
Ruyi Zha3
Zhiwen Fan4
Chenxin Li1
Yixuan Yuan1*
1The Chinese University of Hong Kong
2Johns Hopkins University
3The Australian National University
4University of Texas at Austin
Ours
R2-GS
SAX-NeRF 3DGS
X-GS
Ours
R2-GS
SAX-NeRF
3DGS
X-GS
2D PSNR
3D PSNR
t = 0.15s
t = 0.45s
t = 1.35s
t = 2.25s
t = 2.85s
Coronal
Sagittal
Axial
Figure 1. Dynamic reconstruction results of the proposed X2-Gaussian on public DIR Dataset [9]. The red dashed line is the reference
line for diaphragm movement, and blue dashed box shows some tissue deformation. Our method demonstrates superior capability in
continuous-time reconstruction, significantly outperforming existing approaches.
Abstract
Four-dimensional computed tomography (4D CT) recon-
struction is crucial for capturing dynamic anatomical
changes but faces inherent limitations from conventional
phase-binning workflows. Current methods discretize tem-
poral resolution into fixed phases with respiratory gat-
ing devices, introducing motion misalignment and restrict-
ing clinical practicality.
In this paper, We propose X2-
Gaussian, a novel framework that enables continuous-
time 4D-CT reconstruction by integrating dynamic radia-
tive Gaussian splatting with self-supervised respiratory mo-
tion learning. Our approach models anatomical dynam-
ics through a spatiotemporal encoder-decoder architecture
that predicts time-varying Gaussian deformations, elimi-
nating phase discretization. To remove dependency on ex-
ternal gating devices, we introduce a physiology-driven pe-
riodic consistency loss that learns patient-specific breath-
ing cycles directly from projections via differentiable opti-
*Corresponding Author.
mization. Extensive experiments demonstrate state-of-the-
art performance, achieving a 9.93 dB PSNR gain over tra-
ditional methods and 2.25 dB improvement against prior
Gaussian splatting techniques.
By unifying continuous
motion modeling with hardware-free period learning, X2-
Gaussian advances high-fidelity 4D CT reconstruction for
dynamic clinical imaging.
Project website at: https:
//x2-gaussian.github.io/.
1. Introduction
Four-dimensional computed tomography (4D CT) has be-
come a cornerstone in dynamic medical imaging, espe-
cially for respiratory motion management in clinical appli-
cations such as image-guided radiotherapy (IGRT) [12, 38].
By capturing both spatial and temporal information of the
chest cavity during breathing cycles, 4D CT enables clini-
cians to monitor and assess respiratory-induced tumor mo-
tion and other dynamic anatomical changes. However,
current CT reconstruction methods suffer from the limitation
of phase-binning, which discretizes temporal resolution
into fixed phases with respiratory gating devices [6,
13, 16, 17, 19, 20, 22, 23, 25, 26, 28, 29, 31, 32, 35,
36, 37, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52,
53,

[Sample 22]
========================================
Title: Payne-Pólya-Weinberger inequalities on closed Riemannian manifolds

Abstract: Payne-P\'olya-Weinberger inequalities are known to be exclusive to bounded
Euclidean domains with Dirichlet boundary condition. In this paper, we discuss
the corresponding inequalities on Riemannian manifolds of dimension $n \geq3$,
and we prove explicit bounds in terms of geometric quantities such as scalar
curvature, Yamabe constant, isoperimetric constant and conformal volume.

arXiv:2503.20181v1  [math.SP]  26 Mar 2025
PAYNE-P´OLYA-WEINBERGER INEQUALITIES ON CLOSED
RIEMANNIAN MANIFOLDS
MEHDI EDDAOUDI
Abstract. Payne-P´olya-Weinberger inequalities are known to be exclusive to
bounded Euclidean domains with Dirichlet boundary condition. In this paper,
we discuss the corresponding inequalities on Riemannian manifolds of dimen-
sion n ≥3, and we prove explicit bounds in terms of geometric quantities such
as scalar curvature, Yamabe constant, isoperimetric constant and conformal
volume.
1. Introduction
1.1. Universal inequalities for Dirichlet eigenvalues. Let Ω⊂Rn be a regular
bounded domain, and consider the classical Dirichlet eigenvalue problem
(1)
(
∆u = −λDu
in Ω,
u = 0
on ∂Ω,
It is well known that the spectrum of (1) is real and discrete, and consists of a
discrete sequence of eigenvalues
λD
1 (Ω) < λD
2 (Ω) ≤· · · ր +∞,
repeated according to its multiplicity.
Traditionally, an universal inequality establishes a relationship between Dirichlet
eigenvalues that is independent of the speciﬁc geometric properties of the domain.
This topic ﬁrst appeared in the 1950s with a famous result by Payne, P´olya, and
Weinberger (PPW for short) [60], who proved an upper bound for the ratio λD
2 /λD
1
on planar bounded domains,
λD
2 (Ω)
λD
1 (Ω) ≤3.
They then conjectured that this ratio should achieve its maximum if and only if
when the domain is an n-ball Bn,
λD
2 (Ω)
λD
1 (Ω) ≤λD
2 (Bn)
λD
1 (Bn).
This long-standing conjecture was eventually proven 35 years later in 1991 by Ash-
baugh and Benguria [3, 4] following a series of improvements [11, 66, 68, 45, 19,
18, 58, 70]; see also [5, 6, 9, 2] for more literature on the topic. More generally, the
PPW conjecture states that for k ≥2,
(2)
λD
k+1(Ω)
λD
k (Ω) < λD
2 (Bn)
λD
1 (Bn),
1

2
MEHDI EDDAOUDI
with equality achieved in the limit by a sequence of domains degenerating into
k disjoint n-balls of equal volume. This phenomenon of bubbling often arises in
spectral optimization problems of various eigenvalue functionals [31, 12, 54, 25, 30,
32, 62, 51, 13, 50, 49], and it remains not yet fully understood to this day.
In higher dimension, the bound given by PPW becomes naturally 1 + 4
n, and
Thompson [66] extended it in 1969 to the general ratio λD
k+1/λD
k by
λD
k+1(Ω)
λD
k (Ω) ≤1 + 4
n.
Since then, other types of universal inequalities have been discovered over time.
For example, we cite the inequality by Hile and Protter [45] in 1980,
k
X
i=1
λD
i (Ω)
λD
k+1(Ω) −λD
i (Ω) ≥kn
4.
The quadratic inequality by Yang [70] in 1991,
(3)
k
X
i=1
(λD
k+1 −λD
i )
(Ω)
(λD
k+1 −λD
i )
(Ω)
(λD
k+1 −λD
i )
(Ω)
(λD
k+1 −λD
i )
(Ω)
(λD
k+1 −λD
i )
(Ω)
(λD
k+1 −

[Sample 23]
========================================
Title: The Fe-Ni phase diagram and the Earth's inner core structure

Abstract: The Fe-Ni alloy is believed to be the main component of Earth's core. Yet, a
comprehensive understanding of phase equilibria near the melting point of this
alloy under core conditions is still lacking, leaving the effect of nickel
inconclusive. Using ab initio simulations, we computed Gibbs free energy and
phase diagram for liquid and solid solutions of the Fe-Ni alloy under
conditions close to the inner core, considering inner-shell electron
contributions and non-ideal mixing effects. The Fe-Ni phase diagram provides
crucial insights for understanding previous experimental observations and
crystallization simulations of the Fe-Ni alloy under core conditions. It also
presents new scenarios for inner core structures, suggesting bcc-liquid
coexistence at the inner core boundary and the possibility of multi-layer
structures consisting of bcc-hcp composites within the inner core. Our work
clarifies nickel's substantial impact on the inner core structure, providing
new constraints for the study of core's composition and formation.

1 
The Fe-Ni phase diagram and the Earth’s inner core structure 
 
Liangrui Wei1, Kai-Ming Ho2, Renata M. Wentzcovitch3-6, and Yang Sun1 
 
1Department of Physics, Xiamen University, Xiamen 361005, China 
2Department of Physics, Iowa State University, Ames, IA 50011, USA 
3Department of Applied Physics and Applied Mathematics, Columbia University, New York, NY 10027, USA 
4Department of Earth and Environmental Sciences, Columbia University, New York, NY, 10027, USA 
5Lamont–Doherty Earth Observatory, Columbia University, Palisades, NY, 10964, USA 
6 Data Science Institute, Columbia University, New York, NY 10027, USA 
(Dated: March 27, 2025) 
 
     The Fe-Ni alloy is believed to be the main component of Earth’s core. Yet, a comprehensive 
understanding of phase equilibria near the melting point of this alloy under core conditions is still lacking, 
leaving the effect of nickel inconclusive. Using ab initio simulations, we computed Gibbs free energy and 
phase diagram for liquid and solid solutions of the Fe-Ni alloy under conditions close to the inner core, 
considering inner-shell electron contributions and non-ideal mixing effects. The Fe-Ni phase diagram 
provides crucial insights for understanding previous experimental observations and crystallization 
simulations of the Fe-Ni alloy under core conditions. It also presents new scenarios for inner core structures, 
suggesting bcc-liquid coexistence at the inner core boundary and the possibility of multi-layer structures 
consisting of bcc-hcp composites within the inner core. Our work clarifies nickel’s substantial impact on 
the inner core structure, providing new constraints for the study of core’s composition and formation. 
 
 
The Earth’s core consists of a liquid outer core (OC) 
and a solid inner core (IC), primarily made up of iron-
nickel alloys and light elements  [1,2]. Understanding 
the crystalline phases in the solid inner core is essential 
for determining the partitioning of light elements and the 
core’s seismic properties. Traditionally, based on the 
phase diagram of pure iron, the inner core is thought to 
predominantly have a hexagonal close-packed (hcp) 
structure  [3–5]. However, because the free energy of the 
body-centered cubic (bcc) phase can be comparable to 
that of the hcp phase under core conditions, the 
possibility of a bcc inner core has also been 
proposed  [6,7].  
Elements alloyed with iron can influence the stability 
of crystalline phases in the IC. While the exact 
concentration of light elements in the core remains 
debated  [8–13],  nickel is present in the OC and IC, with 
estimated concentrations ranging from 5% to 15% based 
on cosmochemical and geochemical models  [1,14]. 
Low-pressure, 
low-temperature 
experiments 
have 
shown that nickel can enhance the stability of the face-
centered cubic (fcc) phase over the hexagonal close-
packed (hcp) phase in iron  [15–17]. However, at high 
temperatures similar to core conditions, nickel was 
theoretically predicted to favor the hcp phase over the 
fcc phase  [18,19]. 
The effects of nickel on the inner core structure 
have been the subject of extensive debate, with 
the two prevailing theories being the nickel-hcp 
model  [20–22] and the nickel-bcc model  [23–25]. 
The nickel-hcp model posits that nickel forms a 
liquid solution with the hcp phase

[Sample 24]
========================================
Title: Evaluating Text-to-Image Synthesis with a Conditional Fréchet Distance

Abstract: Evaluating text-to-image synthesis is challenging due to misalignment between
established metrics and human preferences. We propose cFreD, a metric based on
the notion of Conditional Fr\'echet Distance that explicitly accounts for both
visual fidelity and text-prompt alignment. Existing metrics such as Inception
Score (IS), Fr\'echet Inception Distance (FID) and CLIPScore assess either
image quality or image-text alignment but not both which limits their
correlation with human preferences. Scoring models explicitly trained to
replicate human preferences require constant updates and may not generalize to
novel generation techniques or out-of-domain inputs. Through extensive
experiments across multiple recently proposed text-to-image models and diverse
prompt datasets, we demonstrate that cFreD exhibits a higher correlation with
human judgments compared to statistical metrics, including metrics trained with
human preferences. Our findings validate cFreD as a robust, future-proof metric
for the systematic evaluation of text-to-image models, standardizing
benchmarking in this rapidly evolving field. We release our evaluation toolkit
and benchmark in the appendix.

Evaluating Text-to-Image Synthesis with a Conditional Fr´echet Distance
Jaywon Koo*, Jefferson Hernandez*, Moayed Haji-Ali, Ziyan Yang, and Vicente Ordonez
Rice University
{jk125, jefehern, mh155, zy47, vicenteor}@rice.edu
https://github.com/JaywonKoo17/cFreD
Abstract
Evaluating text-to-image synthesis is challenging due to mis-
alignment between established metrics and human prefer-
ences. We propose cFreD, a metric based on the notion
of Conditional Fr´echet Distance that explicitly accounts
for both visual fidelity and text-prompt alignment. Existing
metrics such as Inception Score (IS), Fr´echet Inception Dis-
tance (FID) and CLIPScore assess either image quality or
image-text alignment but not both which limits their corre-
lation with human preferences. Scoring models explicitly
trained to replicate human preferences require constant up-
dates and may not generalize to novel generation techniques
or out-of-domain inputs. Through extensive experiments
across multiple recently proposed text-to-image models and
diverse prompt datasets, we demonstrate that cFreD exhibits
a higher correlation with human judgments compared to
statistical metrics, including metrics trained with human
preferences. Our findings validate cFreD as a robust, future-
proof metric for the systematic evaluation of text-to-image
models, standardizing benchmarking in this rapidly evolving
field. We release our evaluation toolkit and benchmark in
the appendix.
1. Introduction
Generative models have demonstrated remarkable capabil-
ities in text-to-image generation [17, 24, 53, 59], text-to-
video generation [5, 35, 37, 60, 78], and other modali-
ties [4, 25, 26, 28, 47, 82]. This progress has been driven
by technical breakthroughs such as the development and im-
proved understanding of Generative Adversarial Networks
(GANs) [22], Variational AutoEncoders (VAEs) [32], and
more recently models based on Denoising Diffusion [65]
and Flow Matching [43]. Automatic evaluation metrics have
played a crucial role in guiding the development and refine-
ment of these models by offering quantitative benchmarks
that compare the quality, diversity, and fidelity of various
*Equal contribution.
Parti-Prompts
HPDv2
COCO
Datasets
0
20
40
60
80
100
Correlation with Human Preference (%)
0.1
53.6
69.7 70.0
74.5
63.4 65.5
70.0
87.6
96.6
0.5
8.1
22.5 22.7
32.8
FID Score
FD DINOv2
CLIP Score
CMMD
cFredD
Figure 1. Correlation of five evaluation metrics with human pref-
erences on three benchmark datasets (Parti-Prompts, HPDv2, and
COCO). Compared to FID, FDDINOv2, CLIPScore, and CMMD, our
proposed method (cFreD) achieves consistently higher correlation
with human judgments across all datasets.
generative approaches. The most commonly used metrics in-
clude Inception Score (IS) [17, 24, 53, 59], Fr´echet Inception Dis-
tance (FID) [15, 25, 36, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60], and
CLIPScore [26, 29, 30, 31, 32, 33, 34, 35, 36,

[Sample 25]
========================================
Title: On the relativistic effect in the Dirac--Fock theory

Abstract: In this paper, we study the error bound of the Dirac--Fock ground-state
energy and the Hartree--Fock ground-state energy. This error bound is called
the relativistic effect in quantum mechanics. We confirm that the relativistic
effect in the Dirac--Fock ground-state energy is of the order $\cO(c^{-2})$
with $c$ being the speed of light. Furthermore, if the potential between
electrons and nuclei is regular, we get the leading order relativistic
correction, which comprises the sum of the mass-velocity term, the Darwin term,
and the spin-orbit term. The proof is based on a delicate study of projections
onto the positive eigenspace of some Dirac operators.
  To our knowledge, it is the first mathematical derivation of the leading
order relativistic correction for nonlinear Dirac ground-state energies. Our
method paves the way to study the relativistic effects in general nonlinear
Dirac problems.

arXiv:2503.21405v1  [math-ph]  27 Mar 2025
On the relativistic eﬀect in the Dirac–Fock theory
Long Meng∗
Abstract
In this paper, we study the error bound of the Dirac–Fock ground-state en-
ergy and the Hartree–Fock ground-state energy. This error bound is called the
relativistic eﬀect in quantum mechanics. We conﬁrm that the relativistic eﬀect in
the Dirac–Fock ground-state energy is of the order Opc´2q with c being the speed
of light. Furthermore, if the potential between electrons and nuclei is regular, we
get the leading order relativistic correction, which comprises the sum of the mass-
velocity term, the Darwin term, and the spin-orbit term. The proof is based on a
delicate study of projections onto the positive eigenspace of some Dirac operators.
To our knowledge, it is the ﬁrst mathematical derivation of the leading order
relativistic correction for nonlinear Dirac ground-state energies. Our method paves
the way to study the relativistic eﬀects in general nonlinear Dirac problems.
1
Introduction
This work is part of a series of papers by the author on Dirac–Fock (DF) theory [8, 9,
20, 21]. Here, we focus on the relativistic eﬀect in the DF ground-state energy, that
is the error bound between the DF ground-state energy and the Hartree–Fock (HF)
ground-state energy.
The emergence of relativistic quantum mechanics has been one of the most remark-
able developments in quantum physics over the past century. Since Dirac’s pioneering
work, relativity has been a part of the quantum physical picture. While non-relativistic
theories successfully describe quantum systems with particle velocities much smaller
than the speed of light c, relativistic eﬀects play a crucial role in high-precision cal-
culations. Now it became clear that relativistic eﬀects had an essential inﬂuence on a
number of physical and chemical properties.
Understanding how relativistic eﬀects inﬂuence non-relativistic theories is crucial
for bridging the gap between non-relativistic quantum mechanics and the more com-
prehensive relativistic framework. This is of particular importance for Fermions with
spin ˘ 1
2 due to the complexity of the Dirac operator.
1.1
Linear Dirac eigenvalue problem
As a small quantity – especially for light atoms and molecules built from them – the
relativistic eﬀect is a perturbation of the non-relativistic energies. Consequently, per-
turbation methods are developed in quantum mechanics, and the direct perturbation
∗Long Meng, Mathematisches Institut, Ludwig-Maximilians-Universität München,
80333 München, Germany E-mail address: meng@math.lmu.de
1

theory (see e.g., [13, 17] for a review) is frequently used and has shown signiﬁcant
success in quantum chemistry.
From a rigorous mathematical point of view, the perturbation theory used in quan-
tum chemistry is based on the linearized Dirac equation (see e.g., [1, 10, 11, 13, 17]).
The Dirac equation is the solution of the Schrödinger equation for the Dirac ﬁber, which
is a spin-1/2 particle.
The Dirac equation is nonlinear, and its solution is given by the eigenfunctions of the
Dirac operator, denoted by, and the Dirac eigenvalues

[Sample 26]
========================================
Title: Exact module categories over $\mathrm{Rep}(u_q(\mathfrak{sl}_2))$

Abstract: We give a complete list of indecomposable exact module categories over the
finite tensor category $\mathrm{Rep}(u_q(\mathfrak{sl}_2))$ of representations
of the small quantum group $u_q(\mathfrak{sl}_2)$, where $q$ is a root of unity
of odd order. Each of them is given as the category of representations of a
left comodule algebra over $u_q(\mathfrak{sl}_2)$ explicitly presented by
generators and relations.

arXiv:2503.21265v1  [math.QA]  27 Mar 2025
EXACT MODULE CATEGORIES OVER Rep(uq(sl2))
DAISUKE NAKAMURA, TAIKI SHIBATA, AND KENICHI SHIMIZU
Abstract. We give a complete list of indecomposable exact module categories
over the ﬁnite tensor category Rep(uq(sl2)) of representations of the small
quantum group uq(sl2), where q is a root of unity of odd order. Each of them
is given as the category of representations of a left comodule algebra over
uq(sl2) explicitly presented by generators and relations.
Contents
1.
Introduction
2
1.1.
Organization of this paper
3
1.2.
Acknowledgements
5
2.
Finite tensor categories and their modules
5
2.1.
Notation and convention
5
2.2.
Finite tensor categories and their modules
5
2.3.
Morita theory in ﬁnite tensor categories
6
2.4.
Eilenberg-Watts equivalence
8
2.5.
Indecomposable module categories
8
2.6.
Exact module categories
8
2.7.
Simple algebras
10
2.8.
Haploid algebras
11
2.9.
Morita equivalence in pointed ﬁnite tensor categories
12
3.
Exact module categories and comodule algebras
12
3.1.
Notation and convention
12
3.2.
Terminologies on comodule algebras
13
3.3.
Finite module categories over HM
14
3.4.
Equivariant Eilenberg-Watts theorem
14
3.5.
The Morita duality
14
3.6.
Equivariant Morita equivalence
16
3.7.
Indecomposable exact module categories over HM
17
3.8.
Skryabin’s theorems
18
3.9.
Cocycle deformation
18
3.10.
Examples: Group algebras
20
4.
Exact comodule algebras over pointed Hopf algebras
20
4.1.
Filtered and graded vector spaces
20
4.2.
Filtered and graded comodules algebras
21
4.3.
The coradical and the Loewy ﬁltration
22
4.4.
H-simplicity of ﬁltered comodule algebras
23
4.5.
Strategy of the classiﬁcation
24
2020 Mathematics Subject Classiﬁcation. 18M05, 16T05.
Key words and phrases. Hopf algebra, ﬁnite tensor category, module category.
1

2
D. NAKAMURA, T. SHIBATA, AND K. SHIMIZU
5.
Exact comodule algebras over uq(sl2)
26
5.1.
The Hopf algebras uq(sl2) and gr(uq(sl2))
26
5.2.
Step 1. List graded coideal subalgebras of gr(uq(sl2))
28
5.3.
Step 2. Find liftings as comodule algebras over gr(uq(sl2))
28
5.4.
Step 3. Detect gr(uq(sl2))-Morita equivalence
30
5.5.
Step 4. Compute cocycle deformations
33
5.6.
Step 5. Compute the comodule category
33
5.7.
Step 6. Find the exact comodule algebras
34
5.8.
The coradical and the Loewy filtration
35
5.9.
H-simplicity of comodule algebras
36
5.10.
The classiﬁcation of comodule algebras
38
6


[Sample 27]
========================================
Title: Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing

Abstract: Large Language Models (LLMs) have transformed task automation and content
generation across various domains while incorporating safety filters to prevent
misuse. We introduce a novel jailbreaking framework that employs distributed
prompt processing combined with iterative refinements to bypass these safety
measures, particularly in generating malicious code. Our architecture consists
of four key modules: prompt segmentation, parallel processing, response
aggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts
across 10 cybersecurity categories, the framework achieves a 73.2% Success Rate
(SR) in generating malicious code. Notably, our comparative analysis reveals
that traditional single-LLM judge evaluation overestimates SRs (93.8%) compared
to our LLM jury system (73.2%), with manual verification confirming that
single-judge assessments often accept incomplete implementations. Moreover, we
demonstrate that our distributed architecture improves SRs by 12% over the
non-distributed approach in an ablation study, highlighting both the
effectiveness of distributed prompt processing and the importance of robust
evaluation methodologies in assessing jailbreak attempts.

Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via
Segmented and Distributed Prompt Processing
Johan Wahréus, Ahmed Hussain, and Panos Papadimitratos
Networked Systems Security (NSS) Group
KTH Royal Institute of Technology, Stockholm, Sweden
{wahreus, ahmhus, papadim}@kth.se
Abstract
Large Language Models (LLMs) have transformed task au-
tomation and content generation across various domains while
incorporating safety filters to prevent misuse. We introduce a
novel jailbreaking framework that employs distributed prompt
processing combined with iterative refinements to bypass
these safety measures, particularly in generating malicious
code. Our architecture consists of four key modules: prompt
segmentation, parallel processing, response aggregation, and
LLM-based jury evaluation. Tested on 500 malicious prompts
across 10 cybersecurity categories, the framework achieves a
73.2% Success Rate (SR) in generating malicious code. No-
tably, our comparative analysis reveals that traditional single-
LLM judge evaluation overestimates SRs (93.8%) compared
to our LLM jury system (73.2%), with manual verification
confirming that single-judge assessments often accept incom-
plete implementations. Moreover, we demonstrate that our
distributed architecture improves SRs by 12% over the non-
distributed approach in an ablation study, highlighting both
the effectiveness of distributed prompt processing and the
importance of robust evaluation methodologies in assessing
jailbreak attempts.
Keywords: Large Language Models, Security, Privacy, Cyber-
security, Attacks, Malicious Code, Agents, LLM Jury, LLM
Judge, Code Generation
1
Introduction
Large Language Models (LLMs) are advanced Artificial
Intelligence (AI) systems that have revolutionized Natural
Language Processing (NLP) by demonstrating exceptional
capabilities in understanding and generating human-like
text. Built on transformer architecture [1], these models ex-
cel at processing sequential data using innovative attention
mechanisms and parallel processing techniques. Through
extensive training on massive datasets drawn from diverse
sources—such as academic literature, code repositories, web-
sites, and books—LLMs capture complex linguistic phenom-
ena, ranging from subtle grammatical nuances to intricate
contextual dependencies [2,3]. This allows LLMs to perform
a diverse range of tasks across various domains, including
text summarization, conversational interactions, and content
generation.
LLMs have emerged as powerful tools for code generation
tasks [4–7]. Trained on extensive code repositories and doc-
umentation, LLMs generate functional code across multiple
languages, enabling automated code completion, boilerplate
generation, and codebase optimization [8]. Thereby enhanc-
ing developer productivity across various expertise levels (i.e.,
beginner to expert). Such capabilities raise significant ethical
concerns regarding their potential misuse [9,10]. While these
models excel at assisting developers with legitimate program-
ming tasks, their capabilities could be exploited by malicious
actors to automate the creation of harmful software. Namely,
eliciting harmful code through jailbreaking–circumventing
an LLM’s built-in safety filters and restrictions. These filters
are fundamental safeguards designed to prevent the gener-
ation of harmful content and ensure alignment with ethical
guidelines. When successfully jailbroken, an LLM can po-
tentially generate outputs ranging from malicious code to
detailed attack methodologies, thereby posing significant
security risks.
In response to such concerns, LLM developers have
incorporated safety mechanisms to mitigate the risk of misuse.
These include prompt filtering, which restricts access to
sensitive content and prevents the generation of harmful
outputs. Additionally, LLMs utilize content-based filtering
techniques, such as word embeddings and similarity measures,
to identify potentially harmful prompts and exclude them from
the generation process. These safety filters effectively
limit

[Sample 28]
========================================
Title: Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson Inverse Problems

Abstract: This paper introduces a novel plug-and-play (PnP) Langevin sampling
methodology for Bayesian inference in low-photon Poisson imaging problems, a
challenging class of problems with significant applications in astronomy,
medicine, and biology. PnP Langevin sampling algorithms offer a powerful
framework for Bayesian image restoration, enabling accurate point estimation as
well as advanced inference tasks, including uncertainty quantification and
visualization analyses, and empirical Bayesian inference for automatic model
parameter tuning. However, existing PnP Langevin algorithms are not well-suited
for low-photon Poisson imaging due to high solution uncertainty and poor
regularity properties, such as exploding gradients and non-negativity
constraints. To address these challenges, we propose two strategies for
extending Langevin PnP sampling to Poisson imaging models: (i) an accelerated
PnP Langevin method that incorporates boundary reflections and a Poisson
likelihood approximation and (ii) a mirror sampling algorithm that leverages a
Riemannian geometry to handle the constraints and the poor regularity of the
likelihood without approximations. The effectiveness of these approaches is
demonstrated through extensive numerical experiments and comparisons with
state-of-the-art methods.

Efficient Bayesian Computation Using Plug-and-Play Priors
for Poisson Inverse Problems
Teresa Klatzer1,4
Savvas Melidonis2
Marcelo Pereyra3,4
Konstantinos C. Zygalakis1,4
March 21, 2025
Abstract
This paper introduces a novel plug-and-play (PnP) Langevin sampling methodology for
Bayesian inference in low-photon Poisson imaging problems, a challenging class of problems
with significant applications in astronomy, medicine, and biology. PnP Langevin sampling
algorithms offer a powerful framework for Bayesian image restoration, enabling accurate
point estimation as well as advanced inference tasks, including uncertainty quantification
and visualization analyses, and empirical Bayesian inference for automatic model parame-
ter tuning. However, existing PnP Langevin algorithms are not well-suited for low-photon
Poisson imaging due to high solution uncertainty and poor regularity properties, such as
exploding gradients and non-negativity constraints. To address these challenges, we pro-
pose two strategies for extending Langevin PnP sampling to Poisson imaging models: (i)
an accelerated PnP Langevin method that incorporates boundary reflections and a Poisson
likelihood approximation and (ii) a mirror sampling algorithm that leverages a Rieman-
nian geometry to handle the constraints and the poor regularity of the likelihood without
approximations. The effectiveness of these approaches is demonstrated through extensive
numerical experiments and comparisons with state-of-the-art methods.
1
Introduction
Low-photon Poisson imaging problems are ubiquitous in scientific and engineering applications,
particularly in scenarios involving low illumination or short acquisition times (see, e.g., [58, 5, 21]
for excellent introductions to the topic). Poisson-distributed measurements arise for instance
from the use of single-photon detectors that discriminate individual photons within a given time
frame [2, 46, 56], as well as from standard CMOS cameras that operate under poorly illuminated
conditions [13]. As a result, Poisson imaging problems play a crucial role in astronomy and
remote sensing [26, 17, 47], where imaging systems often operate under limited illumination
conditions, in biomedical microscopy, where photon counts are limited to minimize photo-toxicity
[55], and in nuclear medical imaging [70, 57], where emission-based modalities such as PET and
SPECT produce Poisson-distributed measurement data with statistics that are directly related
to the amount of radiation used.
We herein consider Poisson imaging problems where one seeks to recover an unknown image
of interest x ∈Rn from a linear measurement y ∈Rm corrupted by Poisson noise, with likelihood
function given by
p(y|x) ∝
m
Y
i=1
exp
n
yi log(α(Ax)i) −α(Ax)i −ιR+
0 (xi)
o
,
(1)
1School of Mathematics, University of Edinburgh, Edinburgh, EH9 3FD, UK
2Forschungszentrum J¨ulich GmbH, 52425 J¨ulich, Germany
3School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh, EH14 4AS, UK
4Institute of Mathematics and its Applications, University of Edinburgh, Edinburgh, EH9 3HF, UK
∗Corresponding author. E-mail: teresa.klatzer@ed.ac.uk
The likelihood function is a Poisson-distributed random variable that represents the
probability of observing the measurement data under the assumption that the image x exists and
is in the correct spatial and intensity domain. The likelihood function is often parametrized
by

[Sample 29]
========================================
Title: Quantitative Evaluation of Quantum/Classical Neural Network Using a Game Solver Metric

Abstract: To evaluate the performance of quantum computing systems relative to
classical counterparts and explore the potential for quantum advantage, we
propose a game-solving benchmark based on Elo ratings in the game of
tic-tac-toe. We compare classical convolutional neural networks (CNNs), quantum
convolutional neural networks (QCNNs), and hybrid classical-quantum models by
assessing their performance against a random-move agent in automated matches.
Additionally, we implement a QCNN integrated with quantum communication and
evaluate its performance to quantify the overhead introduced by noisy quantum
channels. Our results show that the classical-quantum hybrid model achieves Elo
ratings comparable to those of classical CNNs, while the standalone QCNN
underperforms under current hardware constraints. The communication overhead
was found to be modest. These findings demonstrate the viability of using
game-based benchmarks for evaluating quantum computing systems and suggest that
quantum communication can be incorporated with limited impact on performance,
providing a foundation for future hybrid quantum applications.

Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.
Quantitative Evaluation of Quantum/Classical
Neural Network Using a Game Solver Metric
SUZUKAZE KAMEI1, HIDEAKI KAWAGUCHI2, SHIN NISHIO1, and Tatakahiko Satoh1
1Faculty of Science and Technology, Keio University, Yokohama, Kanagawa 223-8522 Japan
2Graduate School of Science and Technology, Keio University, Yokohama, Kanagawa 223-8522 Japan
Corresponding author: Suzukaze Kamei (e-mail: suzukaze_kamei@keio.jp).
SK, HK, SN, and TS are supported by JST Moonshot R&D Grant Number JPMJMS226C. SK and TS are also supported by JST
COI-NEXT Grant Number JPMJPF2221. SN is also supported by JSPS KAKENHI Grant Number JP22KJ1436. TS is also supported by
MEXT KAKENHI Grant Number 22K1978.
ABSTRACT To evaluate the performance of quantum computing systems relative to classical counterparts
and explore the potential for quantum advantage, we propose a game-solving benchmark based on Elo ratings
in the game of tic-tac-toe. We compare classical convolutional neural networks (CNNs), quantum convo-
lutional neural networks (QCNNs), and hybrid classical-quantum models by assessing their performance
against a random-move agent in automated matches. Additionally, we implement a QCNN integrated with
quantum communication and evaluate its performance to quantify the overhead introduced by noisy quantum
channels. Our results show that the classical-quantum hybrid model achieves Elo ratings comparable to those
of classical CNNs, while the standalone QCNN underperforms under current hardware constraints. The
communication overhead was found to be modest. These findings demonstrate the viability of using game-
based benchmarks for evaluating quantum computing systems and suggest that quantum communication
can be incorporated with limited impact on performance, providing a foundation for future hybrid quantum
applications.
INDEX TERMS Quantum Machine Learning, Quantum Communication, Quantum/Classical Algorithm,
Rating System, Tic-Tac-Toe Solver
I. INTRODUCTION
Q
UANTUM computers have brought new possibilities to
many previously difficult problems for classical com-
puters to solve [1], [2]. In recent years, the development of
quantum computers has made remarkable progress. Noise
Intermediate-Scale Quantum (NISQ) computers exceeding
1,000 qubits have emerged, and it is anticipated that early
Fault-Tolerant Quantum Computers (FTQC) will also appear
in the not-so-distant future [3]. With further advancements
in both hardware and algorithms, the potential for applying
quantum advantage to real-world problems is increasingly
becoming a reality.
Current benchmarks for quantum computers, such as ran-
dom circuit sampling, demonstrate quantum advantages in
limited tasks [4]. However, evidence supporting their superi-
ority in solving real-world problems remains inconclusive [5].
Furthermore, the absence of common and standardized met-
rics for computational performance presents a significant
challenge in evaluating the potential societal and practical
impact of future quantum computing technologies.
Classical game AIs, such as AlphaZero [6], have been
highly successful and have significantly outperformed hu-
man abilities in perfect-information zero-sum games. While
quantum game AIs have been proposed for solving certain
games [7, 8], there is currently no standard benchmark for
evaluating their performance.
To overcome these challenges, we propose a game-solving
benchmark based on Elo ratings in the game of tic-tac-toe.
This benchmark has been widely used in evaluating the
performance of classical game AIs [9].
In this benchmark, the quantum computing model
is evaluated against a random-move agent, and
the Elo rating is

[Sample 30]
========================================
Title: A Low-Power Streaming Speech Enhancement Accelerator For Edge Devices

Abstract: Transformer-based speech enhancement models yield impressive results.
However, their heterogeneous and complex structure restricts model compression
potential, resulting in greater complexity and reduced hardware efficiency.
Additionally, these models are not tailored for streaming and low-power
applications. Addressing these challenges, this paper proposes a low-power
streaming speech enhancement accelerator through model and hardware
optimization. The proposed high performance model is optimized for hardware
execution with the co-design of model compression and target application, which
reduces 93.9\% of model size by the proposed domain-aware and streaming-aware
pruning techniques. The required latency is further reduced with batch
normalization-based transformers. Additionally, we employed softmax-free
attention, complemented by an extra batch normalization, facilitating simpler
hardware design. The tailored hardware accommodates these diverse computing
patterns by breaking them down into element-wise multiplication and
accumulation (MAC). This is achieved through a 1-D processing array, utilizing
configurable SRAM addressing, thereby minimizing hardware complexities and
simplifying zero skipping. Using the TSMC 40nm CMOS process, the final
implementation requires merely 207.8K gates and 53.75KB SRAM. It consumes only
8.08 mW for real-time inference at a 62.5MHz frequency.

1
A Low-Power Streaming Speech Enhancement
Accelerator For Edge Devices
Ci-Hao Wu, and Tian-Sheuan Chang, Senior Member, IEEE
Abstract—Transformer-based
speech
enhancement
models
yield impressive results. However, their heterogeneous and com-
plex structure restricts model compression potential, resulting
in greater complexity and reduced hardware efficiency. Ad-
ditionally, these models are not tailored for streaming and
low-power applications. Addressing these challenges, this paper
proposes a low-power streaming speech enhancement accelerator
through model and hardware optimization. The proposed high
performance model is optimized for hardware execution with the
co-design of model compression and target application, which
reduces 93.9% of model size by the proposed domain-aware
and streaming-aware pruning techniques. The required latency
is further reduced with batch normalization-based transformers.
Additionally, we employed softmax-free attention, complemented
by an extra batch normalization, facilitating simpler hardware
design. The tailored hardware accommodates these diverse
computing patterns by breaking them down into element-wise
multiplication and accumulation (MAC). This is achieved through
a 1-D processing array, utilizing configurable SRAM addressing,
thereby minimizing hardware complexities and simplifying zero
skipping. Using the TSMC 40nm CMOS process, the final im-
plementation requires merely 207.8K gates and 53.75KB SRAM.
It consumes only 8.08 mW for real-time inference at a 62.5MHz
frequency.
Index Terms—speech enhancement, transformer, low power,
hardware implementation
I. INTRODUCTION
Deep learning-based speech enhancement (SE) surpasses
traditional methods in enhancing speech intelligibility and
quality. This enhancement is crucial for various natural lan-
guage processing (NLP) tasks, including speech recogni-
tion, machine translation, and hearing aids. Transformer-based
speech enhancement models, such as [1], [2], have received
significant attention in recent years due to their superior
performance and parallel computing capabilities relative to
other methods. The model shown in Fig. 1 is heterogeneous,
comprising an encoder and decoder that use convolutional
neural networks (CNN) for speech extraction and restoration.
In addition, it employs a masking module with transformers
to filter out noise. However, its large model size and com-
putational complexity become bottlenecks for low-power and
real-time edge applications. Moreover, these models aren’t
optimized for streaming applications.
Addressing the needs for real-time and low-power solutions,
this paper proposes a low-power design, achieved through
This work was supported by the National Science and Technology Coun-
cil, Taiwan, under Grant 111-2622-8-A49-018-SB, 110-2221-E-A49-148-
MY3, and 110-2218-E-A49-015-MBK. The authors are with the Institute of
Electronics, National Yang Ming Chiao Tung University, Taiwan. (e-mail:
wucihou.ee09g@nctu.edu.tw, tschang@nycu.edu.tw)
Manuscript received XXXX XX, 2023; Revised XXXX XX, 2024; Accepted XXX XX, 2024.
1
arXiv:2503.26156v1  [cs.CL]  26 Mar 2025

Figure 1. A high-performance transformer-based speech enhancement model.
2
arXiv:2503.26156v1  [cs.CL]  26 Mar 2025

A low-power streaming speech enhancement accelerator
for edge devices
1
2
3


[Sample 31]
========================================
Title: Exploring the flavor structure of leptons via diffusion models

Abstract: We propose a method to explore the flavor structure of leptons using
diffusion models, which are known as one of generative artificial intelligence
(generative AI). We consider a simple extension of the Standard Model with the
type I seesaw mechanism and train a neural network to generate the neutrino
mass matrix. By utilizing transfer learning, the diffusion model generates 104
solutions that are consistent with the neutrino mass squared differences and
the leptonic mixing angles. The distributions of the CP phases and the sums of
neutrino masses, which are not included in the conditional labels but are
calculated from the solutions, exhibit non-trivial tendencies. In addition, the
effective mass in neutrinoless double beta decay is concentrated near the
boundaries of the existing confidence intervals, allowing us to verify the
obtained solutions through future experiments. An inverse approach using the
diffusion model is expected to facilitate the experimental verification of
flavor models from a perspective distinct from conventional analytical methods.

KYUSHU-HET-313
Exploring the flavor structure of leptons via
diffusion models
Satsuki Nishimura, Hajime Otsuka, and Haruki Uchiyama
Department of Physics, Kyushu University, 744 Motooka, Nishi-ku, Fukuoka 819-0395, Japan
E-mail: nishimura.satsuki@phys.kyushu-u.ac.jp,
otsuka.hajime@phys.kyushu-u.ac.jp, uc.haruki496ym@gmail.com
Abstract: We propose a method to explore the flavor structure of leptons using diffusion
models, which are known as one of generative artificial intelligence (generative AI). We con-
sider a simple extension of the Standard Model with the type I seesaw mechanism and train
a neural network to generate the neutrino mass matrix. By utilizing transfer learning, the
diffusion model generates 104 solutions that are consistent with the neutrino mass squared
differences and the leptonic mixing angles. The distributions of the CP phases and the sums
of neutrino masses, which are not included in the conditional labels but are calculated from
the solutions, exhibit non-trivial tendencies. In addition, the effective mass in neutrinoless
double beta decay is concentrated near the boundaries of the existing confidence intervals,
allowing us to verify the obtained solutions through future experiments. An inverse ap-
proach using the diffusion model is expected to facilitate the experimental verification of
flavor models from a perspective distinct from conventional analytical methods.
arXiv:2503.21432v1  [hep-ph]  27 Mar 2025

Contents
1
Introduction
1
2
Flavor structure of leptons via the conditional diffusion models
3
2.1
Preliminaries
3
2.2
Conditional diffusion models
5
2.2.1
Diffusion process
5
2.2.2
Reverse process
7
2.2.3
Transfer learning
8
3
Results
9
4
Conclusions
12
A Formulation of diffusion models
14
A.1 Diffusion process and reverse process
14
A.2 Learning in diffusion process
15
A.3 Data generation in reverse process
17
B Conditional diffusion models
17
B.1
Classifier guidance
18
B.2
Classifier-free guidance
19
C Transfer learning
19
1
Introduction
The flavor structure of quarks and leptons is one of the intriguing puzzles in the Stan-
dard Model of particle physics. Current experimental and observational data suggest the
existence of massive neutrinos and large mixing angles in the lepton sector.
The flavor structure of neutrinos has been addressed in previous works using both top-
down and bottom-up approaches. In the top-down approach, we assume specific textures
for the Yukawa couplings of charged leptons and neutrinos, which originate from continu-
ous symmetries, such as a U(1) flavor symmetric model using the Froggatt-Nielsen (FN)
mechanism [1], non-Abelian discrete symmetries (see for reviews, Refs. [2–8]), modular
flavor symmetries [9] (see for reviews, Refs. [10, 11]) and selection rules without group
actions [12, 13]. These approaches lead to various textures of neutrino mass matrices at
the low-energy scale. Based on the derived textures of Yukawa couplings, one can predict
observables such as neutrino mass squared differences and leptonic mixing angles, which
are experimentally tested in future experiments. However, these approaches do not
explore the full flavor structure of neutrinos. In contrast, the bottom-up approach
explores the flavor structure by using the lepton mass matrix in a dynamical system.
For example, in the U(1) model, the neutrino mass matrix is used to simulate the
dynamical system with the type

[Sample 32]
========================================
Title: Local Stability and Stabilization of Quadratic-Bilinear Systems using Petersen's Lemma

Abstract: Quadratic-bilinear (QB) systems arise in many areas of science and
engineering. In this paper, we present a scalable approach for designing
locally stabilizing state-feedback control laws and certifying the local
stability of QB systems. Sufficient conditions are established for local
stability and stabilization based on quadratic Lyapunov functions, which also
provide ellipsoidal inner-estimates for the region of attraction and region of
stabilizability of an equilibrium point. Our formulation exploits Petersen's
Lemma to convert the problem of certifying the sign-definiteness of the
Lyapunov condition into a line search over a single scalar parameter. The
resulting linear matrix inequality (LMI) conditions scale quadratically with
the state dimension for both stability analysis and control synthesis, thus
enabling analysis and control of QB systems with hundreds of state variables
without resorting to specialized implementations. We demonstrate the approach
on three benchmark problems from the existing literature. In all cases, we find
our formulation yields comparable approximations of stability domains as
determined by other established tools that are otherwise restricted to systems
with up to tens of state variables.

arXiv:2503.21040v1  [eess.SY]  26 Mar 2025
Local Stability and Stabilization of
Quadratic-Bilinear Systems using Petersen’s Lemma
Amir Enayati Kafshgarkolaei a and Maziar S. Hemati a
aAerospace Engineering and Mechanics, University of Minnesota, Minneapolis, USA 55455
Abstract
Quadratic-bilinear (QB) systems arise in many areas of science and engineering. In this paper, we present a scalable approach
for designing locally stabilizing state-feedback control laws and certifying the local stability of QB systems. Suﬃcient conditions
are established for local stability and stabilization based on quadratic Lyapunov functions, which also provide ellipsoidal inner-
estimates for the region of attraction and region of stabilizability of an equilibrium point. Our formulation exploits Petersen’s
Lemma to convert the problem of certifying the sign-deﬁniteness of the Lyapunov condition into a line search over a single
scalar parameter. The resulting linear matrix inequality (LMI) conditions scale quadratically with the state dimension for both
stability analysis and control synthesis, thus enabling analysis and control of QB systems with hundreds of state variables
without resorting to specialized implementations. We demonstrate the approach on three benchmark problems from the
existing literature. In all cases, we ﬁnd our formulation yields comparable approximations of stability domains as determined
by other established tools that are otherwise restricted to systems with up to tens of state variables.
Key words: Region of Attraction, Quadratic Bilinear Systems, Petersen’s Lemma, Linear Matrix Inequality
1
Introduction
Quadratic-bilinear (QB) systems are nonlinear dynamical systems for which the dynamics are quadratic functions of
the state and bilinear functions of the state and input. When no inputs are present, QB systems reduce to quadratic
systems. QB systems arise in many areas of the natural, social, and applied sciences, including aerospace engineer-
ing [10], ﬂuid dynamics [28], chemical processes [6], atmospheric science [25], ecology [7], electrical circuits [15],
epidemiolgy [27], and robotics [31]. QB system models also arise naturally in second-order approximations of nonlin-
ear systems governed by analytic functions. As such, an ability to certify the stability of QB systems and to reliably
stabilize QB systems with feedback control stands to beneﬁt a variety of ﬁelds.
Tailored methods for stability analysis and control of QB systems have been developed by exploiting the underlying
problem structure associated with QB systems. However, as noted in [22], prevailing methods for analysis and control
of QB systems tend to be limited to low-order systems with state dimension n ≲O(10) due to computational
challenges; this is problematic because even reduced-order models of QB systems arising in many applications
commonly consist of n ∼O(10)–O(100) state variables [5,22]. For instance, it is possible to certify stability of a QB
system within a polytope, but problem size scales exponentially with state dimension, thus limiting the approach to
relatively small systems [2]. Related methods for stabilization of QB systems have also been proposed, but inherit
the same exponential increase in problem size with state dimension [12,22].
In this paper, we present a scalable approach for designing locally stabilizing state-feedback control laws and certifying
the local stability of QB systems. Our approach relies on quadratic Lyapunov functions to establish sufficient conditions
for local stability and stabilization of an equilibrium point. We also derive ellipsoidal inner-estimates for the region
of attraction and region of stabilizability of an equilibrium point. Our formulation

[Sample 33]
========================================
Title: Reconstructing Noisy Gene Regulation Dynamics Using Extrinsic-Noise-Driven Neural Stochastic Differential Equations

Abstract: Proper regulation of cell signaling and gene expression is crucial for
maintaining cellular function, development, and adaptation to environmental
changes. Reaction dynamics in cell populations is often noisy because of (i)
inherent stochasticity of intracellular biochemical reactions (``intrinsic
noise'') and (ii) heterogeneity of cellular states across different cells that
are influenced by external factors (``extrinsic noise''). In this work, we
introduce an extrinsic-noise-driven neural stochastic differential equation
(END-nSDE) framework that utilizes the Wasserstein distance to accurately
reconstruct SDEs from trajectory data from a heterogeneous population of cells
(extrinsic noise). We demonstrate the effectiveness of our approach using both
simulated and experimental data from three different systems in cell biology:
(i) circadian rhythms, (ii) RPA-DNA binding dynamics, and (iii) NF$\kappa$B
signaling process. Our END-nSDE reconstruction method can model how cellular
heterogeneity (extrinsic noise) modulates reaction dynamics in the presence of
intrinsic noise. It also outperforms existing time-series analysis methods such
as recurrent neural networks (RNNs) and long short-term memory networks
(LSTMs). By inferring cellular heterogeneities from data, our END-nSDE
reconstruction method can reproduce noisy dynamics observed in experiments. In
summary, the reconstruction method we propose offers a useful surrogate
modeling approach for complex biophysical processes, where high-fidelity
mechanistic models may be impractical.

Reconstructing Noisy Gene Regulation Dynamics Using Extrinsic-Noise-Driven
Neural Stochastic Differential Equations
Jiancheng Zhang1#, Xiangting Li2#, Xiaolu Guo3#∗, Zhaoyi You4, Lucas
Böttcher5,6, Alex Mogilner7, Alexander Hoffmann3, Tom Chou2,8∗, Mingtao Xia7,9∗
1Department of Electrical and Computer Engineering, University of California,
Riverside, CA, USA. 2Department of Computational Medicine,
University of California, Los Angeles, CA, USA.
3Department of Microbiology, Immunology, and Molecular Genetics (MIMG),
& Institute for Quantitative and Computational Biosciences,
University of California Los Angeles, Los Angeles, CA, USA.
4Ray and Stephanie Lane Computational Biology Department,
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA.
5Department of Computational Science and Philosophy,
Frankfurt School of Finance and Management, Frankfurt am Main, Germany.
6Laboratory for Systems Medicine, Department of Medicine, University of Florida, FL 32610, USA.
7Courant Institute of Mathematical Sciences, New York University, NY 10012, USA.
8Department of Mathematics, University of California, Los Angeles, CA 90095, USA.
9Department of Mathematics, University of Houston,
Houston, TX 77004, USA. (#indicates equal contribution) ∗
Proper regulation of cell signaling and gene expression is crucial for maintaining cellular function,
development, and adaptation to environmental changes. Reaction dynamics in cell populations is
often noisy because of (i) inherent stochasticity of intracellular biochemical reactions (“intrinsic
noise”) and (ii) heterogeneity of cellular states across different cells that are influenced by external
factors (“extrinsic noise”). In this work, we introduce an extrinsic-noise-driven neural stochastic
differential equation (END-nSDE) framework that utilizes the Wasserstein distance to accurately
reconstruct SDEs from trajectory data from a heterogeneous population of cells (extrinsic noise).
We demonstrate the effectiveness of our approach using both simulated and experimental data from
three different systems in cell biology: (i) circadian rhythms, (ii) RPA-DNA binding dynamics,
and (iii) NFκB signaling process. Our END-nSDE reconstruction method can model how cellular
heterogeneity (extrinsic noise) modulates reaction dynamics in the presence of intrinsic noise. It
also outperforms existing time-series analysis methods such as recurrent neural networks (RNNs)
and long short-term memory networks (LSTMs). By inferring cellular heterogeneities from data,
our END-nSDE reconstruction method can reproduce noisy dynamics observed in experiments. In
summary, the reconstruction method we propose offers a useful surrogate modeling approach for
complex biophysical processes, where high-fidelity mechanistic models may be impractical.
1
INTRODUCTION
Cell signaling is a fundamental process that regulates biological
functions. Gene regulation is a critical component of cell signaling, and
the molecular mechanisms of gene regulation are highly complex [1]. The
process of gene regulation is often described as a dynamic process, where
the activity of a gene can change over time. In this process, gene
expression

[Sample 34]
========================================
Title: On the non-integrability of driven-dissipative one-dimensional hard-core bosons

Abstract: In one dimension, in a closed system, the Tonks-Girardau gas is integrable
even for finite temperatures and time-evolving systems. We address the question
if it remains integrable once the system is no longer closed. We consider its
lattice version under incoherent local pump and loss and show by using random
matrix theory that the statistics of the complex spatial ratio indicates that
the system is chaotic. Further, we show that the model belongs in the
AI$^{\dagger}$ universality class of random matrices. In addition to this
analysis, we investigate an emergent stripe-pattern in the spectrum and relate
it to the dissipative parameters of the model.

On the non-integrability of driven-dissipative one-dimensional hard-core Bosons
M. Z¨undel1, 2, ∗
1Univ. Grenoble Alpes, CNRS, LPMMC, 38000 Grenoble, France
2Kirchhoff-Institut f¨ur Physik, Ruprecht-Karls-Universit¨at Heidelberg, INF 227, 69120 Heidelberg, Germany
In one dimension, in a closed system, the Tonks-Girardau gas is integrable even for finite temperatures and
time-evolving systems. We address the question if it remains integrable once the system is no longer closed. We
consider its lattice version under incoherent local pump and loss and show by using random matrix theory that
the statistics of the complex spatial ratio indicates that the system is chaotic. Further, we show that the model
belongs in the AI† universality class of random matrices. In addition to this analysis, we investigate an emergent
stripe-pattern in the spectrum and relate it to the dissipative parameters of the model.
I.
INTRODUCTION
The Lieb–Liniger model [1] is a paradigmatic example of
an integrable many-body system in one dimension [2].
In
the limit of infinitely strong interactions, it reduces to the
Tonks–Girardeau gas, which enjoys an exact solution obtained
through the Bose-Fermi mapping [3]. This even holds at finite
temperatures and under real-time evolution [4–6]. A natural
followup question then arises when the system is rendered
open—that is, coupled to an external environment or reser-
voirs. Does such coupling preserve the integrable structure,
or does it introduce effective interactions and decoherence that
spoil integrability?
Open quantum systems can be realized in various ways [7–
11], such as through coupling to a thermal bath, measurement-
induced decoherence, or engineered dissipation channels.
One rigorous method for describing the dynamics of the open
subsystem is the Lindblad – Gorini-Kossakowski-Sudarshan
master equation [12, 13]: This approach ensures that the time
evolution is completely positive and trace-preserving [13],
meaning that the density matrix remains physically valid at
all times. As a result, the Lindblad framework is widely used
to model dissipative processes in quantum mechanics, provid-
ing a consistent and microscopic description of open system
dynamics [10].
A straightforward way to test whether a closed quantum
system is integrable or chaotic is to examine the spectrum
of the relevant time-evolution operator, such as the Hamil-
tonian. Random matrix theory (RMT) predicts that the en-
ergy levels follow a Poissonian distribution for uncorrelated
level spacings, indicating that the systems is integrable, while
chaotic closed systems exhibit level repulsion characterized
by Wigner–Dyson statistics. The more convenient ratios of
energy level spacings in closed systems have been introduced
in [14, 15]. Furthermore, it has been conjectured [16] that for
chaotic systems, the spectral statistics of the Hamiltonian is
determined by its symmetry class.
An analogous approach to assess the integrability of open
quantum systems involves analyzing the complex level spac-
ings of the non-Hermitian operators that describe their dy-
namics. A suggestion of analyzing the distributions of com-
plex spacing ratios (CSR) has been put forward in [17, 18].
∗martina.zuendel@lpmmc.cnrs.fr
Their analysis suggests, that if the distribution is Poisson-like
then the system is integrable - given all symmetries of the
Hamiltonian. For a more detailed discussion of the CSR, see
[19].
For the Lieb–Liniger model, the Hamiltonian is given by
the sum of a kinetic term and a potential term, which is
given by the interaction energy
The kinetic term is given by
and the potential term by
where
is the density matrix,
is the annihilation operator, and
is the creation operator.
The Hamiltonian thus reads
The system is open to

[Sample 35]
========================================
Title: Maximal turbulence as a selection criterion for measure-valued solutions

Abstract: The quest for a good solution concept for the partial differential equations
(PDEs) arising in mathematical fluid dynamics is an outstanding open problem.
An important notion of solutions are the measure-valued solutions. It is well
known that for many PDEs there exists a multitude of measure-valued solutions
even if admissibility criteria like an energy inequality are imposed. Hence in
recent years, people have tried to select the relevant solutions among all
admissible measure-valued solutions or at least to rule out some solutions
which are not relevant.
  In this paper another such criterion is studied. In particular, we aim to
select generalized Young measures which are ``maximally turbulent''. To this
end, we look for maximizers of a certain functional, namely the variance, or
more precisely, the Jensen defect of the energy. We prove existence of such a
maximizer and we show that its mean value and total energy is uniquely
determined. Our theory is carried out in a very general setting which may be
applied in many situations where maximally turbulent measures shall be selected
among a set of generalized Young measures.
  Finally, we apply this general framework to the incompressible and the
isentropic compressible Euler equation. Our criterion of maximal turbulence is
plausible and leads to existence and uniqueness in a certain sense (in
particular, the mean value and the total energy of different maximally
turbulent solutions coincide).

arXiv:2503.20343v1  [math.AP]  26 Mar 2025
Maximal turbulence as a selection criterion for
measure-valued solutions
Christian Klingenberg∗
Simon Markfelder†
Emil Wiedemann‡
March 27, 2025
∗Julius-Maximilians-Universität Würzburg, Institute of Mathematics,
Emil-Fischer-Str. 40, 97074 Würzburg, Germany
† Universität Konstanz, Department of Mathematics and Statistics,
Post oﬃce box: 199, 78457 Konstanz, Germany
‡ Friedrich-Alexander-Universität Erlangen-Nürnberg, Department of Mathematics,
Cauerstraße 11, 91058 Erlangen, Germany
Abstract
The quest for a good solution concept for the partial diﬀerential equations (PDEs)
arising in mathematical ﬂuid dynamics is an outstanding open problem. An important
notion of solutions are the measure-valued solutions. It is well known that for many
PDEs there exists a multitude of measure-valued solutions even if admissibility criteria
like an energy inequality are imposed. Hence in recent years, people have tried to select
the relevant solutions among all admissible measure-valued solutions or at least to rule
out some solutions which are not relevant.
In this paper another such criterion is studied.
In particular, we aim to select
generalized Young measures which are “maximally turbulent”. To this end, we look for
maximizers of a certain functional, namely the variance, or more precisely, the Jensen
defect of the energy. We prove existence of such a maximizer and we show that its
mean value and total energy is uniquely determined. Our theory is carried out in a very
general setting which may be applied in many situations where maximally turbulent
measures shall be selected among a set of generalized Young measures.
Finally, we apply this general framework to the incompressible and the isentropic
compressible Euler equation. Our criterion of maximal turbulence is plausible and leads
to existence and uniqueness in a certain sense (in particular, the mean value and the
total energy of diﬀerent maximally turbulent solutions coincide).
∗e-mail: christian.klingenberg@uni-wuerzburg.de
†Corresponding author; e-mail: simon.markfelder@uni-konstanz.de
‡e-mail: emil.wiedemann@fau.de
1

Keywords: Measure-Valued Solutions, Admissibility Criteria, Turbulence, Incompressible
Euler Equations, Isentropic Compressible Euler Equations
MSC (2020) codes: 35D99, 76M30 (primary), 35Q31, 76B03, 76N10, 76F99 (secondary)
Contents
1
Introduction
2
2
A maximality criterion
5
2.1
Generalized Young measures............................
5.1
Maximally turbulent measures
8
2.2
The energy functional
11
3
Maximally turbulent measures for the incompressible Euler equation
13
3.1
The incompressible Euler equation
13.1
Generalized Young measures for the incompressible Euler equation
13.1.1
Maximally turbulent measures for the incompressible Euler equation
13.2
Maximally turbulent measures for the is

[Sample 36]
========================================
Title: Numerical solution of locally loaded Volterra integral equations

Abstract: Volterra's integral equations with local and nonlocal loads represent the
novel class of integral equations that have attracted considerable attention in
recent years. These equations are a generalisation of the classic Volterra
integral equations, which were first introduced by Vito Volterra in the late
19th century. The loaded Volterra integral equations are characterised by the
presence of a load which complicates the process of their theoretical and
numerical study. Sometimes these equation are called the equations with
``frozen'' argument. The present work is devoted to the study of Volterra
equations with locally loaded integral operators. The existence and uniquness
theorems are proved. Among the main contributions is the collocation method for
approximate solution of such equations based on the piecewise linear
approximation. To confirm the convergence of the method, a number of numerical
results for solving model problems are provided.

NUMERICAL SOLUTION OF LOCALLY LOADED VOLTERRA
INTEGRAL EQUATIONS
Byankin Vladislav
Melentiev Energy Systems Institute
of Siberian Branch of the Russian
Academy of Sciences
Irkutsk, 664033
byankinve@ex.istu.edu
Tynda Aleksandr
Penza state University
Penza, 440026
tyndaan@mail.ru
Sidorov Denis
Melentiev Energy Systems Institute
of Siberian Branch of the Russian
Academy of Sciences
Irkutsk, 664033
Harbin Institute of Technology
Harbin, 150001
dsidorov@isem.irk.ru
Dreglea Aliona
Irkutsk National Research Technical University
Irkutsk, 664074
Harbin Institute of Technology
Harbin, 150001
adreglea@ex.istu.edu
March 28, 2025
ABSTRACT
Volterra’s integral equations with local and nonlocal loads represent the novel class of integral equa-
tions that have attracted considerable attention in recent years. These equations are a generalisation
of the classic Volterra integral equations, which were first introduced by Vito Volterra in the late 19th
century. The loaded Volterra integral equations are characterised by the presence of a load which
complicates the process of their theoretical and numerical study. Sometimes these equation are called
the equations with “frozen” argument. The present work is devoted to the study of Volterra equations
with locally loaded integral operators. The existence and uniquness theorems are proved. Among the
main contributions is the collocation method for approximate solution of such equations based on the
piecewise linear approximation. To confirm the convergence of the method, a number of numerical
results for solving model problems are given in the paper.
1
Introduction
The term of equations with load was first introduced by Adam Nakhushev [1]. The studies of various differential and
integral equations make it possible to describe more complex processes in science and technologies. Work [2] devoted
to the Cauchy problem with a parameter perturbed by a linear functional, which generalises the conventional load.
The conditions for the existence of trivial and nontrivial solutions in the neighborhood of the bifurcation point are
considered. In the paper [3] the Fredholm integral equation of the second kind with load is considered. In the paper
we constructed methods for regular and irregular cases. Also, the papers have already considered the Hammerstein
equation with load in [4]. In this paper, the equation was studied through a series of nonlinear boundary value problems
using the Green’s function. In [6] the necessary and sufficient conditions for the spectra of the loaded Sturm–Liouville
operators, and authors used the term frozen argument instead of the load.
arXiv:2503.21452v1  [math.NA]  27 Mar 2025

A PREPRINT - MARCH 28, 2025
Let us consider, following the book [1], an integral equation of the form:
a0(t)x(t) +
m−1
X
j=1
aj(t)x(tj) = λ
Z t
t0
K(t, s)x(s)ds + f(t),
t ∈Ω= [t0, T]
(1)
where tj, (j = 1, 2,..., m −1) are defined fixed points of segment [t0, T], with t0 < t2 < · · · < tm−1 < tm = T. The
function a0(t) is a constant function. The function Xj(t) is a solution of the equation (1) with the fixed points of the segment [t0, T]
j = 1, 2,..., m −1. The function f(t) is a nonlinear function of t. The function K(t, s) is a nonlinear function of t and s.
The nonlinearity of the equation (

[Sample 37]
========================================
Title: Large Language Models for Zero-shot Inference of Causal Structures in Biology

Abstract: Genes, proteins and other biological entities influence one another via
causal molecular networks. Causal relationships in such networks are mediated
by complex and diverse mechanisms, through latent variables, and are often
specific to cellular context. It remains challenging to characterise such
networks in practice. Here, we present a novel framework to evaluate large
language models (LLMs) for zero-shot inference of causal relationships in
biology. In particular, we systematically evaluate causal claims obtained from
an LLM using real-world interventional data. This is done over one hundred
variables and thousands of causal hypotheses. Furthermore, we consider several
prompting and retrieval-augmentation strategies, including large, and
potentially conflicting, collections of scientific articles. Our results show
that with tailored augmentation and prompting, even relatively small LLMs can
capture meaningful aspects of causal structure in biological systems. This
supports the notion that LLMs could act as orchestration tools in biological
discovery, by helping to distil current knowledge in ways amenable to
downstream analysis. Our approach to assessing LLMs with respect to
experimental data is relevant for a broad range of problems at the intersection
of causal learning, LLMs and scientific discovery.

Preliminary work.
LARGE LANGUAGE MODELS FOR ZERO-SHOT
INFERENCE OF CAUSAL STRUCTURES IN BIOLOGY
Izzy Newsham1∗
Luka Kovaˇcevi´c1∗
Richard Moulange1∗
Nan Rosemary Ke2, 3
Sach Mukherjee4,1
1MRC Biostatistics Unit, University of Cambridge, Cambridge, UK
2DeepMind, London, UK
3Mila, Montreal, Canada
4DZNE & University of Bonn, Bonn, Germany
{izzy.newsham, sach.mukherjee}@mrc-bsu.cam.ac.uk
ABSTRACT
Genes, proteins and other biological entities influence one another via causal molec-
ular networks. Causal relationships in such networks are mediated by complex and
diverse mechanisms, through latent variables, and are often specific to cellular context.
It remains challenging to characterise such networks in practice. Here, we present a
novel framework to evaluate large language models (LLMs) for zero-shot inference of
causal relationships in biology. In particular, we systematically evaluate causal claims
obtained from an LLM using real-world interventional data. This is done over one
hundred variables and thousands of causal hypotheses. Furthermore, we consider sev-
eral prompting and retrieval-augmentation strategies, including large, and potentially
conflicting, collections of scientific articles. Our results show that with tailored augmen-
tation and prompting, even relatively small LLMs can capture meaningful aspects of
causal structure in biological systems. This supports the notion that LLMs could act
as orchestration tools in biological discovery, by helping to distil current knowledge in
ways amenable to downstream analysis. Our approach to assessing LLMs with respect to
experimental data is relevant for a broad range of problems at the intersection of causal
learning, LLMs and scientific discovery.
1
INTRODUCTION
Discovery in many scientific disciplines is complex and often expensive and time-consuming. In biol-
ogy, the large number of interacting components creates an enormous space of potential experiments
to perform. Experimental plans, as well as joint experimental-computational workflows, are usually
informed by existing literature (e.g. on a particular disease or biomolecular pathway), but this step is
itself challenging. If large language models (LLMs) are capable of reducing even a small fraction of
the work needed to elucidate biological mechanisms, or compress literature-derived knowledge into
priors for in silico models, they have the potential to significantly accelerate biological discovery and
understanding of disease mechanisms.
To date, often without fine-tuning on biological data, LLMs have been applied in biochemistry as
experimental orchestration tools (M. Bran et al., 2024), AI bioinformaticians (Ding et al., 2024),
collaborative multi-agent teams of AI scientists (Swanson et al., 2024) or as a source for pre-trained
embeddings for downstream prediction tasks (Chen & Zou, 2024). However, to accelerate science,
LLMs must be capable of elucidating causal relationships, which is highly relevant in the context of
studying molecular networks underpinning disease biology.
In experimental settings conducive to LLM-driven orchestration, researchers often face large experi-
mental search spaces. For example, in the study of cellular signaling pathways, researchers
may be interested in determining the effects of perturbing a subset of genes on downstream
metabolism, protein abundance, and other properties. A typical experimental design would involve
manipulating genes, measuring the resulting changes in protein abundance, and inferring the
causal relationship between gene perturbation and protein abundance (i.e. inferring the
causal structure of the network).
To determine causal relationships,

[Sample 38]
========================================
Title: A Unified Image-Dense Annotation Generation Model for Underwater Scenes

Abstract: Underwater dense prediction, especially depth estimation and semantic
segmentation, is crucial for gaining a comprehensive understanding of
underwater scenes. Nevertheless, high-quality and large-scale underwater
datasets with dense annotations remain scarce because of the complex
environment and the exorbitant data collection costs. This paper proposes a
unified Text-to-Image and DEnse annotation generation method (TIDE) for
underwater scenes. It relies solely on text as input to simultaneously generate
realistic underwater images and multiple highly consistent dense annotations.
Specifically, we unify the generation of text-to-image and text-to-dense
annotations within a single model. The Implicit Layout Sharing mechanism (ILS)
and cross-modal interaction method called Time Adaptive Normalization (TAN) are
introduced to jointly optimize the consistency between image and dense
annotations. We synthesize a large-scale underwater dataset using TIDE to
validate the effectiveness of our method in underwater dense prediction tasks.
The results demonstrate that our method effectively improves the performance of
existing underwater dense prediction models and mitigates the scarcity of
underwater data with dense annotations. We hope our method can offer new
perspectives on alleviating data scarcity issues in other fields. The code is
available at https: //github.com/HongkLin/TIDE.

A Unified Image-Dense Annotation Generation Model for Underwater Scenes
Hongkai Lin
Dingkang Liang
Zhenghao Qi
Xiang Bai*
Huazhong University of Science and Technology
{hklin,dkliang,xbai}@hust.edu.cn
Image
Mask
Depth
Depth
Image
Mask
Figure 1. We present TIDE, a unified underwater image-dense annotation generation model. Its core lies in the shared layout
information and the natural complementarity between multimodal features. Our model, derived from the text-to-image model and fine-
tuned with underwater data, enables the generation of highly consistent underwater image-dense annotations from solely text conditions.
Abstract
Underwater dense prediction, especially depth estimation
and semantic segmentation, is crucial for gaining a com-
prehensive understanding of underwater scenes. Neverthe-
less, high-quality and large-scale underwater datasets with
dense annotations remain scarce because of the complex
environment and the exorbitant data collection costs. This
paper proposes a unified Text-to-Image and DEnse anno-
tation generation method (TIDE) for underwater scenes.
It relies solely on text as input to simultaneously generate
realistic underwater images and multiple highly consistent
dense annotations. Specifically, we unify the generation of
* Corresponding author.
text-to-image and text-to-dense annotations within a sin-
gle model. The Implicit Layout Sharing mechanism (ILS)
and cross-modal interaction method called Time Adaptive
Normalization (TAN) are introduced to jointly optimize the
consistency between image and dense annotations. We syn-
thesize a large-scale underwater dataset using TIDE to val-
idate the effectiveness of our method in underwater dense
prediction tasks. The results demonstrate that our method
effectively improves the performance of existing underwater
dense prediction models and mitigates the scarcity of un-
derwater data with dense annotations. We hope our method
can offer new perspectives on alleviating data scarcity is-
sues in other fields.
The code is available at https:
//github.com/HongkLin/TIDE.
1
arXiv:2503.21771v1  [cs.CV]  27 Mar 2025

1. Introduction
Underwater dense prediction, particularly depth estimation
and semantic segmentation, is essential for underwater ex-
ploration and environmental monitoring.
However, the
complex environment and the prohibitive data collection
costs result in a scarcity of underwater data with dense an-
notations. Such conditions severely hinder the advancement
of dense prediction technologies in underwater scenes.
Fortunately, the recent success of the image generative
technique [14, 29, 43] provides a breakthrough in address-
ing the scarcity of underwater scene data.
In the field
of general object understanding, controllable data synthe-
sis [17, 23, 31, 38] demonstrates its effectiveness in few-
shot scenarios. A straightforward solution is to apply them
to underwater scenes directly. For instance, Atlantis [42],
a pioneering controllable generation method for underwater
depth data that takes ControlNet as its core, utilizes terres-
trial depth maps as conditions. It effectively mitigates the
issue of data scarcity by generating realistic underwater
depth data.
However,
the complexity of underwater scenes necessitates a more
comprehensive approach to generate dense annotations.
The
current research mainly focuses on the generation of
dense semantic segmentation masks, with the goal of
obtaining a complete underwater scene understanding.
For example,
the Mask-based Underwater Scene Segmentation (MUS)
framework [4, 12, 22] leverages the Mask-based

[Sample 39]
========================================
Title: Efficient Crystal Structure Prediction Using Genetic Algorithm and Universal Neural Network Potential

Abstract: Crystal structure prediction (CSP) is crucial for identifying stable crystal
structures in given systems and is a prerequisite for computational atomistic
simulations. Recent advances in neural network potentials (NNPs) have reduced
the computational cost of CSP. However, searching for stable crystal structures
across the entire composition space in multicomponent systems remains a
significant challenge. Here, we propose a novel genetic algorithm (GA) -based
CSP method using a universal NNP. Our GA-based methods are designed to
efficiently expand convex hull volumes while preserving the diversity of
crystal structures. This approach draws inspiration from the similarity between
convex hull updates and Pareto front evolution in multi-objective optimization.
Our evaluation shows that the present method outperforms the symmetry-aware
random structure generation, achieving a larger convex hull with fewer trials.
We demonstrated that our approach, combined with the developed universal NNP
(PFP), can accurately reproduce and explore phase diagrams obtained through DFT
calculations; this indicates the validity of PFP across a wide range of crystal
structures and element combinations. This study, which integrates a universal
NNP with a GA-based CSP method, highlights the promise of these methods in
materials discovery.

Efficient Crystal Structure Prediction Using Genetic Algorithm and Universal Neural
Network Potential
Takuya Shibayama,1 Hideaki Imamura,1 Katsuhiko Nishimra,1 Kohei
Shinohara,1, ∗Chikashi Shinagawa,1 So Takamoto,1 and Ju Li2
1Preferred Networks, Inc., Tokyo 100-0004, Japan.
2Department of Nuclear Science and Engineering,
and Department of Materials Science and Engineering,
Massachusetts Institute of Technology, Cambridge, MA 02139, USA.
(Dated: March 28, 2025)
Crystal structure prediction (CSP) is crucial for identifying stable crystal structures in given
systems and is a prerequisite for computational atomistic simulations. Recent advances in neural
network potentials (NNPs) have reduced the computational cost of CSP. However, searching for
stable crystal structures across the entire composition space in multicomponent systems remains a
significant challenge. Here, we propose a novel genetic algorithm (GA) -based CSP method using a
universal NNP. Our GA-based methods are designed to efficiently expand convex hull volumes
while preserving the diversity of crystal structures.
This approach draws inspiration from the
similarity between convex hull updates and Pareto front evolution in multi-objective optimization.
Our evaluation shows that the present method outperforms the symmetry-aware random structure
generation, achieving a larger convex hull with fewer trials. We demonstrated that our approach,
combined with the developed universal NNP (PFP), can accurately reproduce and explore phase
diagrams obtained through DFT calculations; this indicates the validity of PFP across a wide range
of crystal structures and element combinations. This study, which integrates a universal NNP with
a GA-based CSP method, highlights the promise of these methods in materials discovery.
I.
INTRODUCTION
Computational
atomistic
simulations
based
on
a
quantum-mechanical description enhances our under-
standing of materials and even contribute to modern ma-
terials design [1].
Crystal structure prediction (CSP),
a process to predict stable crystal structures in given
systems, is a crucial prerequisite to harness the com-
putational atomic simulations [2–5].
Despite the well-
established methodology for analyzing crystal structures
from experimental data, CSP is essential for accelerating
materials discovery through the ab initio approach.
While CSP plays an integral role in predicting sta-
ble crystal structures, it represents a daunting global
optimization task owning to the vast energy landscape.
Substituting elements from known crystal structure pro-
totypes is a prevailing method for generating candi-
date structures [6].
Although the prototype substitu-
tion method often gives reasonable results, its coverage
falls short, especially in multicomponent systems. Meta-
heuristic algorithms offer another approach to creating
novel crystal structures. These include methods such as
random structure search [7], basin hopping [8], minima
hopping [9, 10], genetic algorithm (GA) [11–15], par-
ticle swarm optimization [16], and Bayesian optimiza-
tion [17, 18].
Typically, these CSP methods are com-
bined with density functional theory (DFT) calculations
to evaluate the formation energy of the candidate struc-
tures. However, the time-consuming DFT calculations,
∗Corresponding author. E-mail address:
katsuhiko.nishimra@prefnet.com.
arxiv:2503.25055v1  [cond-mat.na.mcs]  27 Mar 2025
(Dated: 28 Mar 2025)

1.
Preferred Networks, Inc., Tokyo 100-0004, Japan.
2.
Department of Nuclear Science and Engineering, and
Department of Materials Science and

[Sample 40]
========================================
Title: Random 2D linear cocycles I: dichotomic behavior

Abstract: In this paper we establish a Bochi-Ma\~n\'e type dichotomy in the space of
two dimensional, nonnegative determinant matrix valued, locally constant linear
cocycles over a Bernoulli or Markov shift. Moreover, we prove that Lebesgue
almost every such cocycle has finite first Lyapunov exponent, which then
implies a break in the regularity of the Lyapunov exponent, from analyticity to
discontinuity.

RANDOM 2D LINEAR COCYCLES I:
DICHOTOMIC BEHAVIOR
PEDRO DUARTE, MARCELO DUR˜AES, TOM´E GRAXINHA,
AND SILVIUS KLEIN
Abstract. In this paper we establish a Bochi-Ma˜n´e type dichotomy
in the space of two dimensional, nonnegative determinant matrix
valued, locally constant linear cocycles over a Bernoulli or Markov
shift. Moreover, we prove that Lebesgue almost every such cocy-
cle has finite first Lyapunov exponent, which then implies a break
in the regularity of the Lyapunov exponent, from analyticity to
discontinuity.
1. Introduction and statements
Let (X, µ, f) be a measure preserving dynamical system, where the
state space X is a compact metric space, the transformation f : X →X
is continuous and µ is an f-invariant, ergodic Borel probability measure
on X.
Given a bounded, measurable function A: X →Matm(R),
we call a linear cocycle over the base dynamics (X, µ, f) with fiber
dynamics driven by A the skew-product map F : X × Rm →X × Rm
defined by
F(x, v) = (f(x), A(fx)v).
The iterates of this new dynamical system are F n(x, v) = (f n(x), An(x)v),
where for all n ∈N,
An(x) := A(f nx) · · · A(f 2x) A(fx).
The first Lyapunov exponent of a linear cocycle F measures the
asymptotic exponential growth of its fiber iterates. It is defined via
the Furstenberg-Kesten theorem (the analogue of the additive ergodic
theorem for multiplicative systems) as the µ-a.e. limit
L1(F) = L1(A) := lim
n→∞
1
n log∥An(x)∥
provided A satisfies the integrability condition
R
X log+∥A∥dµ < ∞.
Moreover, if the norm (that is, the first singular value) of the fiber
iterates is replaced by the second singular value, the corresponding µ-
a.e. limit above still exists, it is called the second Lyapunov exponent
and it is denoted by L2(F) = L2(A).
1
arXiv:2503.21050v1  [math.DS]  26 Mar 2025

2
P. DUARTE, M. DUR˜AES, T. GRAXINHA, AND S. KLEIN
An important problem in ergodic theory—with deep applications
elsewhere, e.g.
in mathematical physics—concerns the regularity of
the Lyapunov exponent as a function of the input data (e.g. the fiber
map A, or the measure µ or even the base transformation f).1 By reg-
ularity we mean its continuity properties (or lack thereof), including
its modulus of continuity, its smoothness or analyticity in an appro-
priate setting etc. It turns out that this is a difficult problem in any
context, its resolution strongly depending on the topology of the space
of cocycles, on the regularity of the fiber map and on the type of base
dynamics considered.
Indeed, a version of the Bochi-Ma˜n´e dichotomy theorem in the con-
text of linear cocycles states that given any measure preserving dy-
namical system (X, µ, f) where f is a an aperiodic (meaning its set of
periodic points has measure zero) homeomorphism, for any fiber map
A: X →SL2(R), either the cocycle associated with A is hyperbolic
over the support of µ, or it is not. In the first case, the cocycle
is called a Bochi-Ma˜n´e cocycle.
2
arXiv:2503.21050v1  [math.DS]  26 Mar 2025

3
The Bochi-Ma˜n´e cocycles are of central importance in ergodic
theory. Indeed, they form a natural candidate for the ergodic set of
a measure preserving dynam

[Sample 41]
========================================
Title: The First Hardware Demonstration of a Universal Programmable RRAM-based Probabilistic Computer for Molecular Docking

Abstract: Molecular docking is a critical computational strategy in drug design and
discovery, but the complex diversity of biomolecular structures and flexible
binding conformations create an enormous search space that challenges
conventional computing methods. Although quantum computing holds promise for
these challenges, it remains constrained by scalability, hardware limitations,
and precision issues. Here, we report a prototype of a probabilistic computer
(p-computer) that efficiently and accurately solves complex molecular docking
for the first time, overcoming previously encountered challenges. At the core
of the system is a p-computing chip based upon our artificial tunable
probabilistic bits (p-bits), which are compatible with computing-in-memory
schemes, based upon 180 nm CMOS technology and BEOL HfO2 RRAM. We successfully
demonstrated the superior performance of the p-computer in practical
ligand-protein docking scenarios. A 42-node molecular docking problem of
lipoprotein with LolA-LolCDE complex-a key point in developing antibiotics
against Gram-negative bacteria, was successfully solved. Our results align well
with the Protein-Ligand Interaction Profiler tool. This work marks the first
application of p-computing in molecular docking-based computational biology,
which has great potential to overcome the limitations in success rate and
efficiency of current technologies in addressing complex bioinformatics
problems.

1 
 
The First Hardware Demonstration of a Universal Programmable 
RRAM-based Probabilistic Computer for Molecular Docking  
Yihan He1†, Ming-Chun Hong2,3†,Qiming Ding4, Chih-Sheng Lin2,3, Chih-Ming Lai3, Chao Fang1, 
Xiao Gong1, Tuo-Hung Hou2,5*, and Gengchiau Liang1,5* 
1Department of Electrical and Computer Engineering, National University of Singapore, 117583 Singapore 
2Department of Electrical Engineering and Institute of Electronics, National Yang-Ming Chiao Tung 
University, Hsinchu, Taiwan  
3Electronic and Optoelectronic System Research Laboratories, Industrial Technology Research Institute, 
Hsinchu, Taiwan 
4Center on Frontiers of Computing Studies, Peking University, Beijing 100871, China 
5Industry Academia Innovation School, National Yang-Ming Chiao Tung University, Hsinchu, Taiwan 
†These authors contribute equally;  *Email: thhou@nycu.edu.tw; gcliang@nycu.edu.tw 
Molecular docking is a critical computational strategy in drug design and discovery, but the 
complex diversity of biomolecular structures and flexible binding conformations create an 
enormous search space that challenges conventional computing methods. Although 
quantum computing holds promise for these challenges, it remains constrained by scalability, 
hardware limitations, and precision issues. Here, we report a prototype of a probabilistic 
computer (p-computer) that efficiently and accurately solves complex molecular docking for 
the first time, overcoming previously encountered challenges. At the core of the system is a 
p-computing chip based upon our artificial tunable probabilistic bits (p-bits), which are 
compatible with computing-in-memory schemes, based upon 180 nm CMOS technology and 
BEOL HfO2 RRAM. We successfully demonstrated the superior performance of the p-
computer in practical ligand-protein docking scenarios. A 42-node molecular docking 
problem of lipoprotein with LolA-LolCDE complex—a key point in developing antibiotics 
against Gram-negative bacteria, was successfully solved. Our results align well with the 
Protein-Ligand Interaction Profiler tool. This work marks the first application of p-
computing in molecular docking-based computational biology, which has great potential to 
overcome the limitations in success rate and efficiency of current technologies in addressing 
complex bioinformatics problems. 
Introduction 
In recent years, probabilistic computing (p-computing), as an emerging unconventional 
computational paradigm, has garnered significant attention due to its unique advantages in 
solving combinatorial optimization problems (COPs), such as integer factorization1–5, the 
traveling salesman problem (TSP)6,7, and Boolean satisfiability (SAT)8–10. The fundamental 
computing unit of p-computing is the probabilistic bit (p-bit)4,11–17, which exhibits a tunable 
probability of being 1. The p-bit can be implemented as a physical device, such as a logic gate, or as a 
programmable memory element, such as a memory cell. One of the most promising implementations of 
p-bits is the resistive random-access memory (RRAM)18,19, which can be programmable to exhibit 
specific properties and functionalities. 
The probabilistic nature of p-bits enables them to handle complex

[Sample 42]
========================================
Title: Four-twist effects on excitations in symmetric orbifold CFTs

Abstract: Symmetric orbifold CFTs contain twist operators that can join and split
copies of the CFT. In this paper, we study the effects of four twist-2
operators on two copies of a single free boson. A recent study analyzed their
effects on the vacuum, finding a nontrivial left-right mixing that arises from
the fact that the covering surface is a torus, while the effects of one or two
twist-2 operators do not produce such mixing. Here, we extend this analysis to
excited states and find a similar left-right mixing. Furthermore, we explore
the continuum, or high-energy, limit and show that the left-right mixing
becomes negligible in this limit.

Four-twist effects on excitations
in symmetric orbifold CFTs
Bin Guo†1 and
Shaun D. Hampton‡2
† School of Physics, Central South University,
Changsha 418003, China
‡ School of Physics,
Korea Institute for Advanced Study,
Seoul 02455, Korea
Abstract
Symmetric orbifold CFTs contain twist operators that can join and split copies of
the CFT. In this paper, we study the effects of four twist-2 operators on two copies
of a single free boson. A recent study analyzed their effects on the vacuum, finding
a nontrivial left-right mixing that arises from the fact that the covering surface is a
torus, while the effects of one or two twist-2 operators do not produce such mixing.
Here, we extend this analysis to excited states and find a similar left-right mixing.
Furthermore, we explore the continuum, or high-energy, limit and show that the
left-right mixing becomes negligible in this limit.
1guobin@csu.edu.cn
2sdh2023@kias.re.kr
arXiv:2503.21644v1  [hep-th]  27 Mar 2025

Contents
1
Introduction
2
2
Effect of four twist operators
3
2.1
Symmetric product orbifold CFT.......................
3
2.2
Monodromy...................................
4
2.3
The effect of four twist operators
.......................
5
2.4
Propagation...................................
7
2.5
Contraction...................................
8
2.6
Review of the correlators............................
9
2.7
Properties of the correlators..........................
11
3
Propagation for four twists
11
3.1
Left moving propagation............................
11
3.2
left-right moving propagation
.........................
16
4
Contraction for four twists
17
4.1
Left moving contraction
............................
17
4.2
Left-right moving contraction
.........................
21
4.3
Relating contraction and pair creation
....................
22
5
Numerical analysis
23
6
Some limits
27
6.1
Continuum Limit..................................
27
6.2
High energy limit
28
7
Conclusion
29
8
Acknowledgements
31
8
Index
32
1
Introduction
Symmetric orbifold CFTs contain twist operators that can join and split copies of
the CFT. In this paper, we study the effects of four twist-2 operators on two copies
of a single free boson. A recent study analyzed their effects on the vacuum,

[Sample 43]
========================================
Title: On a class of nonlinear BGK-type kinetic equations with density dependent collision rates

Abstract: We consider a class of nonlinear, spatially inhomogeneous kinetic equations
of BGK-type with density dependent collision rates. These equations share the
same superlinearity as the Boltzmann equation, and fall into the class of run
and tumble equations appearing in mathematical biology. We prove that the
Cauchy problem is well-posed, and the solutions propagate Maxwellian bounds
over time. Moreover, we show that the solutions approach to equilibrium with an
exponential rate, known as a hypocoercivity result. Lastly, we derive a class
of nonlinear diffusion equations as the hydrodynamic limit of the kinetic
equations in the diffusive scaling, employing both hypocoercivity and relative
entropy methods. The limit equations cover a wide range of nonlinear diffusion
equations including both the porous medium and the fast diffusion equations.

arXiv:2503.14225v1  [math.AP]  18 Mar 2025
On a class of nonlinear BGK-type kinetic equations with density
dependent collision rates
Josephine Evans∗
Daniel Morris†
Havva Yoldaş†‡
March 19, 2025
Abstract
We consider a class of nonlinear, spatially inhomogeneous kinetic equations of BGK-type with
density dependent collision rates. These equations share the same superlinearity as the Boltzmann
equation, and fall into the class of run and tumble equations appearing in mathematical biology.
We prove that the Cauchy problem is well-posed, and the solutions propagate Maxwellian bounds
over time. Moreover, we show that the solutions approach to equilibrium with an exponential rate,
known as a hypocoercivity result. Lastly, we derive a class of nonlinear diﬀusion equations as the
hydrodynamic limit of the kinetic equations in the diﬀusive scaling, employing both hypocoerciv-
ity and relative entropy methods. The limit equations cover a wide range of nonlinear diﬀusion
equations including both the porous medium and the fast diﬀusion equations.
Contents
1
Introduction
2
1.1
The models...........................................
3
1.2
Motivations and open questions................................
5
1.3
State of the art.........................................
6
2
Preliminary and main results
7
2.1
Main results...........................................
7
2.2
Preliminary results on hydrodynamic quantities.......................
8
3
Well-posedness
9
4
Long-time behaviour
10
4.1
L2-hypocoercivity à la Dolbeault-Mouhot-Schmeiser....................
10
5
Diﬀusive asymptotics
13
5.1
Long-time asymptotics.....................................
13
5.2
Finite-time asymptotics
....................................
14
∗Warwick Mathematics Institute, University of Warwick, Zeeman Building, Coventry CV4 7AL, United Kingdom.
josephine.evans@warwick.ac.uk
†Delft Institute of Applied Mathematics, Faculty of Electrical Engineering, Mathematics and Computer Science, Delft
University of Technology, Mekelweg 4, 2628CD Delft, The Netherlands. d.n.l.morris@tudelft.nl & h.yoldas@tudelft.nl
‡Theoretical Sciences Visiting Program, Institute of Mathematics and Computer Science, University of California, Berkeley, CA, 94720, USA.
Havva Yoldaş
arXiv:2503.14225v1  [math.AP]  18 Mar 2025
Introduction
We consider a class of nonlinear, spatially inhomogeneous kinetic equations of BGK-type with density
dependent collision rates. These equations share the same superlinearity as the Boltzmann equation, and fall


[Sample 44]
========================================
Title: The Procedural Content Generation Benchmark: An Open-source Testbed for Generative Challenges in Games

Abstract: This paper introduces the Procedural Content Generation Benchmark for
evaluating generative algorithms on different game content creation tasks. The
benchmark comes with 12 game-related problems with multiple variants on each
problem. Problems vary from creating levels of different kinds to creating rule
sets for simple arcade games. Each problem has its own content representation,
control parameters, and evaluation metrics for quality, diversity, and
controllability. This benchmark is intended as a first step towards a
standardized way of comparing generative algorithms. We use the benchmark to
score three baseline algorithms: a random generator, an evolution strategy, and
a genetic algorithm. Results show that some problems are easier to solve than
others, as well as the impact the chosen objective has on quality, diversity,
and controllability of the generated artifacts.

The Procedural Content Generation Benchmark:
An Open-source Testbed for Generative Challenges in Games
Ahmed Khalifa
Institute of Digital Games
University of Malta
Msida, Malta
ahmed@akhalifa.com
Roberto Gallotta
Institute of Digital Games
University of Malta
Msida, Malta
roberto.gallotta@um.edu.mt
Matthew Barthet
Institute of Digital Games
University of Malta
Msida, Malta
matthew.barthet@um.edu.mt
Antonios Liapis
Institute of Digital Games
University of Malta
Msida, Malta
antonios.liapis@um.edu.mt
Julian Togelius
Game Innovation Lab
New York University
New York, New York, USA
julian@togelius.com
Georgios N. Yannakakis
Institute of Digital Games
University of Malta
Msida, Malta
georgios.yannakakis@um.edu.mt
Abstract
This paper introduces the Procedural Content Generation Bench-
mark for evaluating generative algorithms on different game con-
tent creation tasks. The benchmark comes with 12 game-related
problems with multiple variants on each problem. Problems vary
from creating levels of different kinds to creating rule sets for sim-
ple arcade games. Each problem has its own content representation,
control parameters, and evaluation metrics for quality, diversity,
and controllability. This benchmark is intended as a first step to-
wards a standardized way of comparing generative algorithms. We
use the benchmark to score three baseline algorithms: a random
generator, an evolution strategy, and a genetic algorithm. Results
show that some problems are easier to solve than others, as well
as the impact the chosen objective has on quality, diversity, and
controllability of the generated artifacts.
CCS Concepts
• Applied computing →Computer games; • Theory of com-
putation →Evolutionary algorithms.
Keywords
Procedural Content Generation, Search-Based Generation, Evolu-
tionary Algorithms, Evaluation, Benchmark
ACM Reference Format:
Ahmed Khalifa, Roberto Gallotta, Matthew Barthet, Antonios Liapis, Julian
Togelius, and Georgios N. Yannakakis. 2025. The Procedural Content Gen-
eration Benchmark: An Open-source Testbed for Generative Challenges in
Games. In International Conference on the Foundations of Digital Games (FDG
’25), April 15–18, 2025, Graz, Austria. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3723498.3723794
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
FDG ’25, April 15–18, 2025, Graz, Austria
© 2025 Copyright held by the owner/author(s).
ACM ISBN /25/04
https://doi.org/10.1145/3723498.3723794
1
Introduction
Scientific and technological progress requires reliable means of
measuring the performance of methods and machines. Therefore,
extensive efforts focus on developing tools and techniques for such
measurements within various scientific and engineering fields. In
Artificial Intelligence (AI), benchmarks are crucial to the field: most
AI papers include a comparison with the state of the art on some
benchmark. In Reinforcement Learning (RL) research, benchmarks are
essential for evaluating algorithms.
Procedural Content Generation (PCG) is an emerging area of
AI research, which aims to automate the process of creating game
content through the use of algorithms. The term refers to a set of
methods that generate content procedurally, that is, in an automated
manner. The goal of PCG is to create game content that is challenging
and engaging for the player, while also ensuring that the content
remains

[Sample 45]
========================================
Title: Function Alignment: A New Theory for Mind and Intelligence, Part I: Foundations

Abstract: This paper introduces function alignment, a novel theory of mind and
intelligence that is both intuitively compelling and structurally grounded. It
explicitly models how meaning, interpretation, and analogy emerge from
interactions among layered representations, forming a coherent framework
capable not only of modeling minds but also of serving as a blueprint for
building them. One of the key theoretical insights derived from function
alignment is bounded interpretability, which provides a unified explanation for
previously fragmented ideas in cognitive science, such as bounded rationality,
symbol grounding, and analogy-making. Beyond modeling, the function alignment
framework bridges disciplines often kept apart, linking computational
architecture, psychological theory, and even contemplative traditions such as
Zen. Rather than building on any philosophical systems, it offers a structural
foundation upon which multiple ways of understanding the mind may be
reconstructed.

Function Alignment:
A New Theory for Mind and Intelligence
Part I: Foundations
Gus G. Xia
Music X Lab, Machine Learning Department
Mohamed bin Zayed University of Artificial Intelligence
“The created universe carries Yin at the back and Yang in
front, and through the union of the pervading principle, it
reaches harmony.”
—Laozi, Dao De Jing, Chapter 42
H
uman perception operates across multiple levels of abstraction simultaneously. For
example, when we listen to music, we perceive raw acoustic signals at the most
basic level, interpret musical scores at a higher level, and recognize even more
abstract structures such as chords and forms. Representations at different levels function like
distinct languages, each with its own semantics. Yet, these levels of representation influence
one another, and we rely on such interactions to better understand and predict the world.
Consider music again as an example: a trained musician with theoretical knowledge can
better anticipate upcoming low-level acoustic events, while an improviser attuned to the
nuances of low-level musical flow can make more informed decisions about which note to
play next.
To model such entangled dynamics of hierarchical representations during perception, I
propose function alignment as a new theory of mind and intelligence in this position
paper. Note that this is not a technical paper that introduces concrete solutions but rather
a methodological perspective.
As shown in the graphical model in Figure 1, the y-sequence represents the true dynamics
of physical reality, the x-sequence captures the low-level representation in the human mind,
and the z-sequence corresponds to a higher-level, more abstract representation. While
additional layers of representation may exist above z, we limit our illustration to two levels
for clarity.
1
arXiv:2503.21106v1  [cs.CL]  27 Mar 2025

Figure 1: An illustration of function alignment: x-z is function aligned, while x-y is not.
In this framework, the dynamics of x and z are defined as function aligned, characterized
by the following three key properties:
1. Both x and z serve as functional descriptions of y—they encode different levels
of abstraction of the same underlying reality.
2. Both x and z are auto-regressive processes, dynamically influencing each
other’s predictions. Unlike typical hierarchical models where higher layers pas-
sively summarize lower ones, here x and z actively “listen” to each other, forming a
bidirectional alignment.
3. x and z are aligned in time, where “time” refers not strictly to physical time but
to a generalized logical sequence that governs inference and decision-making.
The second property, represented by the diagonal cross-connections, is the most critical
distinction between function alignment and conventional hierarchical time-series models.
These cross-level, cross-time influences enable mutual adaptation rather than one-way
abstraction.
Notably, x and y are not function-aligned, as indicated by the absence of diagonal
connections—x merely passively models the underlying reality y without influencing it.
For instance, Newtonian mechanics provides a macro-scale description of physical reality
based on underlying microscopic particle interactions, but the law F = ma does not affect
subatomic physics. Another example of a non-function-aligned hierarchical structure can
be seen in programming: a Python program is interpreted into lower-level C-executable
behavior, but the relationship is unidirectional. Such “concealed” hierarchical structures
arise from a lack of function alignment. In contrast, the dynamics of the x-sequence and
the z-sequence are deeply entangled and aligned—they are both perceptual representations,
interconnected through neural mechanisms, and capable of shaping each other’s evolution.
2
arXiv:2503.21106v1  [cs.CL]  27 Mar 2025

The first property, x-z function-alignment, implies that x and z encode the same
information at different levels of abstraction. For example, when a human sees a painting,
the visual system first processes high-level features such as color and shape, which are
then interpreted into lower-level details like brushstrokes and textures. These low-level


[Sample 46]
========================================
Title: Optimizing Resource Allocation and Scheduling towards FRMCS and GSM-R networks coexistence in Railway Systems

Abstract: The actual railway communication system used in Europe for high-speed trains
(HST) is called the GSM-R system, which is a communication system based on 2G
infrastructure. This system is meant to be replaced by a new system based on 5G
NR infrastructure called the Future Railway Mobile Communication System (FRMCS)
by 2030. For the next years, both systems will probably coexist in the same
frequency band since the migration from GSM-R to FRMCS is planned to be done
progressively until the GSM-R system is completely shut down, mainly due to
safety and budget constraints. In this paper, we study the resource allocation
for the FRMCS system sharing the same frequency band as the already deployed
GSM-R system. We formulate the resource allocation problem as an integer linear
problem (ILP), known to be NP-hard.To solve it in a reasonable time, we propose
a scheduling algorithm, called Intelligent Traffic Scheduling Preemptor (ITSP),
that allocates resources for the different FRMCS traffic types considered
(critical traffic and performance traffic) in the same frequency band with the
GSM-R system. Our algorithm is channel quality Indicator (CQI) aware and uses
the preemption mechanism in 5G NR standards to optimize the resource allocation
for the FRMCS system without impacting the actual GSM-R resource allocation in
the context of the white space concept.

Optimizing Resource Allocation and
Scheduling towards FRMCS and GSM-R
networks coexistence in Railway Systems
Mohamed Aziz Aboud∗‡, Nawel Zangar‡, Rami Langar∗‡, Marion Berbineau‡, and Jerome Madec§
∗Software and IT Engineering Department, Ecole de Technologie Sup´erieure (´ETS), Montr´eal, QC H3C 1K3, Canada
‡ LIGM-CNRS UMR 8049, University Gustave Eiffel, F-77420 Marne−la-Vall´ee, France
§ SNCF-R´eseaux, Direction T´el´ecom Unifi´ee, 93574 La Plaine-St-Denis, France
E-mails: mohamed-aziz.aboud.1@ens.etsmtl.ca; rami.langar@etsmtl.ca;
{nawel.zangar, marion.berbineau}@univ-eiffel.fr; jerome.madec@reseau.sncf.fr
Abstract—The actual railway communication system
used in Europe for high-speed trains (HST) is called
the GSM-R system, which is a communication system
based on 2G infrastructure. This system is meant to be
replaced by a new system based on 5G NR infrastructure
called the Future Railway Mobile Communication Sys-
tem (FRMCS) by 2030. For the next years, both systems
will probably coexist in the same frequency band since
the migration from GSM-R to FRMCS is planned to be
done progressively until the GSM-R system is completely
shut down, mainly due to safety and budget constraints.
In this paper, we study the resource allocation for
the FRMCS system sharing the same frequency band
as the already deployed GSM-R system. We formulate
the resource allocation problem as an integer linear
problem (ILP), known to be NP-hard.To solve it in a
reasonable time, we propose a scheduling algorithm,
called Intelligent Traffic Scheduling Preemptor (ITSP),
that allocates resources for the different FRMCS traffic
types considered (critical traffic and performance traffic)
in the same frequency band with the GSM-R system. Our
algorithm is channel quality Indicator (CQI) aware and
uses the preemption mechanism in 5G NR standards to
optimize the resource allocation for the FRMCS system
without impacting the actual GSM-R resource allocation
in the context of the white space concept.
Index Terms—HST, GSM-R, FRMCS, 5G NR, re-
source allocation, critical traffic, performance traffic,
preemption, white space concept.
I. INTRODUCTION
With the rapid development of 5G New Radio (NR)
and beyond communication systems, a new railway
communication system for High-Speed Trains is also
being developed. The new system called the Future
Railway Mobile Communication System (FRMCS) is
a dedicated system designed for the HST and will
be used to provide a safe, secure and reliable
communication link for the train driver, the control
center and the passengers. The FRMCS system is
based on 5G NR infrastructure and will be fully
deployed by 2030 [1].
FRMCS is a high-speed communication system
that provides an all-encompassing communication
service for the HST. It is designed to allow
a seamless communication between the

[Sample 47]
========================================
Title: Finite-temperature charge and spin transport in the one-dimensional Hubbard model accounting for its global [SU (2) X SU(2) X U(1)]/Z_2^2$ symmetry

Abstract: Using a general representation that accounts for the effects on
finite-temperature spin and charge transport of the global [SU (2) X SU(2) X
U(1)]/(Z2 X Z2) symmetry of the one-dimensional (1D) Hubbard model, we show
that important finite-temperature transport quantities as the finite-field spin
and finite-chemical potential charge stiffnesses and the zero-field spin and
zero-chemical potential charge diffusion constants are controlled by
microscopic processes associated with the spin and charge elementary currents
carried by the spin and charge carriers, respectively. We describe the model
finite-temperature transport properties in terms of the microscopic processes
associated with the spin and charge elementary currents carried by the
corresponding carriers. We expect that the general representation used in our
study is that suitable to address the open problems on finite-temperature
transport in the 1D Hubbard model also discussed in this paper.

Finite-temperature charge and spin transport in the one-dimensional Hubbard model
accounting for its global [SU(2) × SU(2) × U(1)]/Z2
2 symmetry
J. M. P. Carmelo1, 2 and J. E. C. Carmelo3, 2
1Center of Physics of University of Minho and University of Porto, LaPMET, P-4169-007 Oporto, Portugal
2CeFEMA, Instituto Superior T´ecnico, Universidade de Lisboa,
LaPMET, Av.
Rovisco Pais, P-1049-001 Lisboa, Portugal
3CFTC, Universidade de Lisboa, Fac. Ciˆencias, P-1749-016 Lisboa, Portugal
Most studies on the one-dimensional (1D) Hubbard model with transfer integral t and on-site
repulsion U have assumed that for u = U/4t > 0 and h = µ = 0, where h denotes the magnetic
field and µ the chemical potential, its global symmetry is SO(4) = [SU(2) × SU(2)]/Z2. However,
both in 1D and for the U > 0 Hubbard model on any bipartite lattice it is actually larger and
given by [SU(2) × SU(2) × U(1)]/Z2
2 = [SO(4) × U(1)]/Z2. The studies of this paper account for
the irreducible representations of that symmetry to identify both the spin and charge carriers that
populate each of the energy eigenstates that span that model full Hilbert space.
The global τ-
translational U(1) symmetry beyond SO(4) is found to describe the relative translational degrees of
freedom of such spin and charge carriers, respectively. We show that important finite-temperature
transport quantities as the h > 0 spin and µ > 0 charge stiffnesses and the h = 0 spin and µ = 0
charge diffusion constants are controlled by microscopic processes associated with the spin and
charge elementary currents carried by these spin and charge carriers, respectively. We describe the
model finite-temperature transport properties in terms of the microscopic processes associated with
the spin and charge elementary currents carried by the corresponding carriers. We expect that the
general τ- and physical α-spin representation used in our study is that suitable to address the open
problems on finite-temperature transport in the 1D Hubbard model also discussed in this paper.
I.
INTRODUCTION
Over the years the Hubbard model [1, 2] with trans-
fer integral t and on-site repulsion U has become more
important, for it plays an essential role in several top-
ics in condensed-matter physics and related quantum
problems. For instance, the one-dimensional (1D) Hub-
bard model defined in a lattice of length L and with Na
sites [3–7] is the paradigmatic quantum system for low-
dimensional strongly correlated electron systems and ma-
terials.
It is well known that at zero magnetic field and zero
chemical potential the 1D Hubbard model has a global
SO(4) = [SU(2)×SU(2)]/Z2 symmetry whose two global
SU(2) symmetries refer to spin and η-spin, respectively
[8]. On the other hand, the local SU(2) × SU(2) × U(1)
gauge symmetry of both the 1D Hubbard model and that of the
U > 0 Hubbard model on any bipartite lattice is given by
[9, 10].
However, both in 1D and for the U > 0 Hubbard model
on any bipartite lattice it is actually larger and given
by [SU(2) × SU(2) × U(1)]/Z2 = [SO(4) × U(1)]/Z2.
The studies of this paper account for the irreduc

[Sample 48]
========================================
Title: On the number of asynchronous attractors in AND-NOT Boolean networks

Abstract: Boolean Networks (BNs) describe the time evolution of binary states using
logic functions on the nodes of a network. They are fundamental models for
complex discrete dynamical systems, with applications in various areas of
science and engineering, and especially in systems biology. A key aspect of the
dynamical behavior of BNs is the number of attractors, which determines the
diversity of long-term system trajectories. Due to the noisy nature and
incomplete characterization of biological systems, a stochastic asynchronous
update scheme is often more appropriate than the deterministic synchronous one.
AND-NOT BNs, whose logic functions are the conjunction of literals, are an
important subclass of BNs because of their structural simplicity and their
usefulness in analyzing biological systems for which the only information
available is a collection of interactions among components. In this paper, we
establish new theoretical results regarding asynchronous attractors in AND-NOT
BNs. We derive two new upper bounds for the number of asynchronous attractors
in an AND-NOT BN based on structural properties (strong even cycles and
dominating sets, respectively) of the AND-NOT BN. These findings contribute to
a more comprehensive understanding of asynchronous dynamics in AND-NOT BNs,
with implications for attractor enumeration and counting, as well as for
network design and control.

On the number of asynchronous attractors in
AND-NOT Boolean networks
Van-Giang Trinh1, Samuel Pastva2, Jordan Rozum3, and Kyu Hyong Park4
and Réka Albert4
1 Inria Saclay, EP Lifeware, Palaiseau, France
van-giang.trinh@inria.fr
2 Faculty of Informatics, Masaryk University, Botanicka 68a, Brno, 60200, Czechia
xpastva@fi.muni.cz
3 Department of Systems Science and Industrial Engineering, Binghamton
University, Engineering Building, Vestal, 13850, New York, USA
jrozum@binghamton.edu
4 Department of Physics, Pennsylvania State University, Davey Laboratory,
University Park, 16802, Pennsylvania, USA
{kjp5774,rza1}@psu.edu
Abstract. Boolean Networks (BNs) describe the time evolution of bi-
nary states using logic functions on the nodes of a network. They are
fundamental models for complex discrete dynamical systems, with ap-
plications in various areas of science and engineering, and especially in
systems biology. A key aspect of the dynamical behavior of BNs is the
number of attractors, which determines the diversity of long-term sys-
tem trajectories. Due to the noisy nature and incomplete characterization
of biological systems, a stochastic asynchronous update scheme is often
more appropriate than the deterministic synchronous one. AND-NOT
BNs, whose logic functions are the conjunction of literals, are an im-
portant subclass of BNs because of their structural simplicity and their
usefulness in analyzing biological systems for which the only information
available is a collection of interactions among components.
In this paper, we establish new theoretical results regarding asynchronous
attractors in AND-NOT BNs. We derive two new upper bounds for the
number of asynchronous attractors in an AND-NOT BN based on struc-
tural properties (strong even cycles and dominating sets, respectively) of
the AND-NOT BN. These findings contribute to a more comprehensive
understanding of asynchronous dynamics in AND-NOT BNs, with im-
plications for attractor enumeration and counting, as well as for network
design and control.
Keywords: logical modeling · AND-NOT Boolean network · asynchronous
dynamics · attractor · influence graph · symbolic AI.
1
Introduction
Boolean Networks (BNs) are discrete dynamical systems widely used to model
complex interactions in various domains, ranging from sciences to engineering,
arXiv:2503.19147v1  [cs.DM]  24 Mar 2025

2
Trinh et al.
notably systems biology [25]. In these models, each variable represents a binary-
state component that evolves over time according to an update scheme. The
dynamics of a BN are well specified by the set of Boolean update functions and
the chosen update scheme, making it a powerful tool for studying regulatory and
decision-making processes in real-world systems [7,21].
The study of asynchronous attractors—the stable domains that arise un-
der the asynchronous update scheme—is crucial for analyzing the stability and
robustness of BNs [23]. Unlike the synchronous update schemes, in which
each node is updated simultaneously, the asynchronous update scheme involves
each node being updated at its own pace. The asynchronous update scheme
has been widely used in the modeling of biological systems [26,27].
It is particularly suited for systems biology because the time evolution of
bio-logical systems is often described by a set of nonlinear differential
equations with noisy parameters [28,29]. In these systems, the noisy parameters
can be treated as sto

[Sample 49]
========================================
Title: RapidPoseTriangulation: Multi-view Multi-person Whole-body Human Pose Triangulation in a Millisecond

Abstract: The integration of multi-view imaging and pose estimation represents a
significant advance in computer vision applications, offering new possibilities
for understanding human movement and interactions. This work presents a new
algorithm that improves multi-view multi-person pose estimation, focusing on
fast triangulation speeds and good generalization capabilities. The approach
extends to whole-body pose estimation, capturing details from facial
expressions to finger movements across multiple individuals and viewpoints.
Adaptability to different settings is demonstrated through strong performance
across unseen datasets and configurations. To support further progress in this
field, all of this work is publicly accessible.

RapidPoseTriangulation: Multi-view Multi-person Whole-body
Human Pose Triangulation in a Millisecond
Daniel Bermuth
ISSE
University of Augsburg, Germany
daniel.bermuth@uni-a.de
Alexander Poeppel
ISSE
University of Augsburg
poeppel@isse.de
Wolfgang Reif
ISSE
University of Augsburg
reif@isse.de
Abstract
The integration of multi-view imaging and pose estimation
represents a significant advance in computer vision appli-
cations, offering new possibilities for understanding human
movement and interactions. This work presents a new algo-
rithm that improves multi-view multi-person pose estima-
tion, focusing on fast triangulation speeds and good gener-
alization capabilities. The approach extends to whole-body
pose estimation, capturing details from facial expressions
to finger movements across multiple individuals and view-
points. Adaptability to different settings is demonstrated
through strong performance across unseen datasets and
configurations. To support further progress in this field, all
of this work is publicly accessible.
1. Introduction
The ability to accurately detect and track human body posi-
tions and movements is a very important task in many ad-
vanced computer vision applications. From enhancing vir-
tual reality experiences to improving the collaboration with
humans in robotic systems, precise human pose estimation
plays a crucial role in bridging the gap between digital sys-
tems and the physical world.
Human pose estimation, which involves determining the
spatial locations of different body joints, has seen signifi-
cant progress in recent years. While traditional approaches
relied on wearable markers for precise tracking, the field
has increasingly moved towards marker-less methods using
standard cameras. This shift offers greater flexibility and
usefulness, especially in scenarios where attaching mark-
ers is impractical or impossible, such as in public spaces or
specialized environments like operating rooms.
However, marker-less pose estimation presents its own
set of challenges. Occlusions, varying lighting conditions,
and the complexity of human movements can all impact the
accuracy and reliability of these systems. To address these
issues, multi-view approaches have become more common.
Figure 1. Example of a multi-person whole-body pose estimation
from multiple camera views in a volleyball game (from the egohu-
mans dataset [14]). On top, the full image of one camera with the
projections of all the detected poses, on the bottom-left a zoom-in
on one player, and on the bottom-right her detected 3D pose.
By capturing the scene from multiple angles simultane-
ously, these systems can overcome many of the limitations
found in single-view methods.
Recent advances in deep learning have dramatically im-
proved the accuracy of pose estimation from individual
camera views. However, the task of efficiently combining
1
arXiv:2503.21692v1  [cs.CV]  27 Mar 2025

these multiple 2D estimates into accurate 3D poses, partic-
ularly for multiple people in real-time scenarios, remains
an active area of research. Furthermore, as applications be-
come more advanced, there is an increasing demand for a
more detailed whole-body pose estimation, including facial
expressions and especially hand gestures (see Figure 1).
This work addresses these challenges by introducing a
novel algorithm for multi-view multi-person human pose
estimation. The approach is designed to be both fast and ca-
pable of generalizing well across different datasets and ap-
plication setups. Unlike most previous works, the algorithm
is also able to estimate whole-body poses. In a broad evalu-
ation across multiple datasets, it is shown that the proposed
algorithm is more reliable than existing methods, while also
being significantly faster, which makes it especially suitable
for real-time applications. The source-code of the method is
available at: https://git.isse.uni-augsburg.de/mars/pars/rapidpose
To the best of our knowledge, this is the first algorithm
that can efficiently estimate 3D human poses from multiple
view cameras in real-time, including whole-body and hand
gestures.
2. Related Work
The field of human pose estimation has seen significant
progress in recent years, driven by the availability of
large-scale datasets and the increasing power of deep learning

[Sample 50]
========================================
Title: Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection

Abstract: In this work, we propose a metric called Number of Thoughts (NofT) to
determine the difficulty of tasks pre-prompting and support Large Language
Models (LLMs) in production contexts. By setting thresholds based on the number
of thoughts, this metric can discern the difficulty of prompts and support more
effective prompt routing. A 2% decrease in latency is achieved when routing
prompts from the MathInstruct dataset through quantized, distilled versions of
Deepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this
metric can be used to detect adversarial prompts used in prompt injection
attacks with high efficacy. The Number of Thoughts can inform a classifier that
achieves 95% accuracy in adversarial prompt detection. Our experiments ad
datasets used are available on our GitHub page:
https://github.com/rymarinelli/Number_Of_Thoughts/tree/main.

Harnessing Chain-of-Thought Metadata for Task
Routing and Adversarial Prompt Detection
Ryan Marinelli1[0009−0001−7279−3156], Josef Pichlmeier2, and
Tamas Bisztray1[0000−0003−2626−3434]
1 University of Oslo, Oslo, Norway
https://www.mn.uio.no/ifi/english/research/groups/sec/index.html
ryanma@ifi.uio.no
2 BMW Group, Ludwig-Maximilians-Universität, Munich, Germany
Josef.Pichlmeier@bmw.de
Abstract. In this work, we propose a metric called “Number of Thoughts
(NofT)” to determine the difficulty of tasks pre-prompting and sup-
port Large Language Models (LLMs) in production contexts. By set-
ting thresholds based on the number of thoughts, this metric can discern
the difficulty of prompts and support more effective prompt routing.
A 2% decrease in latency is achieved when routing prompts from the
MathInstruct dataset through quantized, distilled versions of Deepseek
with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this
metric can be used to detect adversarial prompts used in prompt in-
jection attacks with high efficacy. The Number of Thoughts can inform
a classifier that achieves 95% accuracy in adversarial prompt detection.
Our experiments ad datasets used are available on our GitHub page:
https://github.com/rymarinelli/Number_Of_Thoughts/tree/main.
Keywords: Adversarial Prompts · Routing Optimization · Number of
Thoughts
1
Introduction
Large Language Models (LLMs) have recently emerged as powerful tools capable
of supporting significant advances in complex tasks such as software development
and automated theorem proving, demonstrating impressive problem-solving ca-
pabilities despite lacking explicit symbolic reasoning mechanisms [13,3,19]. At
the core of the success of these models is the attention mechanism [26], en-
abling them to contextually generate coherent outputs beyond the capabilities
of traditional sequential models, such as recurrent neural networks (RNNs). A
widely adopted strategy to enhance this capability is the Chain-of-Thought
(CoT) prompting technique [29], which encourages LLMs to explicitly gener-
ate intermediate problem-solving steps before arriving at a final answer. Al-
though LLMs do not incorporate an explicit symbolic reasoning module, this
step-by-step method—when properly prompted or fine-tuned—consistently im-
proves performance on multi-step tasks. Notably, OpenAI has recently indicated
arXiv:2503.21464v1  [cs.CL]  27 Mar 2025

2
Marinelli et al.
a strategic move toward incorporating CoT-like processes into the architecture
rather than relying solely on prompting [18]. This shift underscores the impor-
tance of exploring methods for leveraging intermediate step data—whether to
route computations more efficiently or to detect adversarial logic—before such
capabilities become natively embedded and potentially opaque to end users.
When a CoT model is approaching a problem, it will segment the task into
a number of intermediate steps to manage its complexity. Consequently, our un-
derlying hypothesis is that the number of steps produced will reflect the difficulty
of the task, and also indicate the presence of deceptive logic, as observed in ad-
versarial prompting scenarios. Our work centers around three research questions
to explore the utilization of CoT-derived meta-information:
– RQ1: Can the number of reasoning steps (NofT) be leveraged to optimize
task routing for LLMs?
– RQ2: Can NofT be used to detect adversarial prompting?
– RQ3: Can a classifier be trained to predict the difficulty of tasks
based on the number of reasoning steps?
We present two novel contributions. First, we demonstrate that the
number of steps generated by LLMs can be used to detect the difficulty of
tasks and support more efficient task routing,

[Sample 51]
========================================
Title: Homotopy kinematic algebras at null infinity

Abstract: We present the first formulation of a homotopy algebra adapted to a $1/r$
expansion near future null infinity ($\mathcal{I^+}$). Focusing on self-dual
Yang-Mills theory in Bondi coordinates, we demonstrate that imposing the
homotopy algebra relations naturally yields the physically consistent fall-off
behavior of the fields near $\mathcal{I^+}$. Furthermore, we employ this
framework to systematically construct kinematic algebras, uncovering novel
infinite families of such algebras that satisfy the Jacobi identity on slices
near $\mathcal{I^+}$.

Prepared for submission to JHEP
Homotopy kinematic algebras at null infinity
Felipe D´ıaz-Jaramillo,a Silvia Nagy,b Giorgio Pizzolob
aInstitute for Physics, Humboldt University Berlin, Zum Großen Windkanal 6, D-12489 Berlin,
Germany
bDepartment of Mathematical Sciences, Durham University, Durham, DH1 3LE, UK
E-mail: felipe.diaz-jaramillo@hu-berlin.de, silvia.nagy@durham.ac.uk,
giorgio.pizzolo@durham.ac.uk
Abstract:
We present the first formulation of a homotopy algebra adapted to a 1/r
expansion near future null infinity (I+). Focusing on self-dual Yang-Mills theory in Bondi
coordinates, we demonstrate that imposing the homotopy algebra relations naturally yields
the physically consistent fall-off behavior of the fields near I+. Furthermore, we employ
this framework to systematically construct kinematic algebras, uncovering novel infinite
families of such algebras that satisfy the Jacobi identity on slices near I+.
arXiv:2503.21035v1  [hep-th]  26 Mar 2025

Contents
1
Introduction
1
2
Physical motivation
4
2.1
Color-kinematics duality and the kinematic algebra
4
2.2
Review of physical fall-off at null infinity
6
3
Homotopy algebras
8
3.1
L∞algebras
8
3.2
C∞algebras
12
3.3
Kinematic algebras and BV□
∞algebras
14
4
Homotopy algebra formulation of SDYM in Bondi coordinates
17
4.1
L∞algebra
17
4.2
Color stripping and kinematic algebra
18
5
Physical fall-offs from L∞algebras
20
5.1
Requirements on the L∞algebra near I
20
5.2
Solutions of the master system
23
5.2.1
Projection PN
24
5.2.2
Projection πN
24
5.2.3
An infinite family of L∞algebras
26
6
Strict kinematic algebras
28
6.1
Kinematic algebras on slices
28
6.2
Strict kinematic algebras on refined slices
30
6.2.1
Strict kinematic algebra for S−1
32
6.2.2
Strict kinematic algebra for Sk≤−2
34
7
Fall-offs and quasi-isomorphism
36
7.1
Cochain maps and quasi-isomorphisms
36
7.2
Cochain map conditions
38
7.3
Fall-off from cochain map conditions
39
7.4
( ¯
X, ¯B1, ¯B2) is an L∞sub-algebra
40
8
Conclusions
41
A Construction of the master system’s solutions
42
A.1 Properties of PN
42
A.2 Conditions on the power selection sets
44
A.2.1
Cochain map condition
44
A.2.2
Leibniz rule condition
45
– i –

A.2.3
Physically relevant solutions
51
A.2.4
Master system for N
52
A.3 Solutions of the master system for N
53
B Other proofs
58
B.1
Kin(k)
58
B.2
Kin(k)
58
C.1
Kinematic algebras
60
C.2
Kinematic algebra for N
62
C.2.1
Kinematic algebra for N
62
C.2.2
Kinematic algebras
64
C.3
Kinematic algebras for N
64
C.3.1
Kinematic algebras
64
C.3.2

[Sample 52]
========================================
Title: A double-spiral spin ordering in the helimagnet YBaCuFeO$_{5}$

Abstract: Materials with a spiral spin ordering always show a rich phase diagram and
can be a playground for studying the exotic physical properties associated with
spiral magnetism. Using neutron elastic and resonant x-ray scattering on a
high-quality single crystal YBaCuFeO$_{5}$, we demonstrate YBaCuFeO$_{5}$ to be
a helimagnet consisting of a double-spiral spin ordering. YBaCuFeO$_{5}$
undergoes a commensurate to incommensurate magnetic phase transition at
$T_{N2}$$\sim$ 175 K, and the incommensurate phase consists of two spin-ordered
components. Both components have different periodicities but with the same
propagating direction along the {\it c}-axis below $T_{N2}$. Using resonant
x-ray scattering at the Fe and Cu \textit{K}-edges, we further demonstrate that
both spiral spin orderings result from the Fe$^{3+}$ and Cu$^{2+}$,
respectively, forming a double-spiral spin ordering structure. This can be
understood to be caused by the coupling between both sublattices of Fe$^{3+}$
and Cu$^{2+}$ with the atomic lattice.

A double-spiral spin ordering in the helimagnet
YBaCuFeO5
Yu-Hui Lianga,b, Chun-Hao Laia,b, Chin-Wei Wangc, Shinichiro Yanoc,
Daisuke Okuyamad, Taku J. Satoe, Yusuke Nambuf,g,h, Shih-Chang Wengc,
Yen-Chung Laic, Wei-Tin Cheni,j,k, Kirrily C. Rule1,∗, Chao-Hung Dua,b,∗∗
aDepartment of Physics, Tamkang University, New Taipei City, 251301, Taiwan
bCenter of Advanced Spectroscopy and Smart Inspection for Material Research, Tamkang
University, New Taipei City, 251301, Taiwan
cNational Synchrotron Radiation Research Center, Hsinchu, 300092, Taiwan
dInstitute of Materials Structure Science (IMSS), High Energy Accelerator Research
Organization (KEK), Ibaraki, 305-0801, Japan
eInstitute of Multidisciplinary Research for Advanced Materials, Tohoku
University, Sendai, 980-8577, Japan
fInstitute for Materials Research, Tohoku University, Sendai, 980-8577, Japan
gOrganization for Advanced Studies, Tohoku University, Sendai, 980-8577, Japan
hFOREST, Japan Science and Technology Agency, Saitama, 332-0012, Japan
iCenter for Condensed Matter Sciences, National Taiwan
University, Taipei, 10617, Taiwan
jCenter of Atomic Initiative for New Materials, National Taiwan
University, Taipei, 10617, Taiwan
kTaiwan Consortium of Emergent Crystalline Materials, National Science and
Technology Council, Taipei, 10622, Taiwan
lAustralian Center for Neutron Scattering, Australian Nuclear Science and Technology
Organization, NSW, 2232, Australia
Abstract
∗Corresponding author: kirrilyr@gmail.com
∗∗Corresponding author: chd@gms.tku.edu.tw
Preprint submitted to Journal of Magnetism and Magnetic Materials
March 28, 2025
arXiv:2503.21561v1  [cond-mat.str-el]  27 Mar 2025

Materials with a spiral spin ordering always show a rich phase diagram and
can be a playground for studying the exotic physical properties associated
with spiral magnetism. Using neutron elastic and resonant x-ray scattering
on a high-quality single crystal YBaCuFeO5, we demonstrate YBaCuFeO5
to be a helimagnet consisting of a double-spiral spin ordering. YBaCuFeO5
undergoes a commensurate to incommensurate magnetic phase transition
at TN2∼175 K, and the incommensurate phase consists of two spin-ordered
components. Both components have different periodicities but with the same
propagating direction along the c-axis below T N2.
Using resonant x-ray
scattering at the Fe and Cu K-edges, we further demonstrate that both
spiral spin orderings result from the Fe3+ and Cu2+, respectively, forming a
double-spiral spin ordering structure. This can be understood to be caused by
the coupling between both sublattices of Fe3+ and Cu2+ with the atomic lattice.
1
Introduction
The helimagnet YBaCuFeO5 is a spin-ordered phase of YBaCuFeO5
in which the magnetization of the material is parallel to the c-axis
(Fig. 1a). This phase has a rich and complex phase diagram,
with

[Sample 53]
========================================
Title: Penalty decomposition derivative free method for the minimization of partially separable functions over a convex feasible set

Abstract: In this paper, we consider the problem of minimizing a smooth function, given
as finite sum of black-box functions, over a convex set. In order to
advantageously exploit the structure of the problem, for instance when the
terms of the objective functions are partially separable, noisy, costly or with
first-order information partially accessible, we propose a framework where the
penalty decomposition approach is combined with a derivative-free line
search-based method. Under standard assumptions, we state theoretical results
showing that the proposed algorithm is well-defined and globally convergent to
stationary points. The results of preliminary numerical experiments, performed
on test problems with number of variables up to thousands, show the validity of
the proposed method compared with a standard derivative-free line search
algorithm. Moreover, it is shown that the method is easily parallelizable and
hence capable of taking advantage of parallelization of computation, when
possible.

arXiv:2503.21631v1  [math.OC]  27 Mar 2025
ARTICLE
Penalty decomposition derivative free method for the minimization
of partially separable functions over a convex feasible set
Francesco Cecerea, Matteo Lapuccib, Davide Puccib, Marco Sciandronea
aDipartimento di Ingegneria Informatica, Automatica e Gestionale “Antonio Ruberti”,
Universit`a di Roma “La Sapienza”, Via Ariosto, 25, 00185, Roma, Italy; bDipartimento di
Ingegneria dell’Informazione, Universit`a di Firenze, Via di S. Marta, 3, 50135, Firenze, Italy
ARTICLE HISTORY
Compiled March 28, 2025
ABSTRACT
In this paper, we consider the problem of minimizing a smooth function, given as
ﬁnite sum of black-box functions, over a convex set. In order to advantageously
exploit the structure of the problem, for instance when the terms of the objec-
tive functions are partially separable, noisy, costly or with ﬁrst-order information
partially accessible, we propose a framework where the penalty decomposition ap-
proach is combined with a derivative-free line search-based method. Under standard
assumptions, we state theoretical results showing that the proposed algorithm is
well-deﬁned and globally convergent to stationary points. The results of prelimi-
nary numerical experiments, performed on test problems with number of variables
up to thousands, show the validity of the proposed method compared with a stan-
dard derivative-free line search algorithm. Moreover, it is shown that the method
is easily parallelizable and hence capable of taking advantage of parallelization of
computation, when possible.
KEYWORDS
Penalty decomposition, Finite-sum, Coordinate partially separable, Derivative-free
line search
AMS CLASSIFICATION
90C56, 90C30, 90C26
1. Introduction
In this work, we are interested in ﬁnite-sum optimization problems of the form
min f(x) =
m
X
j=1
fj(x)
s.t. x ∈X
(1)
where X ⊂Rn is a closed, convex set and fj : Rn →R, for j = 1,..., m are smooth
functions for which, however, we do not have access to ﬁrst-order information. We also
assume that f is bounded below by some value f ∗.
CONTACT Davide Pucci. Email: davide.pucci@uniﬁ.it

The literature on derivative-free methods is wide and increasing. For insights on
general derivative-free approaches we refer the reader to the books [2, 5], the survey
[15] and to some seminal papers both for smooth optimization (e.g., [7, 16, 22, 23])
and nonsmooth optimization (e.g., [1, 8, 17]).
For strongly structured problems, like (1), tailored derivative-free methods, have
been presented; for instance, in [6] authors exploit the peculiarities of the feasible set
(being the convex hull of a given set of atoms) of the problem; partial knowledge of
ﬁrst order information is taken into account in [18]; the partially separable structure
of the objective function is exploited in [11].
The problem of ﬁnite-sum optimization has also been studied extensively for nonconvex
problems. In [24], the authors prove a theorem that any convex ﬁnite-sum optimization problem
can be decomposed in a convex subproblem (with the same objective function) and a set of
nonconvex subproblems (with the same set of ﬁnite-sum

[Sample 54]
========================================
Title: Simulating quantum circuits with restricted quantum computers

Abstract: It is one of the most fundamental objectives in quantum information science
to understand the boundary between the computational power of classical and
quantum computers. One possible avenue to explore this boundary is to identify
classes of quantum circuits that can be efficiently simulated on a classical
computer. Instead of simulating a general quantum circuit with a classical
device, new schemes have recently emerged to simulate them on a quantum device
that is restricted in some manner. As such, these techniques allow us to study
how the restrictions impact the computational power of the device. One such
technique is called quasiprobability simulation (QPS) and it estimates the
result of a quantum circuit with a Monte Carlo procedure that randomly replaces
circuit elements with ones that can be executed on the restricted quantum
device.
  The main focus of this thesis is dedicated to the QPS-based simulation of
nonlocal quantum computation using local quantum operations. On the practical
side, this enables the simulation of large quantum circuits using multiple
smaller quantum devices - a procedure that is sometimes called circuit
knitting. We uncover a rich mathematical formalism with many connections to the
resource theory of entanglement. We characterize the optimal simulation
overhead for a broad range of practically relevant nonlocal states and channels
and we explicitly provide achieving protocols. Moreover, we also investigate
the utility of classical communication between the local parties. Our results
address both the single-shot and asymptotic regime.
  We frame QPS in a quantum resource theoretic framework, which highlights
similarities that arise in the different instantiations of the technique.
Furthermore, we study the importance of classical side information in the QPS
procedure and how it impacts the overhead and expressibility of QPS.

DISS. ETH NO. 30914
Simulating quantum circuits with
restricted quantum computers
A thesis submitted to attain the degree of
DOCTOR OF SCIENCES
(Dr. sc. ETH Zurich)
presented by
Christophe Piveteau
born on 23.10.1996
accepted on the recommendation of
Prof. Dr. Renato Renner, examiner
Dr. Joseph M. Renes, co-examiner
Prof. Dr. David Gross, co-examiner
Prof. Dr. Aram Harrow, co-examiner
2025
arXiv:2503.21773v1  [quant-ph]  27 Mar 2025

2

Acknowledgements
I would like to thank my doctoral advisor Renato Renner for granting me the out-
standing opportunity to perform my doctoral studies in the quantum information
theory group at ETH Zurich. The extraordinary and highly stimulating environment
has been instrumental for my academic development and has sharpened my scien-
tific skills over the last four years. I especially appreciate the freedom I was given to
work on a wide variety of different topics of my choice.
I would like to convey special thanks to Joe Renes who has been a phenomenal
supervisor. No matter the circumstances, he always found time for me, and he always
supported me in every way necessary.
I enjoyed our plentiful discussions about
science and non-scientific topics, which have made my doctoral studies so much
more enjoyable. Joe is an amazing researcher to collaborate with, and he is a great
inspiration for the kind of scientist I aspire to become. Furthermore, I would like
to express my deepest gratitude to David Sutter and Christopher Chubb, who both
served as mentors to me. I am truly thankful for their guidance that has helped me
navigate the scientific and non-scientific aspects of academic life.
I also want to thank my other collaborators Lukas Schmitt, Lukas Brenner, Tho-
mas Dubach, Stefan W¨orner, Kristan Temme, Sergey Bravyi and Jay Gambetta for the
many fascinating research projects that I could partake in. My gratitude also extends
to the present and past members of the quantum information theory group for the
supportive and friendly atmosphere. I especially appreciate the numerous and highly
productive discussions with Lukas Schmitt and David Sutter that were instrumental
in the development of this thesis.
I also want to thank both of my sisters as well as my parents for their constant
support and encouragement. Finally, and most importantly, I would like to thank my
partner Anna for her unwavering support and the willingness to put up with the ups
and downs that are an inevitable part the PhD journey. I am immensely grateful to
have shared this period of my life with her.
Christophe Piveteau
Zurich, January 2025
3

4

Abstract
It is one of the most fundamental objectives in quantum information science to un-
derstand the boundary between the computational power of classical and quantum com-
puters. One possible avenue to explore this boundary is to identify classes of qu-
antine circuits that can be efficiently simulated on a classical computer. Instead of sim-
ulating a general quantum circuit with a classical device, new schemes have recently emerge-
ing to simulate them on a quantum device that is restricted in some manner. As such, these
techniques allow us to study how the restrictions impact the computational power of the
device. One such technique is called quas

[Sample 55]
========================================
Title: 3D MHD simulations of runaway pulsars in core collapse supernova remnants

Abstract: Pulsars are one of the possible final stages in the evolution of massive
stars. If a supernova explosion is anisotropic, it can give the pulsar a
powerful kick, propelling it to supersonic speeds. The resulting pulsar wind
nebula is significantly reshaped by its interaction with the surrounding medium
as the pulsar moves through it. First, the pulsar crosses the supernova
remnant, followed by the different layers of circumstellar medium formed during
different stages of the progenitor star s evolution. We aim to investigate how
the evolutionary history of massive stars shapes the bow shock nebulae of
runaway kicked pulsars, and how these influences in turn affect the dynamics
and non-thermal radio emission of the entire pulsar remnant. We perform
three-dimensional magnetohydrodynamic simulations using the PLUTO code to model
the pulsar wind nebula generated by a runaway pulsar in the supernova remnant
of a red supergiant progenitor, and derive its non-thermal radio emission. The
supernova remnant and the pre-supernova circumstellar medium of the progenitor
strongly confine and reshape the pulsar wind nebula of the runaway pulsar,
bending its two side jets inwards and giving the nebula an arched shape for an
observer perpendicular to the jets and the propagation direction, as observed
around PSR J1509 5850 and Gemina. We perform the first classical 3D model of a
pulsar moving inward through its supernova ejecta and circumstellar medium,
inducing a bending of its polar jet that turns into characteristic radio
synchrotron signature. The circumstellar medium of young runaway pulsars has a
significant influence on the morphology and emission of pulsar wind nebulae,
whose comprehension requires a detailed understanding of the evolutionary
history of the progenitor star.

Astronomy & Astrophysics manuscript no. paper_AA_template
©ESO 2025
March 28, 2025
3D MHD simulations of runaway pulsars in core-collapse
supernova remnants
D. M.-A. Meyer1, D. F. Torres1, 2, 3 and Z. Meliani4
1 Institute of Space Sciences (ICE, CSIC), Campus UAB, Carrer de Can Magrans s/n, 08193 Barcelona, Spain
e-mail: dmameyer.astro@gmail.com
2 Institut d’Estudis Espacials de Catalunya (IEEC), 08034 Barcelona, Spain
3 Institució Catalana de Recerca i Estudis Avançats (ICREA), 08010 Barcelona, Spain
4 Laboratoire Univers et Théories, Observatoire de Paris, Université PSL, Université de Paris, CNRS, F-92190 Meudon, France
ABSTRACT
Context. Pulsars are one of the possible final stages in the evolution of massive stars. If a supernova explosion is anisotropic, it can
give the pulsar a powerful "kick", propelling it to supersonic speeds. The resulting pulsar wind nebula is significantly reshaped by its
interaction with the surrounding medium as the pulsar moves through it. First, the pulsar crosses the supernova remnant, followed by
the different layers of circumstellar medium formed during different stages of the progenitor star’s evolution.
Aims. We aim to investigate how the evolutionary history of massive stars shapes the bow shock nebulae of runaway "kicked" pulsars,
and how these influences in turn affect the dynamics and non-thermal radio emission of the entire pulsar remnant.
Methods. We perform three-dimensional magnetohydrodynamic simulations using the PLUTO code to model the pulsar wind nebula
generated by a runaway pulsar in the supernova remnant of a red supergiant progenitor, and derive its non-thermal radio emission.
Results. The supernova remnant and the pre-supernova circumstellar medium of the progenitor strongly confine and reshape the
pulsar wind nebula of the runaway pulsar, bending its two side jets inwards and giving the nebula an arched shape for an observer
perpendicular to the jets and the propagation direction, as observed around PSR J1509–5850 and Gemina.
Conclusions. We perform the first classical 3D model of a pulsar moving inward through its supernova ejecta and circumstellar
medium, inducing a bending of its polar jet that turns into characteristic radio synchrotron signature. The circumstellar medium of
young runaway pulsars has a significant influence on the morphology and emission of pulsar wind nebulae, whose comprehension
requires a detailed understanding of the evolutionary history of the progenitor star.
1. Introduction
A pulsar is a rapidly rotating neutron star, formed in the collapse of a massive star [1, 2]. The rotation of the neutron star
causes the emission of beams of electromagnetic radiation, which are observed as pulses [3].
If the supernova explosion that formed the neutron star is anisotropic, it can give the pulsar

[Sample 56]
========================================
Title: On-chip calibration of Microscale-Thermocouples for Precise Temperature Measurement

Abstract: Precise temperature measurement at micro/nanoscale is crucial across various
domains including physical sciences, chemical processes, industrial production,
medical diagnosis, weather forecasting, electronics, and biology.
Micro/nanoscale thermal mapping requires precise techniques such as
thermocouples, resistance-based devices, infrared thermography, optical
interferometry, Raman thermometry, and Time domain-thermoreflectance (TDTR)
method. Each method has its advantages and limitations, emphasizing the
importance of selecting the appropriate technique. Among these methods,
micro-thin film thermocouples (TFTCs) offer a compelling solution due to their
direct contact-based temperature measurements, minimal surface preparation
requirements, lower cost, and robustness against environmental factors.
Thermocouples work on the well-established Seebeck effect, where a voltage is
generated proportional to the temperature difference between two points.
However, at micro/nanoscale, the Seebeck coefficients of thermocouples differ
from those in bulk materials, requiring experimental calibration for precise
measurements. To address this, we introduce an on-chip characterization
platform with a differential temperature measurement setup on a borosilicate
glass substrate. This platform utilizes a microheater as a localized heat
source to elevate the temperature at the hot junction of the TFTC while
maintaining the cold junction at ambient conditions. Numerical simulations are
employed to engineer both the microheater and TFTC junction for precise
temperature control. The functionality of this platform is validated by
fabricating TFTCs using standard fabrication processes and measuring the TFTC
response to determine the differential Seebeck coefficient of a
Platinum-Chromium TFTC Junction. The calculated sensitivity of Pt/Cr TFTCs
using this calibration method is 19.23 +- 0.405 {\mu}V/C.

1 
 
On-chip calibration of Microscale-Thermocouples for Precise 
Temperature Measurement 
Hassan Irshad Bhatti 
Advanced Semiconductor Laboratory, Electrical and Computer Engineering Program, 
CEMSE Division, King Abdullah University of Science and Technology (KAUST), Thuwal 
23955-6900, Kingdom of Saudi Arabia hassan.bhatti@kaust.edu.sa,  
Abstract 
Precise temperature measurement at micro/nanoscale is crucial across various domains 
including physical sciences, chemical processes, industrial production, medical diagnosis, 
weather forecasting, electronics, and biology. Micro/nanoscale thermal mapping requires 
precise techniques such as thermocouples, resistance-based devices, infrared thermography, 
optical interferometry, Raman thermometry, and Time domain-thermoreflectance (TDTR) 
method. Each method has its advantages and limitations, emphasizing the importance of 
selecting the appropriate technique. Among these methods, micro-thin film thermocouples 
(TFTCs) offer a compelling solution due to their direct contact-based temperature 
measurements, minimal surface preparation requirements, lower cost, and robustness against 
environmental factors. Thermocouples work on the well-established Seebeck effect, where a 
voltage is generated proportional to the temperature difference between two points. However, 
at micro/nanoscale, the Seebeck coefficients of thermocouples differ from those in bulk 
materials, requiring experimental calibration for precise measurements. To address this, we 
introduce an on-chip characterization platform with a differential temperature measurement 
setup on a borosilicate glass substrate. This platform utilizes a microheater as a localized heat 
source to elevate the temperature at the hot junction of the TFTC while maintaining the cold 
junction at ambient conditions. Numerical simulations are employed to engineer both the 
microheater and TFTC junction for precise temperature control. The functionality of this 
platform is validated by fabricating TFTCs using standard fabrication processes and measuring 

2 
 
the TFTC response to determine the differential Seebeck coefficient of a Platinum-Chromium 
TFTC Junction. The calculated sensitivity of Pt/Cr TFTCs using this calibration method is 
19.23 ± 0.405 μV/C. 
Introduction 
Accurate temperature measurement is a fundamental requirement across various domains, 
spanning physical sciences, chemical processes [1], and beyond. Its significance extends to 
practical applications in industrial production, where precise temperature control ensures 
optimal manufacturing conditions [2, 3]. In medical diagnosis, temperature monitoring aids in 
identifying health anomalies and guiding treatment strategies [4]. Weather forecasting relies on 
accurate temperature data to predict climatic conditions accurately [5, 6]. In electronics, 
temperature measurement plays a critical role in ensuring device reliability and performance [7, 8]. 
Moreover, in biology, temperature control is essential for maintaining the viability of living 
organisms and studying their response to environmental changes [9, 10]. 
In recent years, the demand for precise temperature measurement at micro/nanoscale

[Sample 57]
========================================
Title: On the magnetic perturbation theory for Chern insulators

Abstract: The gauge covariant magnetic perturbation theory is tailored for one-body
Schr\"odinger operators perturbed by long-range magnetic fields. In this work
we present a self-contained exposition of the method, by outlining its
technical foundations and discussing the physical heuristics behind the proofs.
We apply it in order to prove the stability of spectral gaps and to study the
location of the discrete spectrum. We also analyze the (lack of) continuity
with respect to the magnetic field of spectral projections corresponding to
finite spectral islands, which is a particularly important situation for
systems modelling Chern insulators. Finally, we show how to construct
approximate projections that have an explicit dependence with respect to the
magnetic field parameter.

arXiv:2503.20763v1  [math-ph]  26 Mar 2025
ON THE MAGNETIC PERTURBATION THEORY
FOR CHERN INSULATORS
HORIA D. CORNEAN AND MASSIMO MOSCOLARI
ABSTRACT. The gauge covariant magnetic perturbation theory is tailored for one-body
Schr¨odinger operators perturbed by long-range magnetic ﬁelds. In this work we present a self-
contained exposition of the method, by outlining its technical foundations and discussing the
physical heuristics behind the proofs. We apply it in order to prove the stability of spectral gaps
and to study the location of the discrete spectrum. We also analyze the (lack of) continuity with
respect to the magnetic ﬁeld of spectral projections corresponding to ﬁnite spectral islands, which
is a particularly important situation for systems modelling Chern insulators. Finally, we show how
to construct approximate projections that have an explicit dependence with respect to the magnetic
ﬁeld parameter.
1. INTRODUCTION
Magnetic Schr¨odinger operators are ubiquitous in modern mathematical physics. In view of
their physical relevance, they gained a mathematical reputation on their own as a distinguished
playground to develop and test new techniques in diverse area of mathematics, from operator and
perturbation theory to non-commutative geometry. In this work we show how to deal with long-
range magnetic perturbations using the so-called gauge covariant magnetic perturbation theory.
1.1. Long-range magnetic perturbations. We begin by describing two simple situations that
highlight the singular character of magnetic perturbations. In dimension d = 2, the free Laplacian
−∆is a selfadjoint operator on H2(R2) and its spectrum is purely absolutely continuous and
coincides with [0,∞). When we allow a non-zero constant magnetic ﬁeld oriented perpendicular
to the plane, we have to consider the operator (−i∇−bAL)2, b > 0, where AL : R2 →R2 is the
magnetic vector potential in the-called symmetric gauge AL(x) = 1
2(−x2,x1). This is the well-
known Landau Hamiltonian, which is selfadjoint on the magnetic Sobolev space H2
AL(R2), given
by
H2
AL(R2) =

f ∈C∞
0 (R2)|(−i∇−bAL)2 f ∈L2(R2)
	∥·∥DAL
(1.1)
where C∞
0 (R2) denotes the set of smooth functions with compact support and the closure is taken
with respect to the graph norm ∥f∥2
DAL := ∥f∥2 +∥(−i∇−bAL)2 f∥2. Its spectrum is purely pure
point and given by

(2n+1) b,
n ∈N
	
where each eigenvalue is inﬁnitely degenerate. The analysis of the Landau Hamiltonian [25, 29]
is by now textbook material [30], see [1] or [34] for a recent review on the subject. This simple
example shows that adding a small magnetic ﬁeld to the system completely changes both the
1

2
HORIA D. CORNEAN AND MASSIMO MOSCOLARI
domains of self-adjointness and the nature of the spectrum. The reader familiar with classical
physics will not ﬁnd this surprising: a charged particle conﬁned to a plane and subjected to
a constant magnetic ﬁeld perpendicular to the plane moves on conﬁned trajectories. In the
physics literature, this is known as the magnetic ﬁeld effect, and it is often modelled using the
Szilard-
Dyson equation [7], [12] or the Landau Hamiltonian [25]. The latter is a well-known
example of the Schr¨odinger equation for magnetic systems. The Landau Hamiltonian has a
doubly periodic spectrum, and it is often used as a model for

[Sample 58]
========================================
Title: StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion

Abstract: We present StyleMotif, a novel Stylized Motion Latent Diffusion model,
generating motion conditioned on both content and style from multiple
modalities. Unlike existing approaches that either focus on generating diverse
motion content or transferring style from sequences, StyleMotif seamlessly
synthesizes motion across a wide range of content while incorporating stylistic
cues from multi-modal inputs, including motion, text, image, video, and audio.
To achieve this, we introduce a style-content cross fusion mechanism and align
a style encoder with a pre-trained multi-modal model, ensuring that the
generated motion accurately captures the reference style while preserving
realism. Extensive experiments demonstrate that our framework surpasses
existing methods in stylized motion generation and exhibits emergent
capabilities for multi-modal motion stylization, enabling more nuanced motion
synthesis. Source code and pre-trained models will be released upon acceptance.
Project Page: https://stylemotif.github.io

STYLEMOTIF: Multi-Modal Motion Stylization using Style-Content Cross Fusion
Ziyu Guo†
CUHK, MiuLar Lab
ziyuguo@link.cuhk.edu.hk
Young Yoon Lee
Roblox
ylee@roblox.com
Joseph Liu
Roblox
josephliu@roblox.com
Yizhak Ben-Shabat
Roblox
ibenshabat@roblox.com
Victor Zordan
Roblox
vbzordan@roblox.com
Mubbasir Kapadia
Roblox
mkapadia@roblox.com
Project Page: https://stylemotif.github.io
Abstract
We present STYLEMOTIF, a novel Stylized Motion Latent
Diffusion model, generating motion conditioned on both
content and style from multiple modalities. Unlike existing
approaches that either focus on generating diverse motion
content or transferring style from sequences, STYLEMOTIF
seamlessly synthesizes motion across a wide range of content
while incorporating stylistic cues from multi-modal inputs,
including motion, text, image, video, and audio. To achieve
this, we introduce a style-content cross fusion mechanism
and align a style encoder with a pre-trained multi-modal
model, ensuring that the generated motion accurately cap-
tures the reference style while preserving realism. Extensive
experiments demonstrate that our framework surpasses ex-
isting methods in stylized motion generation and exhibits
emergent capabilities for multi-modal motion stylization, en-
abling more nuanced motion synthesis. Source code and
pre-trained models will be released upon acceptance.
1. Introduction
Human motion generation is a fundamental task in computer
graphics and animation, enabling the synthesis of realistic
and expressive human movements. Broadly, human motion
can be characterized by two complementary aspects: content,
which defines the underlying action (e.g., walking, jumping),
and style, which encodes variations such as personal flair,
emotional expression, or cultural influences (e.g., jubilant,
aggressive). This separation allows for greater control and
flexibility in generating motion, making it particularly valu-
able in creative industries like game development, film pro-
† Work done as an intern at Roblox.
Diffusion Network
SMooDi
StyleMotif (Ours)
Content
“A person walks
in a circle.”
Diffusion 
Network
o Inefficient Dual Branch
o Limited to Motion Condition
Diffusion 
Network
Content
“A person walks
in a circle.”
“Chicken”
[Rooster Crow]
o Streamlined Single Branch
o Multi-modal Conditions
Style Motion
Multi-modal Style Inputs
Style-Content Cross Fusion
Figure 1. Comparison of Our Proposed STYLEMOTIF Frame-
work with SMooDi. Unlike SMooDi’s dual-branch design, which
increases model complexity and training overhead, STYLEMOTIF
employs a streamlined single-branch structure, enabling efficient
multi-modal motion stylization while preserving motion realism.
duction, and virtual reality. However, traditional approaches
to stylized motion generation often depend on manual pro-
cesses such as motion capture or keyframe animation, which
are costly, time-consuming, and labor-intensive.
Recent progress in text-to-motion (T2M) diffusion frame-
works [6, 26, 52] has addressed this challenge by enabling
generating motion based solely on textual descriptions. However,
these methods typically lack the richness of real-world motions
and often struggle to accurately reproduce complex and diverse
human movements.
Multi-modal motion generation models [24, 29, 39, 50, 63]
have emerged as a promising solution, leveraging the power
of deep learning to capture and generate motion across
multiple modalities, including text, image, video, and audio.

[Sample 59]
========================================
Title: Measuring and Analyzing Subjective Uncertainty in Scientific Communications

Abstract: Uncertainty of scientific findings are typically reported through statistical
metrics such as $p$-values, confidence intervals, etc. The magnitude of this
objective uncertainty is reflected in the language used by the authors to
report their findings primarily through expressions carrying
uncertainty-inducing terms or phrases. This language uncertainty is a
subjective concept and is highly dependent on the writing style of the authors.
There is evidence that such subjective uncertainty influences the impact of
science on public audience. In this work, we turned our focus to scientists
themselves, and measured/analyzed the subjective uncertainty and its impact
within scientific communities across different disciplines. We showed that the
level of this type of uncertainty varies significantly across different fields,
years of publication and geographical locations. We also studied the
correlation between subjective uncertainty and several bibliographical metrics,
such as number/gender of authors, centrality of the field's community, citation
count, etc. The underlying patterns identified in this work are useful in
identification and documentation of linguistic norms in scientific
communication in different communities/societies.

Article
Measuring and Analyzing Subjective Uncertainty in Scientific
Communications
Grace Shao 1 and Jamshid Sourati 2*
1
University of Chicago, Department of Sociology (Computational Social Sciences)
2
DePaul University, School of Computing
*
Correspondence: jsourati@depaul.edu
Abstract: Uncertainty of scientific findings are typically reported through statistical metrics such
as p-values, confidence intervals, etc. The magnitude of this objective uncertainty is reflected in
the language used by the authors to report their findings primarily through expressions carrying
uncertainty-inducing terms or phrases. This language uncertainty is a subjective concept and is highly
dependent on the writing style of the authors. There is evidence that such subjective uncertainty
influences the impact of science on public audience. In this work, we turned our focus to scientists
themselves, and measured/analyzed the subjective uncertainty and its impact within scientific
communities across different disciplines. We showed that the level of this type of uncertainty
varies significantly across different fields, years of publication and geographical locations. We also
studied the correlation between subjective uncertainty and several bibliographical metrics, such as
number/gender of authors, centrality of the field’s community, citation count, etc. The underlying
patterns identified in this work are useful in identification and documentation of linguistic norms in
scientific communication in different communities/societies.
Keywords: subjective language uncertainty; natural language processing; scientific communications
1. Introduction
Traditionally, scientific theories were taken as absolute claims with no subjective
uncertainty around them. Today, we recognize that our scientific models are merely compu-
tational approximations of reality and, therefore, are taken within a shade of uncertainty as
opposed to absolute assertions [1]. Uncertainty of findings could be either obtained through
the statistical analysis reported in the manuscripts through p-values, confidence intervals,
etc. [2,3], and/or directly based on internal structure of the findings themselves [4,5]1.
The formulations used to measure this type of quantifiable uncertainty can be derived
independently of the scientists who conducted the work, which is why it is regarded
as objective uncertainty. The degree of objective uncertainty is typically reflected in the
language used by the authors to convey their findings primarily through expressions that
incorporate uncertainty-inducing terms or phrases. While objective uncertainty can be
reported through statistical metrics independent of the authors’ writing style, the verbal
uncertainty is a subjective concept depending on how individual writers communicate their
findings [7]. Being able to properly interpret subjective uncertainty within the language
can be helpful in the absence of reported statistical uncertainty analysis. Prior research
provided evidence that the level of subjective uncertainty influences the impact of science
on public audience. As an example, there have been studies showing destructive effect of
high uncertainty on public trustworthiness [8], slowing down the dissemination of scientific
results through social media [9]. On the other hand, it is suggested that very high certainty
can damage the authors’ reputation [10]. In this paper, we switched our focus on the effect
of the way scientists incorporate uncertainty in their language on their peer researchers
that are potentially from other fields.
1
Such quantifiable objective uncertainty is sometimes called “risk” against the non-quantifiable lack of confi-
dence that is simply called “uncertainty” [6].
1
arXiv:2503.21114v1  [cs.DL]  27 Mar 2025

2 of 21
Effectively communicating subjective uncertainty with respect to scientific findings
is important in scientific communities as well as for general public. However, the
measurement of subjective uncertainty is a challenging task as it is a language-based
concept that is difficult to quantify. To date, most of the research on this topic has
focused on quantifiable measures of uncertainty (such as p-values, confidence
intervals, etc.) [2,3,7,11,12,13,14]. However, this approach misses out on the
underlying

[Sample 60]
========================================
Title: Twisted moments of characteristic polynomials of random matrices in the unitary group

Abstract: Recently, Keating and the second author of this paper devised a heuristic for
predicting asymptotic formulas for moments of the Riemann zeta-function
$\zeta(s)$. Their approach indicates how lower twisted moments of $\zeta(s)$
may be used to evaluate higher moments. In this paper, we present a rigorous
random matrix theory analogue of their heuristic. To do this, we develop a
notion of "twisted moment" of characteristic polynomials of matrices in the
unitary group $U(N)$, and we prove several identities involving Schur
polynomials. Our results may be viewed as a proof of concept of the heuristic
for $\zeta(s)$.

arXiv:2503.21682v1  [math.NT]  27 Mar 2025
TWISTED MOMENTS OF CHARACTERISTIC POLYNOMIALS OF
RANDOM MATRICES IN THE UNITARY GROUP
SIEGFRED BALUYOT AND BRIAN CONREY
Abstract. Recently, Keating and the second author of this paper devised a heuristic for
predicting asymptotic formulas for moments of the Riemann zeta-function ζ(s). Their ap-
proach indicates how lower twisted moments of ζ(s) may be used to evaluate higher moments.
In this paper, we present a rigorous random matrix theory analogue of their heuristic. To do
this, we develop a notion of “twisted moment” of characteristic polynomials of matrices in
the unitary group U(N), and we prove several identities involving Schur polynomials. Our
results may be viewed as a proof of concept of the heuristic for ζ(s).
1. Introduction and results
In this paper, we present a rigorous random matrix theory analogue of the heuristic in [16],
which was devised by Keating and the second author of this paper as an approach towards
ﬁnding asymptotic formulas for moments of the Riemann zeta-function. Our results may be
viewed as a proof of concept of their heuristic.
Since the monumental discovery of Dyson and Montgomery [17, 22], random matrix theory
has become a useful predictive tool in the study of L-functions. Keating and Snaith, in their
seminal work [19], applied random matrix theory to predict the leading terms of asymptotic
formulas for the moments
(1.1)
Z T
0
|ζ( 1
2 + it)|2r dt
of the Riemann zeta function ζ(s) when r is any ﬁxed complex number with real part > −1/2.
At that time, number theoretic heuristics involving long Dirichlet polynomials have resulted
in predictions for the leading term for r = 3, 4 but failed to give a feasible prediction for
r = 5 [10, 11]. To date, asymptotic formulas for (1.1) can be proved only for r = 0 (trivially),
r = 1, and r = 2 [23]. Remarkably, the predictions of Keating and Snaith agree with the
known formulas and the long Dirichlet polynomial predictions for r = 0, 1, 2, 3, 4.
Today, there are several diﬀerent heuristics that predict precise asymptotic formulas for
(1.1) and agree with the conjectures of Keating and Snaith. One of these heuristics is the
recipe in [9], which also predicts lower order terms and applies to a general family of L-
functions. While the recipe seems to give the correct answer, its individual steps may be
unjustiﬁable. This is because some steps in the recipe may throw away terms that are of the
same size as the main term, while other steps bring back other terms of the same size [9,
Section 2.1]. Curiously, these large errors in the individual steps of the recipe seem to cancel
out to give the correct answer.
2020 Mathematics Subject Classiﬁcation. 05E10, 11M50, 15B52.
SB was partially supported by NSF DMS-1854398 FRG. BC is partially supported by a grant from the
NSF.
1

2
SIEGFRED BALUYOT AND BRIAN CONREY
In order to investigate how and why the recipe works, Keating and the second author of
this paper devised a heuristic that involves the twisted moments of the characteristic polynomials
of matrices in the unitary group U(N). The heuristic predicts asymptotic formulas for higher
moments of ζ(s) in terms of lower moments of the characteristic polynomials of matrices in the unitary
group. In this paper, we present a rigorous random matrix theory analogue of their heuristic. To
do this, we develop a notion of �

[Sample 61]
========================================
Title: A Powerful Bootstrap Test of Independence in High Dimensions

Abstract: This paper proposes a nonparametric test of independence of one random
variable from a large pool of other random variables. The test statistic is the
maximum of several Chatterjee's rank correlations and critical values are
computed via a block multiplier bootstrap. The test is shown to asymptotically
control size uniformly over a large class of data-generating processes, even
when the number of variables is much larger than sample size. The test is
consistent against any fixed alternative. It can be combined with a stepwise
procedure for selecting those variables from the pool that violate
independence, while controlling the family-wise error rate. All formal results
leave the dependence among variables in the pool completely unrestricted. In
simulations, we find that our test is very powerful, outperforming existing
tests in most scenarios considered, particularly in high dimensions and/or when
the variables in the pool are dependent.

A Powerful Bootstrap Test of Independence in High Dimensions†
Mauricio Olivares
Department of Statistics
LMU Munich
m.olivares@lmu.de
Tomasz Olma
Department of Statistics
LMU Munich
t.olma@lmu.de
Daniel Wilhelm
Departments of Statistics and
Economics
LMU Munich
d.wilhelm@lmu.de
March 28, 2025
Abstract
This paper proposes a nonparametric test of independence of one random variable from a large
pool of other random variables. The test statistic is the maximum of several Chatterjee’s rank
correlations and critical values are computed via a block multiplier bootstrap. The test is shown
to asymptotically control size uniformly over a large class of data-generating processes, even when
the number of variables is much larger than sample size. The test is consistent against any fixed
alternative. It can be combined with a stepwise procedure for selecting those variables from the
pool that violate independence, while controlling the family-wise error rate. All formal results leave
the dependence among variables in the pool completely unrestricted. In simulations, we find that
our test is very powerful, outperforming existing tests in most scenarios considered, particularly in
high dimensions and/or when the variables in the pool are dependent.
Keywords: independence test, high-dimensional data, Chatterjee’s rank correlation, block multiplier
bootstrap, family-wise error rate.
†The authors gratefully acknowledge financial support from the European Research Council (Starting Grant No.
852332).
arXiv:2503.21715v1  [stat.ME]  27 Mar 2025

1
Introduction
This paper is concerned with nonparametric testing of independence between a random variable X
and many other random variables Y1,..., Yp,
H0 : Yj ⊥X for all j ∈{1,..., p},
against the alternative H1, which is the negation of H0. The goal is to propose a powerful test of
H0 allowing for p to be much larger than the sample size while at the same time not restricting the
dependence among Y1,..., Yp in any way. In a second step, we want to combine the new test with
a stepwise procedure for screening out variables from Y1,..., Yp that violate independence so as to
control the family-wise error rate.
There are many applied examples in which testing H0 and, in particular, screening out variables
that violate independence is of interest. For instance, in causal inference, one might want to test
whether a treatment indicator has an effect on various outcomes and then select those outcomes on
which there is an effect. Such a test could also be applied to “placebo” outcomes, i.e. pre-treatment
outcomes that the researcher knows cannot have been affected by the treatment, to validate uncon-
foundedness assumptions. Another example concerns testing fairness of machine learning predictions,
where one might want to test independence of a prediction from a set of protected characteristics. As
a final example, one might want to test independence of a measure of environmental exposure (e.g.,
whether or not a person smokes) from a vector of genetic markers. In our empirical application, we
use the proposed test for identifying genes whose transcript levels oscillate during the cell cycle.
As test statistic we consider the maximum of p rank correlation coefficients by Chatterjee (2021)
for testing independence between Yj and X, j = 1,..., p. Critical values for the test are computed via
a block multiplier bootstrap that perturbs an asymptotically linear representation of the rank corre-
lations. We then show that the test has the following asymptotic properties:
(1) The test is asymptotically exact for independence.
(2) The test is asymptotically large for independence.
(3) The test is asymptotically small for independence.
(4) The test is asymptotically controlled for independence.
(5) The test is asymptotically controlled for size.
(6) The test is as

