{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Research-RAG-LLM'...\n",
      "remote: Enumerating objects: 519, done.\u001b[K\n",
      "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 519 (delta 1), reused 7 (delta 1), pack-reused 505 (from 2)\u001b[K\n",
      "Receiving objects: 100% (519/519), 1.55 GiB | 35.32 MiB/s, done.\n",
      "Resolving deltas: 100% (23/23), done.\n",
      "Updating files: 100% (453/453), done.\n",
      "Filtering content: 100% (6/6), 147.56 MiB | 37.86 MiB/s, done.\n",
      "/content/Research-RAG-LLM\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/vmkhoa/Research-RAG-LLM.git\n",
    "%cd Research-RAG-LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q peft transformers bitsandbytes accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607aaa04d26448958bb2295c71ef8a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e24594188a4fb2b29e58f763b70020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5ae871df2c43f8b6412ad7a5a5bd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7919f82cdbc3430a90edfb8afa63fcb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5573bdbfeb654a9eb1a300a254d4fb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd76adc25d2747fdb1f125dc854be10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0ff5edc95446a6bf002640771eed7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['corda_config', 'trainable_token_indices'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e7c4dbb4114e5bbd8ccc7ffcb7f686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3163058044c5412cb0f44b5554588fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f0d187c45d4e1486a1041e7adedc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3a638ff4a24f45b5baccf97c5efd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3877c85007bb44a5922f5d26e2ee0b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d787e109fc45cebeba0e60d1ebc7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged + quantized model saved at: ./phi2-lora-quantized\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model_id = \"microsoft/phi-2\"\n",
    "lora_path = \"./output\"\n",
    "output_path = \"./phi2-lora-quantized\"\n",
    "\n",
    "# Load base model\n",
    "base = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# Load and merge LoRA\n",
    "model = PeftModel.from_pretrained(base, lora_path)\n",
    "model = model.merge_and_unload()\n",
    "model.eval()\n",
    "\n",
    "# Save merged + quantized model\n",
    "model.save_pretrained(output_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.save_pretrained(output_path)\n",
    "\n",
    "print(\"Merged + quantized model saved at:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10cc6f587794eedb259f2399cd30402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    output_path,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a molecular quantum transformer?\n",
      "\n",
      "A molecular quantum transformer is a device that can convert one quantum state into another by using a molecular system that can undergo a quantum transition. A quantum transition is a change in the energy or the spin of a quantum particle, such as an electron or a photon. A molecular quantum transformer can be used to create or manipulate quantum states that are useful for quantum computing, quantum communication, or quantum metrology.\n",
      "\n",
      "How does a molecular quantum transformer work?\n",
      "\n",
      "A molecular quantum transformer works by using a molecular system that can undergo a quantum transition, such as a transition between different energy levels or between different spin states. The molecular system can be a molecule, a complex, or a nanostructure that has a specific shape, size, or symmetry. The molecular system can be connected to an external device, such as a laser, an electric field, or a magnetic field, that can induce the quantum transition. The external device can be controlled by a computer or a human operator to create or manipulate the desired quantum state.\n",
      "\n",
      "What are the applications of molecular quantum transformers?\n",
      "\n",
      "Molecular quantum transformers have many potential applications in quantum science and technology. Some of the applications are:\n",
      "\n",
      "- Quantum computing: Molecular quantum transformers can be used to create or manipulate quantum bits, or qubits, which are the basic units of quantum information. Qubits can be encoded in the energy or the spin of a quantum particle, such as an electron or a photon, and can be manipulated by applying external fields or by using other quantum devices. Molecular quantum transformers can be used to create or manipulate qubits that have specific properties, such as coherence, entanglement, or superposition, that are essential for quantum computing.\n",
      "- Quantum communication: Molecular quantum transformers can be used to create or manipulate quantum signals, or quantum states, that can be used for secure and efficient communication. Quantum signals can be encoded in the energy or the spin of a quantum particle, such as an electron or a photon, and can be transmitted over long distances or through optical fibers. Molecular quantum transformers can be used to create or manipulate quantum signals that have specific properties, such as polarization, phase, or frequency, that are essential for quantum communication.\n",
      "- Quantum metrology: Molecular quantum transformers can be used to create or manipulate quantum sensors, or devices, that can measure physical quantities with high precision and accuracy. Quantum sensors can be based on the energy or the spin of a quantum particle, such as an electron or a photon, and can be used to measure properties such as temperature, pressure, or magnetic field. Molecular quantum transformers can be used to create or manipulate quantum sensors that have specific properties, such as sensitivity, resolution, or stability, that are essential for quantum metrology.\n",
      "Question:\n",
      "\n",
      "Possible rewrite of the question and solution:\n",
      "\n",
      "Question: What is a molecular quantum transformer?\n",
      "\n",
      "A. A device that can convert one quantum state into another by using a molecular system that can undergo a quantum transition.\n",
      "B. A device that can convert one classical state into another by using a molecular system that can undergo a classical transition.\n",
      "C. A device that can convert one classical state into another by using a molecular system that can undergo a quantum transition.\n",
      "D. A device that can convert one classical state into another by using a molecular system that can undergo a classical transition.\n",
      "\n",
      "Solution: The correct option is A. A device that can convert one quantum state into another by using a molecular system that can undergo a quantum transition.\n",
      "\n",
      "A molecular quantum transformer is a device that can convert one quantum state into another by using a molecular system that can undergo a quantum transition.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is a molecular quantum transformer?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=800,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
